[
  {
    "id": "2508.14066",
    "url": "https://arxiv.org/abs/2508.14066",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.310119",
    "title": "Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation",
    "authors": "Lorenz Brehme, Benedikt Dornauer, Thomas Ströhle, Maximilian Ehrhart, Ruth Breu",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "Retrieval-Augmented Generation (RAG) is a well-established and rapidly evolving field within AI that enhances the outputs of large language models by integrating relevant information retrieved from external knowledge sources. While industry adoption of RAG is now beginning, there is a significant lack of research on its practical application in industrial contexts. To address this gap, we conducted a semistructured interview study with 13 industry practitioners to explore the current state of RAG adoption in real-world settings. Our study investigates how companies apply RAG in practice, providing (1) an overview of industry use cases, (2) a consolidated list of system requirements, (3) key challenges and lessons learned from practical experiences, and (4) an analysis of current industry evaluation methods. Our main findings show that current RAG applications are mostly limited to domain-specific QA tasks, with systems still in prototype stages; industry requirements focus primarily on data protection, security, and quality, while issues such as ethics, bias, and scalability receive less attention; data preprocessing remains a key challenge, and system evaluation is predominantly conducted by humans rather than automated methods.",
    "comments": "This preprint was accepted for presentation at the 17th International Conference on Knowledge Discovery and Information Retrieval (KDIR25)",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14077",
    "url": "https://arxiv.org/abs/2508.14077",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.312125",
    "title": "Label Smoothing is a Pragmatic Information Bottleneck",
    "authors": "Sota Kudo",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "This study revisits label smoothing via a form of information bottleneck. Under the assumption of sufficient model flexibility and no conflicting labels for the same input, we theoretically and experimentally demonstrate that the model output obtained through label smoothing explores the optimal solution of the information bottleneck. Based on this, label smoothing can be interpreted as a practical approach to the information bottleneck, enabling simple implementation. As an information bottleneck method, we experimentally show that label smoothing also exhibits the property of being insensitive to factors that do not contain information about the target, or to factors that provide no additional information about it when conditioned on another variable.",
    "comments": "18 pages, 8 figures, published in Transactions on Machine Learning Research (TMLR), 2025",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14275",
    "url": "https://arxiv.org/abs/2508.14275",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.338531",
    "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders",
    "authors": "Cliff O'Reilly, Ernesto Jimenez-Ruiz, Tillman Weyde",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14297",
    "url": "https://arxiv.org/abs/2508.14297",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.342587",
    "title": "Grid-Edge Energy-Flexible Technologies: A Comparative Analysis Across Generators, Loads, and Energy Storage Systems",
    "authors": "Jesus Silva-Rodriguez, Tianxia Zhao, Ran Mo, Xingpeng Li",
    "subjects": "Systems and Control (eess.SY)",
    "abstract": "This review analysis presents a comprehensive exploration of energy flexibility in modern power systems. It examines the roles and mechanisms of flexible technologies across three main categories: generators, energy storage systems (ESS), and loads. Energy flexibility is defined as the ability to dynamically adjust supply and/or demand in response to grid conditions to maintain balance and stability. This is of particular importance to facilitate the integration of the growing variable renewable energy sources (RES) into modern power grids. Additionally, traditional supply-side mechanisms to maintain balance and stability are complemented by advancements in demand-side management and demand response strategies, which enable loads to adjust consumption patterns and schedules in response to grid requirements. ESS are also explored to further enhance flexibility by absorbing excess generation and/or supplying large load increases that are not able to be met by the less flexible resources. This paper also explores specific flexibility technologies, examining their characteristics, control strategies, advantages, and limitations. Energy flexibility services are also categorized into intermittency mitigation, peak shaving, and energy reserve provisioning. Each service is supported by case studies and examples demonstrating how different resources respond to varying conditions. Ultimately, the findings and reviews of the various flexible resources in this paper provide a roadmap for optimizing energy flexibility across diverse resource types, paving the way for a more sustainable and resilient energy future.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14335",
    "url": "https://arxiv.org/abs/2508.14335",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.347913",
    "title": "The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations",
    "authors": "Hailong Su, Jinshu Su, Yusheng Xia, Haibin Li",
    "subjects": "Networking and Internet Architecture (cs.NI); Social and Information Networks (cs.SI)",
    "abstract": "In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14817",
    "url": "https://arxiv.org/abs/2508.14817",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.413925",
    "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs",
    "authors": "Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14832",
    "url": "https://arxiv.org/abs/2508.14832",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.415768",
    "title": "On Defining Neural Averaging",
    "authors": "Su Hyeong Lee, Richard Ngo",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "What does it even mean to average neural networks? We investigate the problem of synthesizing a single neural network from a collection of pretrained models, each trained on disjoint data shards, using only their final weights and no access to training data. In forming a definition of neural averaging, we take insight from model soup, which appears to aggregate multiple models into a singular model while enhancing generalization performance. In this work, we reinterpret model souping as a special case of a broader framework: Amortized Model Ensembling (AME) for neural averaging, a data-free meta-optimization approach that treats model differences as pseudogradients to guide neural weight updates. We show that this perspective not only recovers model soup but enables more expressive and adaptive ensembling strategies. Empirically, AME produces averaged neural solutions that outperform both individual experts and model soup baselines, especially in out-of-distribution settings. Our results suggest a principled and generalizable notion of data-free model weight aggregation and defines, in one sense, how to perform neural averaging.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14848",
    "url": "https://arxiv.org/abs/2508.14848",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.415768",
    "title": "Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach",
    "authors": "Qiao Zhang, Rabab Alomairy, Dali Wang, Zhuowei Gu, Qinglei Cao",
    "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)",
    "abstract": "General Matrix Multiplication (GEMM) is a critical operation underpinning a wide range of applications in high-performance computing (HPC) and artificial intelligence (AI). The emergence of hardware optimized for low-precision arithmetic necessitates a reevaluation of numerical algorithms to leverage mixed-precision computations, achieving improved performance and energy efficiency. This research introduces an adaptive mixed-precision GEMM framework that supports different precision formats at fine-grained tile/block levels. We utilize the PaRSEC runtime system to balance workloads across various architectures. The performance scales well on ARM CPU-based Fugaku supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier supercomputer. This research aims to enhance computational efficiency and accuracy by bridging algorithmic advancements and hardware innovations, driving transformative progress in various applications.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14048",
    "url": "https://arxiv.org/abs/2508.14048",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.423478",
    "title": "RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition",
    "authors": "Pengcheng Wang, Sheng Li, Takahiro Shinozaki",
    "subjects": "Audio and Speech Processing (eess.AS); Computation and Language (cs.CL)",
    "abstract": "In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I) with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR hypothesis queries a vector store of audio-text pairs and domain terms, and the retrieved results are fused with the live ASR hypotheses to fix recognition errors. The fused hypotheses are passed to the LLM, yielding improved responses.",
    "comments": "accepted at Interspeech2025 MLC-SLM Challenge workshop (task I system description)",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2502.11277",
    "url": "https://arxiv.org/abs/2502.11277",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.462881",
    "title": "\"When I lost it, they dragged me out\": How Care Encounters Empower Marginalized Young Adults' Aspiration and Mental Health Care-Seeking",
    "authors": "Jiaying \"Lizzy\" Liu, Yan Zhang",
    "subjects": "Human-Computer Interaction (cs.HC)",
    "abstract": "Mental health care-seeking among marginalized young adults has received limited attention in CSCW research. Through in-depth interviews and visual elicitation methods with 18 diverse U.S. participants, our study reveals how marginalized identities shape mental health care-seeking journeys, often characterized by low aspirations and passive care-seeking influenced by lived experiences of marginalization. However, we found the transformative function of \"care encounters\" - serendipitous interactions with mental health resources that occur when individuals are not actively seeking support. These encounters serve as critical turning points, catalyzing shifts in aspiration and enabling more proactive care-seeking behaviors. Our analysis identifies both the infrastructural conditions that enable transformative care encounters and the aspiration breakdowns that impede care-seeking processes. This work makes conceptual contributions by supplementing traditional motivation-based care-seeking models with a reconceptualization of \"care encounters\" that accounts for the infrastructural and serendipitous nature of mental health access. We advance understanding of how marginalized identity uniquely influences care-seeking behaviors while providing actionable design implications for embedding technology-mediated \"care encounters\" into socio-technical interventions that can better support mental health care access for vulnerable populations.",
    "comments": "Accepted by CSCW 25. arXiv admin note: text overlap with arXiv:2401.08994",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2507.03608",
    "url": "https://arxiv.org/abs/2507.03608",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.486325",
    "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)",
    "authors": "Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi",
    "subjects": "Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)",
    "abstract": "Generative AI (GenAI) is expected to play a pivotal role in enabling autonomous optimization in future wireless networks. Within the ORAN architecture, Large Language Models (LLMs) can be specialized to generate xApps and rApps by leveraging specifications and API definitions from the RAN Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for telecom-specific tasks remains expensive and resource-intensive. Retrieval-Augmented Generation (RAG) offers a practical alternative through in-context learning, enabling domain adaptation without full retraining. While traditional RAG systems rely on vector-based retrieval, emerging variants such as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval strategies to support multi-hop reasoning and improve factual grounding. Despite their promise, these methods lack systematic, metric-driven evaluations, particularly in high-stakes domains such as ORAN. In this study, we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid GraphRAG using ORAN specifications. We assess performance across varying question complexities using established generation metrics: faithfulness, answer relevance, context relevance, and factual correctness. Results show that both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG improves factual correctness by 8%, while GraphRAG improves context relevance by 11%.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2507.17442",
    "url": "https://arxiv.org/abs/2507.17442",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.493427",
    "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
    "authors": "Shiting Chen, Zijian Zhao, Jinsong Chen",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  }
]