[
  {
    "id": "2508.14076",
    "url": "https://arxiv.org/abs/2508.14076",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.312125",
    "title": "PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning",
    "authors": "Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14080",
    "url": "https://arxiv.org/abs/2508.14080",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.312125",
    "title": "KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge",
    "authors": "Guanghao Jin, Jingpei Wu, Tianpei Guo, Yiyi Niu, Weidong Zhou, Guoyang Liu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Referring Expression Comprehension (REC) is a popular multimodal task that aims to accurately detect target objects within a single image based on a given textual expression. However, due to the limitations of earlier models, traditional REC benchmarks either rely solely on intra-image cues or lack sufficiently fine-grained instance annotations, making them inadequate for evaluating the reasoning capabilities of Multi-modal Large Language Models (MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC, characterized by three key features: Firstly, it is built upon real-world knowledge, requiring fine-grained multimodal reasoning across text and image. Secondly, the dataset includes elaborately constructed negative samples via fine-grained expression editing, designed to evaluate a model's robustness and anti-hallucination ability. Lastly, we introduce three novel evaluation metrics to systematically explore the model's internal reasoning process. We evaluate 16 state-of-the-art multimodal models on KnowDR-REC, with experimental results showing that existing MLLMs still struggle with knowledge-driven visual grounding tasks. Furthermore, we observe a decoupling between textual understanding and visual grounding in MLLMs, where many models are significantly influenced by memorized shortcut correlations, which severely affect their behavior on our benchmark and hinder genuine multimodal reasoning. We anticipate that the proposed benchmark will inspire future research towards developing more robust, interpretable, and knowledge-intensive visual grounding frameworks, driving the development of more reliable and robust multimodal systems for complex real-world scenarios.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14119",
    "url": "https://arxiv.org/abs/2508.14119",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.320128",
    "title": "Documenting Deployment with Fabric: A Repository of Real-World AI Governance",
    "authors": "Mackenzie Jorgensen, Kendall Brogle, Katherine M. Collins, Lujain Ibrahim, Arina Shah, Petra Ivanovic, Noah Broestl, Gabriel Piles, Paul Dongha, Hatim Abdulhussein, Adrian Weller, Jillian Powers, Umang Bhatt",
    "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
    "abstract": "Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.",
    "comments": "AIES 2025",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14120",
    "url": "https://arxiv.org/abs/2508.14120",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.320128",
    "title": "SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning",
    "authors": "Yuhang Lin, Yijia Xie, Jiahong Xie, Yuehao Huang, Ruoyu Wang, Jiajun Lv, Yukai Ma, Xingxing Zuo",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
    "abstract": "Generating physically realistic humanoid-object interactions (HOI) is a fundamental challenge in robotics. Existing HOI generation approaches, such as diffusion-based models, often suffer from artifacts such as implausible contacts, penetrations, and unrealistic whole-body actions, which hinder successful execution in physical environments. To address these challenges, we introduce SimGenHOI, a unified framework that combines the strengths of generative modeling and reinforcement learning to produce controllable and physically plausible HOI. Our HOI generative model, based on Diffusion Transformers (DiT), predicts a set of key actions conditioned on text prompts, object geometry, sparse object waypoints, and the initial humanoid pose. These key actions capture essential interaction dynamics and are interpolated into smooth motion trajectories, naturally supporting long-horizon generation. To ensure physical realism, we design a contact-aware whole-body control policy trained with reinforcement learning, which tracks the generated motions while correcting artifacts such as penetration and foot sliding. Furthermore, we introduce a mutual fine-tuning strategy, where the generative model and the control policy iteratively refine each other, improving both motion realism and tracking robustness. Extensive experiments demonstrate that SimGenHOI generates realistic, diverse, and physically plausible humanoid-object interactions, achieving significantly higher tracking success rates in simulation and enabling long-horizon manipulation tasks. Code will be released upon acceptance on our project page: this https URL.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14287",
    "url": "https://arxiv.org/abs/2508.14287",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.341577",
    "title": "Nearly Tight Bounds for the Online Sorting Problem",
    "authors": "Yossi Azar, Debmalya Panigrahi, Or Vardi",
    "subjects": "Data Structures and Algorithms (cs.DS)",
    "abstract": "In the online sorting problem, a sequence of $n$ numbers in $[0, 1]$ (including $\\{0,1\\}$) have to be inserted in an array of size $m \\ge n$ so as to minimize the sum of absolute differences between pairs of numbers occupying consecutive non-empty cells. Previously, Aamand {\\em et al.} (SODA 2023) gave a deterministic $2^{\\sqrt{\\log n} \\sqrt{\\log \\log n + \\log (1/\\varepsilon)}}$-competitive algorithm when $m = (1+\\varepsilon) n$ for any $\\varepsilon \\ge \\Omega(\\log n/n)$. They also showed a lower bound: with $m = \\gamma n$ space, the competitive ratio of any deterministic algorithm is at least $\\frac{1}{\\gamma}\\cdot\\Omega(\\log n / \\log \\log n)$. This left an exponential gap between the upper and lower bounds for the problem.\nIn this paper, we bridge this exponential gap and almost completely resolve the online sorting problem. First, we give a deterministic $O(\\log^2 n / \\varepsilon)$-competitive algorithm with $m = (1+\\varepsilon) n$, for any $\\varepsilon \\ge \\Omega(\\log n / n)$. Next, for $m = \\gamma n$ where $\\gamma = [O(1), O(\\log^2 n)]$, we give a deterministic $O(\\log^2 n / \\gamma)$-competitive algorithm. In particular, this implies an $O(1)$-competitive algorithm with $O(n \\log^2 n)$ space, which is within an $O(\\log n\\cdot \\log \\log n)$ factor of the lower bound of $\\Omega(n \\log n / \\log \\log n)$. Combined, the two results imply a close to optimal tradeoff between space and competitive ratio for the entire range of interest: specifically, an upper bound of $O(\\log^2 n)$ on the product of the competitive ratio and $\\gamma$ while the lower bound on this product is $\\Omega(\\log n / \\log\\log n)$. We also show that these results can be extended to the case when the range of the numbers is not known in advance, for an additional $O(\\log n)$ factor in the competitive ratio.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14313",
    "url": "https://arxiv.org/abs/2508.14313",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.344622",
    "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS",
    "authors": "Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14340",
    "url": "https://arxiv.org/abs/2508.14340",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.348973",
    "title": "A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations",
    "authors": "Konur Tholl, Mariam El Mezouar, Ranwa Al Mallah",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to train agents to make effective decisions in the cybersecurity domain. However, existing ACO applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. While teacher-guided techniques have demonstrated promise in other domains, they have not yet been applied to ACO. In this study, we implement four distinct teacher-guided techniques in the simulated CybORG environment and conduct a comparative evaluation. Our results demonstrate that teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14411",
    "url": "https://arxiv.org/abs/2508.14411",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.359366",
    "title": "A Real-world Display Inverse Rendering Dataset",
    "authors": "Seokjun Choi, Hoon-Gyu Chung, Yujin Jeon, Giljoo Nam, Seung-Hwan Baek",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Inverse rendering aims to reconstruct geometry and reflectance from captured images. Display-camera imaging systems offer unique advantages for this task: each pixel can easily function as a programmable point light source, and the polarized light emitted by LCD displays facilitates diffuse-specular separation. Despite these benefits, there is currently no public real-world dataset captured using display-camera systems, unlike other setups such as light stages. This absence hinders the development and evaluation of display-based inverse rendering methods. In this paper, we introduce the first real-world dataset for display-based inverse rendering. To achieve this, we construct and calibrate an imaging system comprising an LCD display and stereo polarization cameras. We then capture a diverse set of objects with diverse geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We also provide high-quality ground-truth geometry. Our dataset enables the synthesis of captured images under arbitrary display patterns and different noise levels. Using this dataset, we evaluate the performance of existing photometric stereo and inverse rendering methods, and provide a simple, yet effective baseline for display inverse rendering, outperforming state-of-the-art inverse rendering methods. Code and dataset are available on our project page at this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14466",
    "url": "https://arxiv.org/abs/2508.14466",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.369298",
    "title": "LookOut: Real-World Humanoid Egocentric Navigation",
    "authors": "Boxiao Pan, Adam W. Harley, C. Karen Liu, Leonidas J. Guibas",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "The ability to predict collision-free future trajectories from egocentric observations is crucial in applications such as humanoid robotics, VR / AR, and assistive navigation. In this work, we introduce the challenging problem of predicting a sequence of future 6D head poses from an egocentric video. In particular, we predict both head translations and rotations to learn the active information-gathering behavior expressed through head-turning events. To solve this task, we propose a framework that reasons over temporally aggregated 3D latent features, which models the geometric and semantic constraints for both the static and dynamic parts of the environment. Motivated by the lack of training data in this space, we further contribute a data collection pipeline using the Project Aria glasses, and present a dataset collected through this approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4 hours of recording of users navigating in real-world scenarios. It includes diverse situations and navigation behaviors, providing a valuable resource for learning real-world egocentric navigation policies. Extensive experiments show that our model learns human-like navigation behaviors such as waiting / slowing down, rerouting, and looking around for traffic while generalizing to unseen environments. Check out our project webpage at this https URL.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14606",
    "url": "https://arxiv.org/abs/2508.14606",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.390813",
    "title": "Approximating 1-in-3 SAT by linearly ordered hypergraph 3-colouring is NP-hard",
    "authors": "Andrei Krokhin, Danny Vagnozzi",
    "subjects": "Computational Complexity (cs.CC)",
    "abstract": "Given a satisfiable instance of 1-in-3 SAT, it is NP-hard to find a satisfying assignment for it, but it may be possible to efficiently find a solution subject to a weaker (not necessarily Boolean) predicate than `1-in-3'. There is a folklore conjecture predicting which choices of weaker predicates lead to tractability and for which the task remains \\NP-hard. One specific predicate, corresponding to the problem of linearly ordered $3$-colouring of 3-uniform hypergraphs, has been mentioned in several recent papers as an obstacle to further progress in proving this conjecture. We prove that the problem for this predicate is NP-hard, as predicted by the conjecture.\nWe use the Promise CSP framework, where the complexity analysis is performed via the algebraic approach, by studying the structure of polymorphisms, which are multidimensional invariants of the problem at hand. The analysis of polymorphisms is in general a highly non-trivial task, and topological combinatorics was recently discovered to provide a useful tool for this. There are two distinct ways in which it was used: one is based on variations of the Borsuk-Ulam theorem, and the other aims to classify polymorphisms up to certain reconfigurations (homotopy). Our proof, whilst combinatorial in nature, shows that our problem is the first example where the features behind the two uses of topology appear together. Thus, it is likely to be useful in guiding further development of the topological method aimed at classifying Promise CSPs. An easy consequence of our result is the hardness of another specific Promise CSP, which was recently proved by Filakovský et al. by employing a deep topological analysis of polymorphisms.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14676",
    "url": "https://arxiv.org/abs/2508.14676",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.396132",
    "title": "Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach",
    "authors": "Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani",
    "subjects": "Networking and Internet Architecture (cs.NI)",
    "abstract": "Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14679",
    "url": "https://arxiv.org/abs/2508.14679",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.396132",
    "title": "Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach",
    "authors": "Parham Soltani, Mehrshad Eskandarpour, Amir Ahmadizad, Hossein Soleimani",
    "subjects": "Networking and Internet Architecture (cs.NI)",
    "abstract": "Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14765",
    "url": "https://arxiv.org/abs/2508.14765",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.407875",
    "title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning",
    "authors": "Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14786",
    "url": "https://arxiv.org/abs/2508.14786",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.410011",
    "title": "Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns",
    "authors": "Veronika Ivanova, Evgeny Frolov, Alexey Vasilev",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "We consider the task of learning from both positive and negative feedback in a sequential recommendation scenario, as both types of feedback are often present in user interactions. Meanwhile, conventional sequential learning models usually focus on considering and predicting positive interactions, ignoring that reducing items with negative feedback in recommendations improves user satisfaction with the service. Moreover, the negative feedback can potentially provide a useful signal for more accurate identification of true user interests. In this work, we propose to train two transformer encoders on separate positive and negative interaction sequences. We incorporate both types of feedback into the training objective of the sequential recommender using a composite loss function that includes positive and negative cross-entropy as well as a cleverly crafted contrastive term, that helps better modeling opposing patterns. We demonstrate the effectiveness of this approach in terms of increasing true-positive metrics compared to state-of-the-art sequential recommendation methods while reducing the number of wrongly promoted negative items.",
    "matched_keyword": "ppo",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14881",
    "url": "https://arxiv.org/abs/2508.14881",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.420576",
    "title": "Compute-Optimal Scaling for Value-Based Deep RL",
    "authors": "Preston Fu, Oleh Rybkin, Zhiyuan Zhou, Michal Nauman, Pieter Abbeel, Sergey Levine, Aviral Kumar",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "As models grow larger and training them becomes expensive, it becomes increasingly important to scale training recipes not just to larger models and more data, but to do so in a compute-optimal manner that extracts maximal performance per unit of compute. While such scaling has been well studied for language modeling, reinforcement learning (RL) has received less attention in this regard. In this paper, we investigate compute scaling for online, value-based deep RL. These methods present two primary axes for compute allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed compute budget, we ask: how should resources be partitioned across these axes to maximize sample efficiency? Our analysis reveals a nuanced interplay between model size, batch size, and UTD. In particular, we identify a phenomenon we call TD-overfitting: increasing the batch quickly harms Q-function accuracy for small models, but this effect is absent in large models, enabling effective use of large batch size at scale. We provide a mental model for understanding this phenomenon and build guidelines for choosing batch size and UTD to optimize compute usage. Our findings provide a grounded starting point for compute-optimal scaling in deep RL, mirroring studies in supervised learning but adapted to TD learning.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14893",
    "url": "https://arxiv.org/abs/2508.14893",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.422663",
    "title": "Virtual Community: An Open World for Humans, Robots, and Society",
    "authors": "Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Robotics (cs.RO)",
    "abstract": "The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.",
    "comments": "website this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2208.07563",
    "url": "https://arxiv.org/abs/2208.07563",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.441273",
    "title": "Reinforcement Learning to Rank Using Coarse-grained Rewards",
    "authors": "Yiteng Tu, Zhichao Xu, Tao Yang, Weihang Su, Yujia Zhou, Yiqun Liu, Fen Lin, Qin Liu, Qingyao Ai",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Learning to rank (LTR) plays a crucial role in various Information Retrieval (IR) tasks. Although supervised LTR methods based on fine-grained relevance labels (e.g., document-level annotations) have achieved significant success, their reliance on costly and potentially biased annotations limits scalability and alignment with realistic goals. In contrast, coarse-grained feedback signals, such as duration time and session-level engagement, are more accessible and affordable. Reinforcement Learning (RL) offers a promising framework to directly optimize these objectives using reward signals, but most existing Reinforcement Learning to Rank (RLTR) approaches suffer from high variance and low sample efficiency. Motivated by recent advances in large language models (LLMs), we re-examine the problem of RLTR with coarse-grained rewards and propose new RLTR methods based on widely used RL algorithms for LLMs. We systematically compare supervised learning and RL-based methods across various model architectures and coarse-grained reward functions on large-scale LTR benchmarks. Experimental results demonstrate that advanced RL methods can directly learn from coarse-grained rewards and outperform strong supervised learning baselines even with fine-grained labels. This shows the great potential of RLTR for metric-agnostic ranking optimization.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2406.05881",
    "url": "https://arxiv.org/abs/2406.05881",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.448588",
    "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning",
    "authors": "Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri",
    "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Robotics (cs.RO)",
    "abstract": "Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2407.16661",
    "url": "https://arxiv.org/abs/2407.16661",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.450665",
    "title": "Regenerative Ulam-von Neumann Algorithm: An Innovative Markov chain Monte Carlo Method for Matrix Inversion",
    "authors": "Soumyadip Ghosh, Lior Horesh, Vassilis Kalantzis, Yingdong Lu, Tomasz Nowicki",
    "subjects": "Numerical Analysis (math.NA); Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)",
    "abstract": "This paper presents a regenerative variant of the classical Ulam-von Neumann Markov chain Monte Carlo algorithm for the approximation of the matrix inverse. The algorithm presented in this paper, termed regenerative Ulam-von Neumann algorithm, utilizes the regenerative structure of classical, non-truncated Neumann series defined by a non-singular matrix and produces an estimator of the matrix inverse via ratios of unbiased estimators of the regenerative quantities. The accuracy of the proposed algorithm depends on a single parameter that controls the total number of simulated Markov transitions, thus avoiding the challenge of balancing between the total number of Markov chain replications and their length as in the classical Ulam-von Neumann algorithm. To efficiently utilize Markov chain transition samples in the calculation of the regenerative variables, the proposed algorithm automatically quantifies the contribution of each Markov transition to all regenerative quantities by a carefully designed updating scheme that utilized three separate matrices containing the current weights, total weights, and regenerative cycle count, respectively. A probabilistic analysis of the performance of the algorithm, including the variance of the estimator, is provided. Finally, numerical experiments verify the effectiveness of the proposed scheme.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2410.03136",
    "url": "https://arxiv.org/abs/2410.03136",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.453709",
    "title": "Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model",
    "authors": "Siheng Xiong, Ali Payani, Yuan Yang, Faramarz Fekri",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.",
    "comments": "ACL25 (main)",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2410.03844",
    "url": "https://arxiv.org/abs/2410.03844",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.453709",
    "title": "Projected Walk on Spheres: A Monte Carlo Closest Point Method for Surface PDEs",
    "authors": "Ryusuke Sugimoto, Nathan King, Toshiya Hachisuka, Christopher Batty",
    "subjects": "Numerical Analysis (math.NA); Graphics (cs.GR)",
    "abstract": "We present projected walk on spheres (PWoS), a novel pointwise and discretization-free Monte Carlo solver for surface PDEs with Dirichlet boundaries, as a generalization of the walk on spheres method (WoS) [Muller 1956; Sawhney and Crane 2020]. We adapt the recursive relationship of WoS designed for PDEs in volumetric domains to a volumetric neighborhood around the surface, and at the end of each recursion step, we project the sample point on the sphere back to the surface. We motivate this simple modification to WoS with the theory of the closest point extension used in the closest point method. To define the valid volumetric neighborhood domain for PWoS, we develop strategies to estimate the local feature size of the surface and to compute the distance to the Dirichlet boundaries on the surface extended in their normal directions. We also design a mean value filtering method for PWoS to improve the method's efficiency when the surface is represented as a polygonal mesh or a point cloud. Finally, we study the convergence of PWoS and demonstrate its application to graphics tasks, including diffusion curves, geodesic distance computation, and wave propagation animation. We show that our method works with various types of surfaces, including a surface of mixed codimension.",
    "comments": "Accepted to SIGGRAPH Asia 2024 (Conference Papers). See this https URL for updates",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2410.23805",
    "url": "https://arxiv.org/abs/2410.23805",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.455735",
    "title": "UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM Architecture",
    "authors": "Sitian Chen, Amelie Chi Zhou, Yucheng Shi, Yusen Li, Xin Yao",
    "subjects": "Hardware Architecture (cs.AR)",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) is a critical component of modern AI systems, such as recommendation engines and retrieval-augmented large language models (RAG-LLMs). However, scaling ANNS to billion-entry datasets exposes critical inefficiencies: CPU-based solutions are bottlenecked by memory bandwidth limitations, while GPU implementations underutilize hardware resources, leading to suboptimal performance and energy consumption. To address these challenges, we introduce \\emph{UpANNS}, a novel framework leveraging Processing-in-Memory (PIM) architecture to accelerate billion-scale ANNS. UpANNS integrates four key innovations, including 1) architecture-aware data placement to minimize latency through workload balancing, 2) dynamic resource management for optimal PIM utilization, 3) co-occurrence optimized encoding to reduce redundant computations, and 4) an early-pruning strategy for efficient top-k selection. Evaluation on commercial UPMEM hardware demonstrates that UpANNS achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU performance with 2.3x greater energy efficiency. Its near-linear scalability ensures practicality for growing datasets, making it ideal for applications like real-time LLM serving and large-scale retrieval systems.",
    "comments": "Accepted by SC 25",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2501.08096",
    "url": "https://arxiv.org/abs/2501.08096",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.459817",
    "title": "Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving",
    "authors": "Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, Chen Sun",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
    "abstract": "Reinforcement Learning (RL) has shown excellent performance in solving decision-making and control problems of autonomous driving, which is increasingly applied in diverse driving scenarios. However, driving is a multi-attribute problem, leading to challenges in achieving multi-objective compatibility for current RL methods, especially in both policy updating and policy execution. On the one hand, a single value evaluation network limits the policy updating in complex scenarios with coupled driving objectives. On the other hand, the common single-type action space structure limits driving flexibility or results in large behavior fluctuations during policy execution. To this end, we propose a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action for multi-objective compatible autonomous driving. Specifically, an advanced MORL architecture is constructed, in which the ensemble-critic focuses on different objectives through independent reward functions. The architecture integrates a hybrid parameterized action space structure, and the generated driving actions contain both abstract guidance that matches the hybrid road modality and concrete control commands. Additionally, an uncertainty-based exploration mechanism that supports hybrid actions is developed to learn multi-objective compatible policies more quickly. Experimental results demonstrate that, in both simulator-based and HighD dataset-based multi-lane highway scenarios, our method efficiently learns multi-objective compatible autonomous driving with respect to efficiency, action consistency, and safety.",
    "comments": "13 pages, 10 figures, 5 tables, Submitted to IEEE T-NNLS (under review, 2nd round)",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2505.19717",
    "url": "https://arxiv.org/abs/2505.19717",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.473200",
    "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning",
    "authors": "Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the minimum or maximum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: this https URL",
    "comments": "2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids), Sep 2025, Seoul, South Korea",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.18897",
    "url": "https://arxiv.org/abs/2506.18897",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.481377",
    "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
    "authors": "Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
    "abstract": "Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.21205",
    "url": "https://arxiv.org/abs/2506.21205",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.483825",
    "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations",
    "authors": "Elia Trevisan, Khaled A. Mustafa, Godert Notten, Xinwei Wang, Javier Alonso-Mora",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Deploying mobile robots safely among humans requires the motion planner to account for the uncertainty in the other agents' predicted trajectories. This remains challenging in traditional approaches, especially with arbitrarily shaped predictions and real-time constraints. To address these challenges, we propose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI), a motion planner that incorporates uncertain future motions modelled with potentially non-Gaussian stochastic predictions. By leveraging MPPI's gradient-free nature, we propose a method that efficiently approximates the joint Collision Probability (CP) among multiple dynamic obstacles for several hundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This enables the rejection of samples exceeding a predefined CP threshold or the integration of CP as a weighted objective within the navigation cost function. Consequently, DRA-MPPI mitigates the freezing robot problem while enhancing safety. Real-world and simulated experiments with multiple dynamic obstacles demonstrate DRA-MPPI's superior performance compared to state-of-the-art approaches, including Scenario-based Model Predictive Control (S-MPC), Frenet planner, and vanilla MPPI.",
    "comments": "Accepted for presentation at IROS 2025. Accepted Version",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.23036",
    "url": "https://arxiv.org/abs/2506.23036",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.485807",
    "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress",
    "authors": "Zain ul Abdeen, Ming Jin",
    "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
    "abstract": "This paper explores Reinforcement learning (RL) policy robustness by systematically analyzing network parameters under internal and external stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering introduces internal stress by selectively perturbing parameters, while adversarial attacks apply external stress through modified agent observations. This dual approach enables the classification of parameters as fragile, robust, or antifragile, based on their influence on policy performance in clean and adversarial settings. Parameter scores are defined to quantify these characteristics, and the framework is validated on PPO-trained agents in Mujoco continuous control environments. The results highlight the presence of antifragile parameters that enhance policy performance under stress, demonstrating the potential of targeted filtering techniques to improve RL policy adaptability. These insights provide a foundation for future advancements in the design of robust and antifragile RL systems.",
    "comments": "Withdrawn pending a review of attribution and overlap with Pravin et al., Artificial Intelligence (2024), DOI: https://doi.org/10.1016/j.artint.2023.104060. Further dissemination is paused while we determine appropriate next steps",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2507.12911",
    "url": "https://arxiv.org/abs/2507.12911",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.491729",
    "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR",
    "authors": "Hayeon Oh",
    "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
    "abstract": "Out-of-distribution (OOD) scenarios in autonomous driving pose critical challenges, as planners often fail to generalize beyond their training experience, leading to unsafe or unexpected behavior. Vision-Language Models (VLMs) have shown promise in handling such scenarios by providing high-level scene understanding and user-aligned decisions. However, existing VLMs often exhibit a misalignment between their language-based reasoning and the low-level trajectories required for action-level planning. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to fine-tune VLMs using planning-oriented metrics. Experimental results show that LaViPlan improves planning performance across both in-domain and out-of-domain datasets. While linguistic fidelity slightly decreases after RLVR-based fine-tuning, qualitative evaluation indicates that the outputs remain coherent. We also conduct ablation studies to analyze the effects of sampling ratio and reasoning guidance, highlighting how these design choices influence performance. These findings demonstrate the potential of RLVR as a post-training paradigm for aligning language-guided reasoning with action-level planning in autonomous driving.",
    "comments": "Accepted to the 2nd ICCV 2025 Workshop on the Challenge of Out-of-Label Hazards in Autonomous Driving (13 pages, 6 figures)",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.02091",
    "url": "https://arxiv.org/abs/2508.02091",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.496447",
    "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
    "authors": "Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)",
    "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at this https URL",
    "comments": "Preprint Version",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.04941",
    "url": "https://arxiv.org/abs/2508.04941",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.496447",
    "title": "Toward Errorless Training ImageNet-1k",
    "authors": "Bo Deng, Levi Heath",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
    "abstract": "In this paper, we describe a feedforward artificial neural network trained on the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are perfectly classified over the 10 batch partitions of the dataset. The best performing model uses 322,430,160 parameters, with 4 decimal places precision. We conjecture that the reason our model does not achieve a 100% accuracy rate is due to a double-labeling problem, by which there are duplicate images in the dataset with different labels.",
    "comments": "14 pages, 2 figures, 5 tables",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.09128",
    "url": "https://arxiv.org/abs/2508.09128",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.500452",
    "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions",
    "authors": "Dhruv Singh Kushwaha, Zoleikha Abdollahi Biron",
    "subjects": "Systems and Control (eess.SY)",
    "abstract": "Reinforcement learning (RL) has proven to be particularly effective in solving complex decision-making problems for a wide range of applications. From a control theory perspective, RL can be considered as an adaptive optimal control scheme. Lyapunov and barrier functions are the most commonly used certificates to guarantee system stability for a proposed/derived controller and constraint satisfaction guarantees, respectively, in control theoretic approaches. However, compared to theoretical guarantees available in control theoretic methods, RL lacks closed-loop stability of a computed policy and constraint satisfaction guarantees. Safe reinforcement learning refers to a class of constrained problems where the constraint violations lead to partial or complete system failure. The goal of this review is to provide an overview of safe RL techniques using Lyapunov and barrier functions to guarantee this notion of safety discussed (stability of the system in terms of a computed policy and constraint satisfaction during training and deployment). The different approaches employed are discussed in detail along with their shortcomings and benefits to provide critique and possible future research directions. Key motivation for this review is to discuss current theoretical approaches for safety and stability guarantees in RL similar to control theoretic approaches using Lyapunov and barrier functions. The review provides proven potential and promising scope of providing safety guarantees for complex dynamical systems with operational constraints using model-based and model-free RL.",
    "comments": "pages - 19, figures - 9, Submitted to IEEE TAI",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.12769",
    "url": "https://arxiv.org/abs/2508.12769",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.509850",
    "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
    "authors": "Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Peng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14029",
    "url": "https://arxiv.org/abs/2508.14029",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.516248",
    "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR",
    "authors": "Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, Weizhu Chen",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2407.11353",
    "url": "https://arxiv.org/abs/2407.11353",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.519453",
    "title": "Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early-Stopping",
    "authors": "Yingzhen Yang, Ping Li",
    "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
    "abstract": "We study nonparametric regression using an over-parameterized two-layer neural networks trained with algorithmic guarantees in this paper. We consider the setting where the training features are drawn uniformly from the unit sphere in $\\mathbb{R}^d$, and the target function lies in an interpolation space commonly studied in statistical learning theory. We demonstrate that training the neural network with a novel Preconditioned Gradient Descent (PGD) algorithm, equipped with early stopping, achieves a sharp regression rate of $\\mathcal O(n^{-\\frac{2\\alpha s'}{2\\alpha s'+1}})$ when the target function is in the interpolation space $[\\mathcal H_K]^{s'}$ with $s' \\ge 3$. This rate is even sharper than the currently known nearly-optimal rate of $\\mathcal O(n^{-\\frac{2\\alpha s'}{2\\alpha s'+1}})\\log^2(1/\\delta)$~\\citep{Li2024-edr-general-domain}, where $n$ is the size of the training data and $\\delta \\in (0,1)$ is a small probability. This rate is also sharper than the standard kernel regression rate of $\\mathcal O(n^{-\\frac{2\\alpha}{2\\alpha+1}})$ obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent (GD), where $2\\alpha = d/(d-1)$. Our analysis is based on two key technical contributions. First, we present a principled decomposition of the network output at each PGD step into a function in the reproducing kernel Hilbert space (RKHS) of a newly induced integral kernel, and a residual function with small $L^{\\infty}$-norm. Second, leveraging this decomposition, we apply local Rademacher complexity theory to tightly control the complexity of the function class comprising all the neural network functions obtained in the PGD iterates. Our results further suggest that PGD enables the neural network to escape the linear NTK regime and achieve improved generalization by inducing a new integral kernel of lower kernel complexity.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.07614",
    "url": "https://arxiv.org/abs/2506.07614",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.521472",
    "title": "Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds",
    "authors": "Rishikesh Srinivasan, Dheeraj Nagaraj",
    "subjects": "Probability (math.PR); Machine Learning (cs.LG); Statistics Theory (math.ST)",
    "abstract": "We study the problem of sampling from strongly log-concave distributions over $\\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.",
    "matched_keyword": "dpo",
    "matched_category": "强化学习"
  }
]