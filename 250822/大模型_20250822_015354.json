[
  {
    "id": "2508.14056",
    "url": "https://arxiv.org/abs/2508.14056",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.308111",
    "title": "Confidence Estimation for Text-to-SQL in Large Language Models",
    "authors": "Sepideh Entezari Maleki, Mohammadreza Pourreza, Davood Rafiei",
    "subjects": "Computation and Language (cs.CL); Databases (cs.DB)",
    "abstract": "Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14061",
    "url": "https://arxiv.org/abs/2508.14061",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.308111",
    "title": "GPT-2 as a Compression Preprocessor: Improving Gzip for Structured Text Domains",
    "authors": "Anurag Kumar Ojha",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "In the modern era, large volumes of data are being produced continuously, especially in domain-specific fields such as medical records and clinical files, defence logs and HTML-based web traffic. Data with such volume and complexity needs to be compressed before storing and transmitting efficiently. Data compression has gained significant attention from modern researchers, resulting in the development of fast and efficient compression algorithms such as Gzip. However, since gzip works on the principle of repetition of binary patterns, one of the limitations of gzip is that domain-specific formats like JSON, XML, HTML, and log files, while structured, may have semantic repetition but not syntactic repetition, which gzip finds difficult to compress. In this article, we propose a GPT-based preprocessor for such domain-specific files. We propose a pipeline made up of GPT-2 taking domain-specific files as input, which pattern-based compressors like gzip find difficult to work on. The preprocessor results are output in a file that is designed for compressors like gzip. After preprocessing, the gzip works on the other end of the pipeline and compresses the data as usual. We used different types of both real-world and synthetically generated data, such as logs and HTML files, for the experiment of the proposed model. We found promising results and an improvement of the Defence logs by 0.34 per cent and HTML files by 5.8 per cent.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14062",
    "url": "https://arxiv.org/abs/2508.14062",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.310119",
    "title": "Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models",
    "authors": "Badrinath Ramakrishnan, Akshaya Balaji",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.",
    "comments": "14 pages, 2 figures. Code and experimental framework available at this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14064",
    "url": "https://arxiv.org/abs/2508.14064",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.310119",
    "title": "An automatic patent literature retrieval system based on LLM-RAG",
    "authors": "Yao Ding, Yuqing Wu, Ziyang Ding",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "With the acceleration of technological innovation efficient retrieval and classification of patent literature have become essential for intellectual property management and enterprise RD Traditional keyword and rulebased retrieval methods often fail to address complex query intents or capture semantic associations across technical domains resulting in incomplete and lowrelevance results This study presents an automated patent retrieval framework integrating Large Language Models LLMs with RetrievalAugmented Generation RAG technology The system comprises three components: 1) a preprocessing module for patent data standardization, 2) a highefficiency vector retrieval engine leveraging LLMgenerated embeddings, and 3) a RAGenhanced query module that combines external document retrieval with contextaware response generation Evaluations were conducted on the Google Patents dataset 20062024 containing millions of global patent records with metadata such as filing date domain and status The proposed gpt35turbo0125RAG configuration achieved 805 semantic matching accuracy and 92.1% recall surpassing baseline LLM methods by 28 percentage points The framework also demonstrated strong generalization in crossdomain classification and semantic clustering tasks These results validate the effectiveness of LLMRAG integration for intelligent patent retrieval providing a foundation for nextgeneration AIdriven intellectual property analysis platforms",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14090",
    "url": "https://arxiv.org/abs/2508.14090",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.316128",
    "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models",
    "authors": "Chen Xu, Dawei Yang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.",
    "comments": "12 pages, 6 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14128",
    "url": "https://arxiv.org/abs/2508.14128",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.320128",
    "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection",
    "authors": "Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, Ioannis Ch. Paschalidis",
    "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
    "abstract": "Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs' vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.",
    "comments": "11 pages, 1 figure",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14190",
    "url": "https://arxiv.org/abs/2508.14190",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.328331",
    "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text",
    "authors": "Zixin Rao, Youssef Mohamed, Shang Liu, Zeyan Liu",
    "subjects": "Cryptography and Security (cs.CR); Computation and Language (cs.CL); Machine Learning (cs.LG)",
    "abstract": "Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.",
    "comments": "Securecomm 2025",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14214",
    "url": "https://arxiv.org/abs/2508.14214",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.328331",
    "title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli",
    "authors": "Mattson Ogg, Chace Ashcraft, Ritwik Bose, Raphael Norman-Tenazas, Michael Wolmetz",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14273",
    "url": "https://arxiv.org/abs/2508.14273",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.337520",
    "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper",
    "authors": "Krishna Garg, Firoz Shaikh, Sambaran Bandyopadhyay, Cornelia Caragea",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMs' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.",
    "comments": "20 pages, 15 figures",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14279",
    "url": "https://arxiv.org/abs/2508.14279",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.339543",
    "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs",
    "authors": "Adrian-Marius Dumitran, Alexandra-Mihaela Danila, Angela-Liliana Dumitran",
    "subjects": "Computation and Language (cs.CL); Computers and Society (cs.CY)",
    "abstract": "LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.",
    "comments": "Accepted as long paper @RANLP2025",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14285",
    "url": "https://arxiv.org/abs/2508.14285",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.340569",
    "title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models",
    "authors": "Liyi Zhang, Jake Snell, Thomas L. Griffiths",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
    "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.",
    "comments": "6 pages, 2 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14288",
    "url": "https://arxiv.org/abs/2508.14288",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.341577",
    "title": "Measuring LLM Code Generation Stability via Structural Entropy",
    "authors": "Yewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Rajput, Tegawende F. Bissyande, Jacques Klein",
    "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL)",
    "abstract": "Assessing the stability of code generation from large language models (LLMs) is essential for judging their reliability in real-world development. We extend prior \"structural-entropy concepts\" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural overlap, and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading LLMs on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit.",
    "comments": "ASE-NIER",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14302",
    "url": "https://arxiv.org/abs/2508.14302",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.343602",
    "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation",
    "authors": "Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14314",
    "url": "https://arxiv.org/abs/2508.14314",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.344622",
    "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency",
    "authors": "Aman Goel, Daniel Schwartz, Yanjun Qi",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14377",
    "url": "https://arxiv.org/abs/2508.14377",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.354015",
    "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities",
    "authors": "Wenhan Dong, Zhen Sun, Yuemeng Zhao, Zifan Peng, Jun Wu, Jingyi Zheng, Yule Liu, Xinlei He, Yu Wang, Ruiming Wang, Xinyi Huang, Lei Mo",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
    "abstract": "Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14387",
    "url": "https://arxiv.org/abs/2508.14387",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.356027",
    "title": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models",
    "authors": "Yuxiao Zhu, Junfeng Chen, Xintong Zhang, Meng Guo, Zhongkui Li",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Online coordination of multi-robot systems in open and unknown environments faces significant challenges, particularly when semantic features detected during operation dynamically trigger new tasks. Recent large language model (LLMs)-based approaches for scene reasoning and planning primarily focus on one-shot, end-to-end solutions in known environments, lacking both dynamic adaptation capabilities for online operation and explainability in the processes of planning. To address these issues, a novel framework (DEXTER-LLM) for dynamic task planning in unknown environments, integrates four modules: (i) a mission comprehension module that resolves partial ordering of tasks specified by natural languages or linear temporal logic formulas (LTL); (ii) an online subtask generator based on LLMs that improves the accuracy and explainability of task decomposition via multi-stage reasoning; (iii) an optimal subtask assigner and scheduler that allocates subtasks to robots via search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop verification module that implements multi-rate, event-based updates for both subtasks and their assignments, to cope with new features and tasks detected online. The framework effectively combines LLMs' open-world reasoning capabilities with the optimality of model-based assignment methods, simultaneously addressing the critical issue of online adaptability and explainability. Experimental evaluations demonstrate exceptional performances, with 100% success rates across all scenarios, 160 tasks and 480 subtasks completed on average (3 times the baselines), 62% less queries to LLMs during adaptation, and superior plan quality (2 times higher) for compound tasks. Project page at this https URL",
    "comments": "submitted to IROS 2025",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14390",
    "url": "https://arxiv.org/abs/2508.14390",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.357028",
    "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play",
    "authors": "Ke Fang, Tianyi Zhao, Lu Cheng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at this https URL.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14408",
    "url": "https://arxiv.org/abs/2508.14408",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.359366",
    "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs",
    "authors": "Yinghan Zhou, Weifeng Zhu, Juan Wen, Wanli Peng, Zhengxian Wu, Yiming Xue",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14410",
    "url": "https://arxiv.org/abs/2508.14410",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.359366",
    "title": "Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning",
    "authors": "Beinuo Yang, Qishen Zhou, Junyi Li, Xingchen Su, Simon Hu",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42\\%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14419",
    "url": "https://arxiv.org/abs/2508.14419",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.360379",
    "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness",
    "authors": "Scott Blyth, Sherlock A. Licorish, Christoph Treude, Markus Wagner",
    "subjects": "Software Engineering (cs.SE)",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in code generation, achieving high scores on benchmarks such as HumanEval and MBPP. However, these benchmarks primarily assess functional correctness and neglect broader dimensions of code quality, including security, reliability, readability, and maintainability. In this work, we systematically evaluate the ability of LLMs to generate high-quality code across multiple dimensions using the PythonSecurityEval benchmark. We introduce an iterative static analysis-driven prompting algorithm that leverages Bandit and Pylint to identify and resolve code quality issues. Our experiments with GPT-4o show substantial improvements: security issues reduced from >40% to 13%, readability violations from >80% to 11%, and reliability warnings from >50% to 11% within ten iterations. These results demonstrate that LLMs, when guided by static analysis feedback, can significantly enhance code quality beyond functional correctness.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14427",
    "url": "https://arxiv.org/abs/2508.14427",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.361626",
    "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models",
    "authors": "Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14460",
    "url": "https://arxiv.org/abs/2508.14460",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.367773",
    "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization",
    "authors": "Shuaijie She, Yu Bao, Yu Lu, Lu Xu, Tao Li, Wenhao Zhu, Shujian Huang, Shanbo Cheng, Lu Lu, Yuxuan Wang",
    "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
    "abstract": "We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.",
    "comments": "18 pages, 4 figures,",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14496",
    "url": "https://arxiv.org/abs/2508.14496",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.373370",
    "title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy",
    "authors": "Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, Haifeng Wang",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14537",
    "url": "https://arxiv.org/abs/2508.14537",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.382611",
    "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion",
    "authors": "Yonghan Shin, SeungKyu Kim, Won-Ki Jeong",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Whole slide images (WSIs) in computational pathology (CPath) pose a major computational challenge due to their gigapixel scale, often requiring the processing of tens to hundreds of thousands of high-resolution patches per slide. This results in prohibitive encoding costs, with preprocessing and training times extending to days or even weeks-making WSI encoding the most significant bottleneck in real-world deployment. In this work, we propose WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain vision-language models and large language models to address this challenge by selectively processing diagnostically relevant regions. WISE-FUSE first computes similarity scores between low-resolution patches and class-specific textual descriptions using a knowledge distillation mechanism that preserves fine-grained diagnostic features. Based on these similarity scores, we select a small subset of informative regions for the target task, which quickly eliminates irrelevant patches at the coarse level. The corresponding high-resolution patches are then selectively encoded and fused with textual embeddings to reinforce diagnostic context. Extensive experiments demonstrate that WISE-FUSE reduces WSI encoding time by over threefold while achieving diagnostic performance comparable to or surpassing that of exhaustive patch processing, offering a scalable and practical solution for CPath.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14540",
    "url": "https://arxiv.org/abs/2508.14540",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.383624",
    "title": "Post-hoc LLM-Supported Debugging of Distributed Processes",
    "authors": "Dennis Schiese, Andreas Both",
    "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
    "abstract": "In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.",
    "comments": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14544",
    "url": "https://arxiv.org/abs/2508.14544",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.383624",
    "title": "Adaptively Robust LLM Inference Optimization under Prediction Uncertainty",
    "authors": "Zixi Chen, Yinyu Ye, Zijie Zhou",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
    "abstract": "We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.\nWe first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14553",
    "url": "https://arxiv.org/abs/2508.14553",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.384656",
    "title": "Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems",
    "authors": "Dennis Schiese, Aleksandr Perevalov, Andreas Both",
    "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
    "abstract": "Over time, software systems have reached a level of complexity that makes it difficult for their developers and users to explain particular decisions made by them. In this paper, we focus on the explainability of component-based systems for Question Answering (QA). These components often conduct processes driven by AI methods, in which behavior and decisions cannot be clearly explained or justified, s.t., even for QA experts interpreting the executed process and its results is hard. To address this challenge, we present an approach that considers the components' input and output data flows as a source for representing the behavior and provide explanations for the components, enabling users to comprehend what happened. In the QA framework used here, the data flows of the components are represented as SPARQL queries (inputs) and RDF triples (outputs). Hence, we are also providing valuable insights on verbalization regarding these data types. In our experiments, the approach generates explanations while following template-based settings (baseline) or via the use of Large Language Models (LLMs) with different configurations (automatic generation). Our evaluation shows that the explanations generated via LLMs achieve high quality and mostly outperform template-based approaches according to the users' ratings. Therefore, it enables us to automatically explain the behavior and decisions of QA components to humans while using RDF and SPARQL as a context for explanations.",
    "comments": "Presented at ICWI 2024, Zagreb. Released with ISBN: 978-989-8704-62-7. Data source: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14564",
    "url": "https://arxiv.org/abs/2508.14564",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.386726",
    "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs",
    "authors": "Luca Annese, Sabrina Patania, Silvia Serino, Tom Foulsham, Silvia Rossi, Azzurra Ruggeri, Dimitri Ognibene",
    "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
    "abstract": "Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.",
    "comments": "Accepted at ICSR25",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14692",
    "url": "https://arxiv.org/abs/2508.14692",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.400137",
    "title": "Sociotechnical Imaginaries of ChatGPT in Higher Education: The Evolving Media Discourse",
    "authors": "Yinan Sun, Ali Unlu, Aditya Johri",
    "subjects": "Computers and Society (cs.CY)",
    "abstract": "This study investigates how U.S. news media framed the use of ChatGPT in higher education from November 2022 to October 2024. Employing Framing Theory and combining temporal and sentiment analysis of 198 news articles, we trace the evolving narratives surrounding generative AI. We found that the media discourse largely centered on institutional responses; policy changes and teaching practices showed the most consistent presence and positive sentiment over time. Conversely, coverage of topics such as human-centered learning, the job market, and skill development appeared more sporadically, with initially uncertain portrayals gradually shifting toward cautious optimism. Importantly, media sentiment toward ChatGPT's role in college admissions remained predominantly negative. Our findings suggest that media narratives prioritize institutional responses to generative AI over long-term, broader ethical, social, and labor-related implications, shaping an emerging sociotechnical imaginary that frames generative AI in education primarily through the lens of adaptation and innovation.",
    "comments": "Under review at a conference",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14704",
    "url": "https://arxiv.org/abs/2508.14704",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.400137",
    "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers",
    "authors": "Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li",
    "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.",
    "comments": "Website: this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14735",
    "url": "https://arxiv.org/abs/2508.14735",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.404783",
    "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference",
    "authors": "Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: this https URL",
    "comments": "Under review",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14782",
    "url": "https://arxiv.org/abs/2508.14782",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.408498",
    "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting",
    "authors": "Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at this https URL.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14853",
    "url": "https://arxiv.org/abs/2508.14853",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.416774",
    "title": "Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent",
    "authors": "Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14879",
    "url": "https://arxiv.org/abs/2508.14879",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.419451",
    "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "authors": "Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14130",
    "url": "https://arxiv.org/abs/2508.14130",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.430052",
    "title": "EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition",
    "authors": "Hugo Thimonier, Antony Perzo, Renaud Seguier",
    "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG)",
    "abstract": "Emotion recognition from speech is a challenging task that requires capturing both linguistic and paralinguistic cues, with critical applications in human-computer interaction and mental health monitoring. Recent works have highlighted the ability of Large Language Models (LLMs) to perform tasks outside of the sole natural language area. In particular, recent approaches have investigated coupling LLMs with other data modalities by using pre-trained backbones and different fusion mechanisms. This work proposes a novel approach that fine-tunes an LLM with audio and text representations for emotion prediction. Our method first extracts audio features using an audio feature extractor, which are then mapped into the LLM's representation space via a learnable interfacing module. The LLM takes as input (1) the transformed audio features, (2) additional features in the form of natural language (e.g., the transcript), and (3) a textual prompt describing the emotion prediction task. To efficiently adapt the LLM to this multimodal task, we employ Low-Rank Adaptation (LoRA), enabling parameter-efficient fine-tuning. Experimental results on standard emotion recognition benchmarks demonstrate that our model outperforms all but one existing Speech-Text LLMs in the literature, while requiring less than half the parameters of competing approaches. This highlights our approach's effectiveness in integrating multi-modal inputs for speech-based emotion understanding while maintaining significant computational efficiency.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14755",
    "url": "https://arxiv.org/abs/2508.14755",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.437796",
    "title": "Reliable generation of isomorphic physics problems using ChatGPT with prompt-chaining and tool use",
    "authors": "Zhongzhou Chen",
    "subjects": "Physics Education (physics.ed-ph); Artificial Intelligence (cs.AI)",
    "abstract": "We present a method for generating large numbers of isomorphic physics problems using ChatGPT through prompt chaining and tool use. This approach enables precise control over structural variations-such as numeric values and spatial relations-while supporting diverse contextual variations in the problem body. By utilizing the Python code interpreter, the method supports automatic solution validation and simple diagram generation, addressing key limitations in existing LLM-based methods. We generated two example isomorphic problem banks and compared the outcome against simpler prompt-based approaches. Results show that prompt-chaining produces significantly higher quality and more consistent outputs than simpler, non-chaining prompts. This work demonstrates a promising method for efficient problem creation accessible to the average instructor, which opens new possibilities for personalized adaptive testing and automated content development.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14869",
    "url": "https://arxiv.org/abs/2508.14869",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.440250",
    "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models",
    "authors": "Hend Al-Khalifa, Raneem Almansour, Layan Abdulrahman Alhuasini, Alanood Alsaleh, Mohamad-Hani Temsah, Mohamad-Hani_Temsah, Ashwag Rafea S Alruwaili",
    "subjects": "Neurons and Cognition (q-bio.NC); Computation and Language (cs.CL)",
    "abstract": "Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2312.11370",
    "url": "https://arxiv.org/abs/2312.11370",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.443251",
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "authors": "Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.",
    "comments": "10 pages",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2403.17983",
    "url": "https://arxiv.org/abs/2403.17983",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.445421",
    "title": "Is The Watermarking Of LLM-Generated Code Robust?",
    "authors": "Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic",
    "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
    "abstract": "We present the first in depth study on the robustness of existing watermarking techniques applied to code generated by large language models (LLMs). As LLMs increasingly contribute to software development, watermarking has emerged as a potential solution for detecting AI generated code and mitigating misuse, such as plagiarism or the automated generation of malicious programs. While previous research has demonstrated the resilience of watermarking in the text setting, our work reveals that watermarking techniques are significantly more fragile in code-based contexts. Specifically, we show that simple semantic-preserving transformations, such as variable renaming and dead code insertion, can effectively erase watermarks without altering the program's functionality. To systematically evaluate watermark robustness, we develop an algorithm that traverses the Abstract Syntax Tree (AST) of a watermarked program and applies a sequence of randomized, semantics-preserving transformations. Our experimental results, conducted on Python code generated by different LLMs, indicate that even minor modifications can drastically reduce watermark detectability, with true positive rates (TPR) dropping below 50% in many cases. Our code is publicly available at this https URL.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2408.06569",
    "url": "https://arxiv.org/abs/2408.06569",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.451685",
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "authors": "Harry Cheng, Yangyang Guo, Qingpei Guo, Ming Yang, Tian Gan, Weili Guan, Liqiang Nie",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks.",
    "comments": "Project page: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2411.02433",
    "url": "https://arxiv.org/abs/2411.02433",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.455735",
    "title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models",
    "authors": "Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (Gemma, Qwen, Mixtral, gpt-oss) and scales (from 1B to 45B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks and the results demonstrate that SLED consistently improves factual accuracy compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.",
    "comments": "Accepted at NeurIPS 2024; project page is available at this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2502.02329",
    "url": "https://arxiv.org/abs/2502.02329",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.461862",
    "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs",
    "authors": "Yuan Tian, Chuhan Zhang, Xiaotong Wang, Sitong Pan, Weiwei Cui, Haidong Zhang, Dazhen Deng, Yingcai Wu",
    "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL)",
    "abstract": "Creating data reports is a labor-intensive task involving iterative data exploration, insight extraction, and narrative construction. A key challenge lies in composing the analysis logic-from defining objectives and transforming data to identifying and communicating insights. Manually crafting this logic can be cognitively demanding. While experienced analysts often reuse scripts from past projects, finding a perfect match for a new dataset is rare. Even when similar analyses are available online, they usually share only results or visualizations, not the underlying code, making reuse difficult. To address this, we present ReSpark, a system that leverages large language models (LLMs) to reverse-engineer analysis logic from existing reports and adapt it to new datasets. By generating draft analysis steps, ReSpark provides a warm start for users. It also supports interactive refinement, allowing users to inspect intermediate outputs, insert objectives, and revise content. We evaluate ReSpark through comparative and user studies, demonstrating its effectiveness in lowering the barrier to generating data reports without relying on existing analysis code.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2502.13953",
    "url": "https://arxiv.org/abs/2502.13953",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.462881",
    "title": "Benchmarking graph construction by large language models for coherence-driven inference",
    "authors": "Steve Huntsman, Jewell Thomas",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "We devise an algorithm to generate propositions that objectively instantiate graphs supporting coherence-driven inference. We also benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a simple transformation of) propositions expressed in natural language, with promising results from a single prompt to reasoning-optimized LLMs. For example, o1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs. Coherence-driven inference on consistency evaluations by LLMs may advance machine cognition capabilities.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2503.24157",
    "url": "https://arxiv.org/abs/2503.24157",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.465675",
    "title": "LLM4FS: Leveraging Large Language Models for Feature Selection",
    "authors": "Jianhao Li, Xianchao Xiu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making. Our code is available at this https URL.",
    "comments": "CAC 2025",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.01519",
    "url": "https://arxiv.org/abs/2504.01519",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.465675",
    "title": "Chain of Correction for Full-text Speech Recognition with Large Language Models",
    "authors": "Zhiyuan Tang, Dong Wang, Zhikai Zhou, Yong Liu, Shen Huang, Shidong Shang",
    "subjects": "Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)",
    "abstract": "Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) is attracting increased attention for its ability to address a wide range of error types, such as punctuation restoration and inverse text normalization, across long context. However, challenges remain regarding stability, controllability, completeness, and fluency. To mitigate these issues, this paper proposes the Chain of Correction (CoC), which uses a multi-turn chat format to correct errors segment by segment, guided by pre-recognized text and full-text context for better semantic understanding. Utilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to evaluate CoC's performance. Experiments show that CoC significantly outperforms baseline and benchmark systems in correcting full-text ASR outputs. We also analyze correction thresholds to balance under-correction and over-rephrasing, extrapolate CoC on extra-long ASR outputs, and explore using other types of information to guide error correction.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.05846",
    "url": "https://arxiv.org/abs/2504.05846",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.469182",
    "title": "PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models",
    "authors": "Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Path recommendation (PR) aims to generate travel paths that are customized to a user's specific preferences and constraints. Conventional approaches often employ explicit optimization objectives or specialized machine learning architectures; however, these methods typically exhibit limited flexibility and generalizability, necessitating costly retraining to accommodate new scenarios. This paper introduces an alternative paradigm that conceptualizes PR as a natural language generation task. We present PathGPT, a retrieval-augmented large language model (LLM) system that leverages historical trajectory data and natural language user constraints to generate plausible paths. The proposed methodology first converts raw trajectory data into a human-interpretable textual format, which is then stored in a database. Subsequently, a hybrid retrieval system extracts path-specific context from this database to inform a pretrained LLM. The primary contribution of this work is a novel framework that demonstrates how integrating established information retrieval and generative model components can enable adaptive, zero-shot path generation across diverse scenarios. Extensive experiments on large-scale trajectory datasets indicate that PathGPT's performance is competitive with specialized, learning-based methods, underscoring its potential as a flexible and generalizable path generation system that avoids the need for retraining inherent in previous data-driven models.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2504.19061",
    "url": "https://arxiv.org/abs/2504.19061",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.469182",
    "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models",
    "authors": "Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
    "abstract": "Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, including admission reasons, major in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. Our results reveal that while the LLMs (e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission reasons and hospitalization events, they are generally less consistent when it comes to identifying follow-up recommendations, highlighting broader challenges in leveraging LLMs for comprehensive summarization.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.19254",
    "url": "https://arxiv.org/abs/2504.19254",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.469182",
    "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers",
    "authors": "Dylan Bouchard, Mohit Singh Chauhan",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.",
    "comments": "UQLM repository: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2504.19959",
    "url": "https://arxiv.org/abs/2504.19959",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.473200",
    "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL Verification",
    "authors": "Junhao Ye, Yuchen Hu, Ke Xu, Dingrong Pan, Qichun Chen, Jie Zhou, Shuai Zhao, Xinwei Fang, Xi Wang, Nan Guan, Zhe Jiang",
    "subjects": "Hardware Architecture (cs.AR)",
    "abstract": "Verification presents a major bottleneck in Integrated Circuit (IC) development, consuming nearly 70% of the total development effort. While the Universal Verification Methodology (UVM) is widely used in industry to improve verification efficiency through structured and reusable testbenches, constructing these testbenches and generating sufficient stimuli remain challenging. These challenges arise from the considerable manual coding effort required, repetitive manual execution of multiple EDA tools, and the need for in-depth domain expertise to navigate complex this http URL, we present UVM^2, an automated verification framework that leverages Large Language Models (LLMs) to generate UVM testbenches and iteratively refine them using coverage feedback, significantly reducing manual effort while maintaining rigorous verification this http URL evaluate UVM^2, we introduce a benchmark suite comprising Register Transfer Level (RTL) designs of up to 1.6K lines of this http URL results show that UVM^2 reduces testbench setup time by up to UVM^2 compared to experienced engineers, and achieve average code and function coverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by 20.96% and 23.51%, respectively.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2505.18889",
    "url": "https://arxiv.org/abs/2505.18889",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.473200",
    "title": "Security Concerns for Large Language Models: A Survey",
    "authors": "Miles Q. Li, Benjamin C. M. Fung",
    "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: prompt injection and jailbreaking; adversarial attacks, including input perturbations and data poisoning; misuse by malicious actors to generate disinformation, phishing emails, and malware; and the worrisome risks inherent in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2505.24773",
    "url": "https://arxiv.org/abs/2505.24773",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.477196",
    "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption",
    "authors": "Yajie Zhou, Xiaoyi Pang, Zhibo Wang",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2506.03106",
    "url": "https://arxiv.org/abs/2506.03106",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.477196",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "authors": "Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.",
    "comments": "52 pages, updated with new experimental results and implementation details",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2506.11091",
    "url": "https://arxiv.org/abs/2506.11091",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.480729",
    "title": "Customizing Speech Recognition Model with Large Language Model Feedback",
    "authors": "Shaoshi Ling, Guoli Ye",
    "subjects": "Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
    "abstract": "Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\\% improvement on entity word error rate over conventional self-training methods.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2506.13790",
    "url": "https://arxiv.org/abs/2506.13790",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.480729",
    "title": "The NordDRG AI Benchmark for Large Language Models",
    "authors": "Tapio Pitkäranta",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) are being piloted for clinical coding and decision support, yet no open benchmark targets the hospital-funding layer where Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD systems, DRGs route a substantial share of multi-trillion-dollar health spending through governed grouper software, making transparency and auditability first-order concerns. We release NordDRG-AI-Benchmark, the first public, rule-complete test bed for DRG reasoning. The package includes (i) machine-readable approximately 20-sheet NordDRG definition tables and (ii) expert manuals and change-log templates that capture governance workflows. It exposes two suites: a 13-task Logic benchmark (code lookup, cross-table inference, grouping features, multilingual terminology, and CC/MCC validity checks) and a 13-task Grouper benchmark that requires full DRG grouper emulation with strict exact-match scoring on both the DRG and the triggering this http URL. Lightweight reference agents (LogicAgent, GrouperAgent) enable artefact-only evaluation. Under an artefact-only (no web) setting, on the 13 Logic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier models (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining models score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5 Thinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13, and all other tested endpoints score 0/13. To our knowledge, this is the first public report of an LLM partially emulating the complete NordDRG grouper logic with governance-grade traceability. Coupling a rule-complete release with exact-match tasks and open scoring provides a reproducible yardstick for head-to-head and longitudinal evaluation in hospital funding. Benchmark materials available in Github.",
    "comments": "23 pages, 4 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2507.03047",
    "url": "https://arxiv.org/abs/2507.03047",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.486325",
    "title": "Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning",
    "authors": "Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
    "abstract": "Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.\nTo address this critical gap, we propose \\underline{C}ounterfactual \\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based \\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at this https URL.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2507.10016",
    "url": "https://arxiv.org/abs/2507.10016",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.490023",
    "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents",
    "authors": "Lixu Wang, Kaixiang Yao, Xinfeng Li, Dong Yang, Haoyang Li, Xiaofeng Wang, Wei Dong",
    "subjects": "Cryptography and Security (cs.CR); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
    "abstract": "Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.",
    "comments": "22 pages, 4 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.03082",
    "url": "https://arxiv.org/abs/2508.03082",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.496447",
    "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design",
    "authors": "Fei Liu, Yilu Liu, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a small-sized complementary heuristic set to serve diverse problem instances, such that each problem instance could be optimized by at least one heuristic in this set. We show that the objective function of AHSD is monotone and supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary population management and complementary-aware memetic search, EoH-S could effectively generate a set of high-quality and complementary heuristics. Comprehensive experimental results on three AHD tasks with diverse instances spanning various sizes and distributions demonstrate that EoH-S consistently outperforms existing state-of-the-art AHD methods and achieves up to 60\\% performance improvements.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.08066",
    "url": "https://arxiv.org/abs/2508.08066",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.500452",
    "title": "ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
    "authors": "Weitai Kang, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
    "abstract": "Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
    "comments": "8 pages for the main paper",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.10142",
    "url": "https://arxiv.org/abs/2508.10142",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.500452",
    "title": "Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs",
    "authors": "Kartikeya Badola, Jonathan Simon, Arian Hosseini, Sara Marie Mc Carthy, Tsendsuren Munkhdalai, Abhimanyu Goyal, Tomáš Kočiský, Shyam Upadhyay, Bahare Fatemi, Mehran Kazemi",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.12096",
    "url": "https://arxiv.org/abs/2508.12096",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.505331",
    "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples",
    "authors": "Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \\textbf{S}tructured \\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \\textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.",
    "comments": "Submit to AAAI 2026",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.12645",
    "url": "https://arxiv.org/abs/2508.12645",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.509850",
    "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation",
    "authors": "Hongyang Liu, Zhu Sun, Tianjun Wei, Yan Wang, Jiajie Zhu, Xinghua Qu",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Recent advances in large language models (LLMs) have enabled realistic user simulators for developing and evaluating recommender systems (RSs). However, existing LLM-based simulators for RSs face two major limitations: (1) static and single-step prompt-based inference that leads to inaccurate and incomplete user profile construction; (2) unrealistic and single-round recommendation-feedback interaction pattern that fails to capture real-world scenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided Dynamic Profile Optimization), a novel framework that constructs user profile through a dynamic and iterative optimization process to enhance the simulation fidelity. Specifically, DGDPO incorporates two core modules within each optimization loop: firstly, a specialized LLM-based diagnostic module, calibrated through our novel training strategy, accurately identifies specific defects in the user profile. Subsequently, a generalized LLM-based treatment module analyzes the diagnosed defect and generates targeted suggestions to refine the profile. Furthermore, unlike existing LLM-based user simulators that are limited to single-round interactions, we are the first to integrate DGDPO with sequential recommenders, enabling a bidirectional evolution where user profiles and recommendation strategies adapt to each other over multi-round interactions. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of our proposed framework.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.13968",
    "url": "https://arxiv.org/abs/2508.13968",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.516248",
    "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
    "authors": "Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.",
    "comments": "20 pages. Code and data: this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2507.07060",
    "url": "https://arxiv.org/abs/2507.07060",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:53:52.522487",
    "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning",
    "authors": "Shreyas Vinaya Sathyanarayana, Sharanabasava D. Hiremath, Rahil Shah, Rishikesh Panda, Rahul Jana, Riya Singh, Rida Irfan, Ashwin Murali, Bharath Ramsundar",
    "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Biomolecules (q-bio.BM); Molecular Networks (q-bio.MN)",
    "abstract": "The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.",
    "comments": "64 pages,",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  }
]