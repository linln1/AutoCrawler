[
  {
    "id": "2508.14052",
    "url": "https://arxiv.org/abs/2508.14052",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.320959",
    "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering",
    "authors": "Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&P 500 and beyond.",
    "comments": "6 pages",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14053",
    "url": "https://arxiv.org/abs/2508.14053",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.320959",
    "title": "MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging",
    "authors": "Jinwei Tang, Jiayin Qin, Nuo Xu, Pragnya Sudershan Nalla, Yu Cao, Yang, Zhao, Caiwen Ding",
    "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
    "abstract": "As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14056",
    "url": "https://arxiv.org/abs/2508.14056",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.320959",
    "title": "Confidence Estimation for Text-to-SQL in Large Language Models",
    "authors": "Sepideh Entezari Maleki, Mohammadreza Pourreza, Davood Rafiei",
    "subjects": "Computation and Language (cs.CL); Databases (cs.DB)",
    "abstract": "Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14058",
    "url": "https://arxiv.org/abs/2508.14058",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.320959",
    "title": "Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks",
    "authors": "Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "The explosive growth of the video game industry has created an urgent need for recommendation systems that can scale with expanding catalogs and maintain user engagement. While prior work has explored accuracy and diversity in recommendations, existing models underutilize playtime, a rich behavioral signal unique to gaming platforms, and overlook the potential of multimodal information to enhance diversity. In this paper, we propose DP2Rec, a novel Dual-Phase Playtime-guided Recommendation model designed to jointly optimize accuracy and diversity. First, we introduce a playtime-guided interest intensity exploration module that separates strong and weak preferences via dual-beta modeling, enabling fine-grained user profiling and more accurate recommendations. Second, we present a playtime-guided multimodal random walks module that simulates player exploration using transitions guided by both playtime-derived interest similarity and multimodal semantic similarity. This mechanism preserves core preferences while promoting cross-category discovery through latent semantic associations and adaptive category balancing. Extensive experiments on a real-world game dataset show that DP2Rec outperforms existing methods in both recommendation accuracy and diversity.",
    "comments": "Accepted for publication at ACM Multimedia (ACM MM) 2025. 10 pages, 5 figures. Code and dataset: this https URL",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14061",
    "url": "https://arxiv.org/abs/2508.14061",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.320959",
    "title": "GPT-2 as a Compression Preprocessor: Improving Gzip for Structured Text Domains",
    "authors": "Anurag Kumar Ojha",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "In the modern era, large volumes of data are being produced continuously, especially in domain-specific fields such as medical records and clinical files, defence logs and HTML-based web traffic. Data with such volume and complexity needs to be compressed before storing and transmitting efficiently. Data compression has gained significant attention from modern researchers, resulting in the development of fast and efficient compression algorithms such as Gzip. However, since gzip works on the principle of repetition of binary patterns, one of the limitations of gzip is that domain-specific formats like JSON, XML, HTML, and log files, while structured, may have semantic repetition but not syntactic repetition, which gzip finds difficult to compress. In this article, we propose a GPT-based preprocessor for such domain-specific files. We propose a pipeline made up of GPT-2 taking domain-specific files as input, which pattern-based compressors like gzip find difficult to work on. The preprocessor results are output in a file that is designed for compressors like gzip. After preprocessing, the gzip works on the other end of the pipeline and compresses the data as usual. We used different types of both real-world and synthetically generated data, such as logs and HTML files, for the experiment of the proposed model. We found promising results and an improvement of the Defence logs by 0.34 per cent and HTML files by 5.8 per cent.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14062",
    "url": "https://arxiv.org/abs/2508.14062",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models",
    "authors": "Badrinath Ramakrishnan, Akshaya Balaji",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.",
    "comments": "14 pages, 2 figures. Code and experimental framework available at this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14063",
    "url": "https://arxiv.org/abs/2508.14063",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "A Multi-Agent Approach to Neurological Clinical Reasoning",
    "authors": "Moran Sorka, Alon Gorenshtein, Dvir Aran, Shahar Shelly",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) have shown promise in medical domains, but their ability to handle specialized neurological reasoning requires systematic evaluation. We developed a comprehensive benchmark using 305 questions from Israeli Board Certification Exams in Neurology, classified along three complexity dimensions: factual knowledge depth, clinical concept integration, and reasoning complexity. We evaluated ten LLMs using base models, retrieval-augmented generation (RAG), and a novel multi-agent system. Results showed significant performance variation. OpenAI-o1 achieved the highest base performance (90.9% accuracy), while specialized medical models performed poorly (52.9% for Meditron-70B). RAG provided modest benefits but limited effectiveness on complex reasoning questions. In contrast, our multi-agent framework, decomposing neurological reasoning into specialized cognitive functions including question analysis, knowledge retrieval, answer synthesis, and validation, achieved dramatic improvements, especially for mid-range models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus 69.5% for its base model, with substantial gains on level 3 complexity questions. The multi-agent approach transformed inconsistent subspecialty performance into uniform excellence, addressing neurological reasoning challenges that persisted with RAG enhancement. We validated our approach using an independent dataset of 155 neurological cases from MedQA. Results confirm that structured multi-agent approaches designed to emulate specialized cognitive processes significantly enhance complex medical reasoning, offering promising directions for AI assistance in challenging clinical contexts.",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14064",
    "url": "https://arxiv.org/abs/2508.14064",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "An automatic patent literature retrieval system based on LLM-RAG",
    "authors": "Yao Ding, Yuqing Wu, Ziyang Ding",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "With the acceleration of technological innovation efficient retrieval and classification of patent literature have become essential for intellectual property management and enterprise RD Traditional keyword and rulebased retrieval methods often fail to address complex query intents or capture semantic associations across technical domains resulting in incomplete and lowrelevance results This study presents an automated patent retrieval framework integrating Large Language Models LLMs with RetrievalAugmented Generation RAG technology The system comprises three components: 1) a preprocessing module for patent data standardization, 2) a highefficiency vector retrieval engine leveraging LLMgenerated embeddings, and 3) a RAGenhanced query module that combines external document retrieval with contextaware response generation Evaluations were conducted on the Google Patents dataset 20062024 containing millions of global patent records with metadata such as filing date domain and status The proposed gpt35turbo0125RAG configuration achieved 805 semantic matching accuracy and 92.1% recall surpassing baseline LLM methods by 28 percentage points The framework also demonstrated strong generalization in crossdomain classification and semantic clustering tasks These results validate the effectiveness of LLMRAG integration for intelligent patent retrieval providing a foundation for nextgeneration AIdriven intellectual property analysis platforms",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14066",
    "url": "https://arxiv.org/abs/2508.14066",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation",
    "authors": "Lorenz Brehme, Benedikt Dornauer, Thomas Ströhle, Maximilian Ehrhart, Ruth Breu",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "Retrieval-Augmented Generation (RAG) is a well-established and rapidly evolving field within AI that enhances the outputs of large language models by integrating relevant information retrieved from external knowledge sources. While industry adoption of RAG is now beginning, there is a significant lack of research on its practical application in industrial contexts. To address this gap, we conducted a semistructured interview study with 13 industry practitioners to explore the current state of RAG adoption in real-world settings. Our study investigates how companies apply RAG in practice, providing (1) an overview of industry use cases, (2) a consolidated list of system requirements, (3) key challenges and lessons learned from practical experiences, and (4) an analysis of current industry evaluation methods. Our main findings show that current RAG applications are mostly limited to domain-specific QA tasks, with systems still in prototype stages; industry requirements focus primarily on data protection, security, and quality, while issues such as ethics, bias, and scalability receive less attention; data preprocessing remains a key challenge, and system evaluation is predominantly conducted by humans rather than automated methods.",
    "comments": "This preprint was accepted for presentation at the 17th International Conference on Knowledge Discovery and Information Retrieval (KDIR25)",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14072",
    "url": "https://arxiv.org/abs/2508.14072",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration",
    "authors": "Anabel Yong",
    "subjects": "Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)",
    "abstract": "We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm that advances the state-of-the-art in molecular optimization. Our approach integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of efficiently handling the full dimensionality of sparse molecular fingerprints without the need for extensive computational resources. GP-MOBO consistently outperforms traditional methods like GP-BO by fully leveraging fingerprint dimensionality, leading to the identification of higher-quality and valid SMILES. Moreover, our model achieves a broader exploration of the chemical search space, as demonstrated by its superior proximity to the Pareto front in all tested scenarios. Empirical results from the DockSTRING dataset reveal that GP-MOBO yields higher geometric mean values across 20 Bayesian optimization iterations, underscoring its effectiveness and efficiency in addressing complex multi-objective optimization challenges with minimal computational overhead.",
    "comments": "Masters of Science thesis",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2508.14076",
    "url": "https://arxiv.org/abs/2508.14076",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.324941",
    "title": "PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning",
    "authors": "Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14077",
    "url": "https://arxiv.org/abs/2508.14077",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.328905",
    "title": "Label Smoothing is a Pragmatic Information Bottleneck",
    "authors": "Sota Kudo",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "This study revisits label smoothing via a form of information bottleneck. Under the assumption of sufficient model flexibility and no conflicting labels for the same input, we theoretically and experimentally demonstrate that the model output obtained through label smoothing explores the optimal solution of the information bottleneck. Based on this, label smoothing can be interpreted as a practical approach to the information bottleneck, enabling simple implementation. As an information bottleneck method, we experimentally show that label smoothing also exhibits the property of being insensitive to factors that do not contain information about the target, or to factors that provide no additional information about it when conditioned on another variable.",
    "comments": "18 pages, 8 figures, published in Transactions on Machine Learning Research (TMLR), 2025",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14079",
    "url": "https://arxiv.org/abs/2508.14079",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.328905",
    "title": "A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy",
    "authors": "Maxime Heuillet, Rishika Bhagwatkar, Jonas Ngnawé, Yann Pequignot, Alexandre Larouche, Christian Gagné, Irina Rish, Ola Ahmad, Audrey Durand",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Deep learning models operating in the image domain are vulnerable to small input perturbations. For years, robustness to such perturbations was pursued by training models from scratch (i.e., with random initializations) using specialized loss objectives. Recently, robust fine-tuning has emerged as a more efficient alternative: instead of training from scratch, pretrained models are adapted to maximize predictive performance and robustness. To conduct robust fine-tuning, practitioners design an optimization strategy that includes the model update protocol (e.g., full or partial) and the specialized loss objective. Additional design choices include the architecture type and size, and the pretrained representation. These design choices affect robust generalization, which is the model's ability to maintain performance when exposed to new and unseen perturbations at test time. Understanding how these design choices influence generalization remains an open question with significant practical implications. In response, we present an empirical study spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3 adaptation protocols, yielding 1,440 training configurations and 7,200 robustness measurements across five perturbation types. To our knowledge, this is the most diverse and comprehensive benchmark of robust fine-tuning to date. While attention-based architectures and robust pretrained representations are increasingly popular, we find that convolutional neural networks pretrained in a supervised manner on large datasets often perform best. Our analysis both confirms and challenges prior design assumptions, highlighting promising research directions and offering practical guidance.",
    "matched_keyword": "pre-training",
    "matched_category": "预训练"
  },
  {
    "id": "2508.14080",
    "url": "https://arxiv.org/abs/2508.14080",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.328905",
    "title": "KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge",
    "authors": "Guanghao Jin, Jingpei Wu, Tianpei Guo, Yiyi Niu, Weidong Zhou, Guoyang Liu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Referring Expression Comprehension (REC) is a popular multimodal task that aims to accurately detect target objects within a single image based on a given textual expression. However, due to the limitations of earlier models, traditional REC benchmarks either rely solely on intra-image cues or lack sufficiently fine-grained instance annotations, making them inadequate for evaluating the reasoning capabilities of Multi-modal Large Language Models (MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC, characterized by three key features: Firstly, it is built upon real-world knowledge, requiring fine-grained multimodal reasoning across text and image. Secondly, the dataset includes elaborately constructed negative samples via fine-grained expression editing, designed to evaluate a model's robustness and anti-hallucination ability. Lastly, we introduce three novel evaluation metrics to systematically explore the model's internal reasoning process. We evaluate 16 state-of-the-art multimodal models on KnowDR-REC, with experimental results showing that existing MLLMs still struggle with knowledge-driven visual grounding tasks. Furthermore, we observe a decoupling between textual understanding and visual grounding in MLLMs, where many models are significantly influenced by memorized shortcut correlations, which severely affect their behavior on our benchmark and hinder genuine multimodal reasoning. We anticipate that the proposed benchmark will inspire future research towards developing more robust, interpretable, and knowledge-intensive visual grounding frameworks, driving the development of more reliable and robust multimodal systems for complex real-world scenarios.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14090",
    "url": "https://arxiv.org/abs/2508.14090",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.328905",
    "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models",
    "authors": "Chen Xu, Dawei Yang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.",
    "comments": "12 pages, 6 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14094",
    "url": "https://arxiv.org/abs/2508.14094",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.332906",
    "title": "Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets",
    "authors": "Benjamin Pikus, Pratyush Ranjan Tiwari, Burton Ye",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate a critical question for resource-constrained alignment: under a fixed acquisition budget, should practitioners prioritize examples that are easy, medium, hard, or of random difficulty? We study Group Relative Policy Optimization (GRPO) fine-tuning across different model sizes and families, comparing four subset selection policies chosen from the same unlabeled pool using base-model difficulty estimates obtained via multi-sample evaluation. Our experiments reveal that training on the hardest examples yields the largest performance gains, up to 47%, while training on easy examples yield the smallest gains. Analysis reveals that this effect arises from harder examples providing more learnable opportunities during GRPO training. These findings provide practical guidance for budget-constrained post-training: prioritizing hard examples yields substantial performance gains on reasoning tasks when using GRPO.",
    "matched_keyword": "post-training",
    "matched_category": "后训练"
  },
  {
    "id": "2508.14111",
    "url": "https://arxiv.org/abs/2508.14111",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.332906",
    "title": "From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery",
    "authors": "Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Xuming He, Qiang Zhang, Chenyu You, Shuangjia Zheng, Ning Ding, Wanli Ouyang, Nanqing Dong, Yu Cheng, Siqi Sun, Lei Bai, Bowen Zhou",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14119",
    "url": "https://arxiv.org/abs/2508.14119",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "Documenting Deployment with Fabric: A Repository of Real-World AI Governance",
    "authors": "Mackenzie Jorgensen, Kendall Brogle, Katherine M. Collins, Lujain Ibrahim, Arina Shah, Petra Ivanovic, Noah Broestl, Gabriel Piles, Paul Dongha, Hatim Abdulhussein, Adrian Weller, Jillian Powers, Umang Bhatt",
    "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
    "abstract": "Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.",
    "comments": "AIES 2025",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14120",
    "url": "https://arxiv.org/abs/2508.14120",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning",
    "authors": "Yuhang Lin, Yijia Xie, Jiahong Xie, Yuehao Huang, Ruoyu Wang, Jiajun Lv, Yukai Ma, Xingxing Zuo",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
    "abstract": "Generating physically realistic humanoid-object interactions (HOI) is a fundamental challenge in robotics. Existing HOI generation approaches, such as diffusion-based models, often suffer from artifacts such as implausible contacts, penetrations, and unrealistic whole-body actions, which hinder successful execution in physical environments. To address these challenges, we introduce SimGenHOI, a unified framework that combines the strengths of generative modeling and reinforcement learning to produce controllable and physically plausible HOI. Our HOI generative model, based on Diffusion Transformers (DiT), predicts a set of key actions conditioned on text prompts, object geometry, sparse object waypoints, and the initial humanoid pose. These key actions capture essential interaction dynamics and are interpolated into smooth motion trajectories, naturally supporting long-horizon generation. To ensure physical realism, we design a contact-aware whole-body control policy trained with reinforcement learning, which tracks the generated motions while correcting artifacts such as penetration and foot sliding. Furthermore, we introduce a mutual fine-tuning strategy, where the generative model and the control policy iteratively refine each other, improving both motion realism and tracking robustness. Extensive experiments demonstrate that SimGenHOI generates realistic, diverse, and physically plausible humanoid-object interactions, achieving significantly higher tracking success rates in simulation and enabling long-horizon manipulation tasks. Code will be released upon acceptance on our project page: this https URL.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14123",
    "url": "https://arxiv.org/abs/2508.14123",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "AI Agents for Photonic Integrated Circuit Design Automation",
    "authors": "Ankita Sharma, YuQi Fu, Vahid Ansari, Rishabh Iyer, Fiona Kuang, Kashish Mistry, Raisa Islam Aishy, Sara Ahmad, Joaquin Matres, Dirk R. Englund, Joyce K.S. Poon",
    "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI); Applied Physics (physics.app-ph); Optics (physics.optics)",
    "abstract": "We present Photonics Intelligent Design and Optimization (PhIDO), a multi-agent framework that converts natural-language photonic integrated circuit (PIC) design requests into layout mask files. We compare 7 reasoning large language models for PhIDO using a testbench of 102 design descriptions that ranged from single devices to 112-component PICs. The success rate for single-device designs was up to 91%. For design queries with less than or equal to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro requiring the fewest output tokens and lowest cost. The next steps toward autonomous PIC development include standardized knowledge representations, expanded datasets, extended verification, and robotic automation.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14128",
    "url": "https://arxiv.org/abs/2508.14128",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection",
    "authors": "Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, Ioannis Ch. Paschalidis",
    "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
    "abstract": "Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs' vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.",
    "comments": "11 pages, 1 figure",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14131",
    "url": "https://arxiv.org/abs/2508.14131",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents",
    "authors": "Junjie Qi, Siqi Mao, Tianyi Tan",
    "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
    "abstract": "We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14135",
    "url": "https://arxiv.org/abs/2508.14135",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.336907",
    "title": "Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach",
    "authors": "Collins O.Ogbodo, Timothy J. Rogers, Mattia Dal Borgo, David J. Wagg",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Modal testing plays a critical role in structural analysis by providing essential insights into dynamic behaviour across a wide range of engineering industries. In practice, designing an effective modal test campaign involves complex experimental planning, comprising a series of interdependent decisions that significantly influence the final test outcome. Traditional approaches to test design are typically static-focusing only on global tests without accounting for evolving test campaign parameters or the impact of such changes on previously established decisions, such as sensor configurations, which have been found to significantly influence test outcomes. These rigid methodologies often compromise test accuracy and adaptability. To address these limitations, this study introduces an agent-based decision support framework for adaptive sensor placement across dynamically changing modal test environments. The framework formulates the problem using an underspecified partially observable Markov decision process, enabling the training of a generalist reinforcement learning agent through a dual-curriculum learning strategy. A detailed case study on a steel cantilever structure demonstrates the efficacy of the proposed method in optimising sensor locations across frequency segments, validating its robustness and real-world applicability in experimental settings.",
    "comments": "17 pages, 11 figures; currently under peer review",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14146",
    "url": "https://arxiv.org/abs/2508.14146",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.340904",
    "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation",
    "authors": "Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.",
    "comments": "Work in progress",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14160",
    "url": "https://arxiv.org/abs/2508.14160",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.340904",
    "title": "RynnEC: Bringing MLLMs into Embodied World",
    "authors": "Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
    "abstract": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: this https URL",
    "comments": "The technical report of RynnEC, an embodied cognition MLLM",
    "matched_keyword": "mllm",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14190",
    "url": "https://arxiv.org/abs/2508.14190",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.340904",
    "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text",
    "authors": "Zixin Rao, Youssef Mohamed, Shang Liu, Zeyan Liu",
    "subjects": "Cryptography and Security (cs.CR); Computation and Language (cs.CL); Machine Learning (cs.LG)",
    "abstract": "Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.",
    "comments": "Securecomm 2025",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14203",
    "url": "https://arxiv.org/abs/2508.14203",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment",
    "authors": "Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Video Anomaly Detection (VAD) has emerged as a pivotal task in computer vision, with broad relevance across multiple fields. Recent advances in deep learning have driven significant progress in this area, yet the field remains fragmented across domains and learning paradigms. This survey offers a comprehensive perspective on VAD, systematically organizing the literature across various supervision levels, as well as adaptive learning methods such as online, active, and continual learning. We examine the state of VAD across three major application categories: human-centric, vehicle-centric, and environment-centric scenarios, each with distinct challenges and design considerations. In doing so, we identify fundamental contributions and limitations of current methodologies. By consolidating insights from subfields, we aim to provide the community with a structured foundation for advancing both theoretical understanding and real-world applicability of VAD systems. This survey aims to support researchers by providing a useful reference, while also drawing attention to the broader set of open challenges in anomaly detection, including both fundamental research questions and practical obstacles to real-world deployment.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14214",
    "url": "https://arxiv.org/abs/2508.14214",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli",
    "authors": "Mattson Ogg, Chace Ashcraft, Ritwik Bose, Raphael Norman-Tenazas, Michael Wolmetz",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14222",
    "url": "https://arxiv.org/abs/2508.14222",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "StarStream: Live Video Analytics over Space Networking",
    "authors": "Miao Zhang, Jiaxing Li, Haoyuan Zhao, Linfeng Shen, Jiangchuan Liu",
    "subjects": "Networking and Internet Architecture (cs.NI); Multimedia (cs.MM)",
    "abstract": "Streaming videos from resource-constrained front-end devices over networks to resource-rich cloud servers has long been a common practice for surveillance and analytics. Most existing live video analytics (LVA) systems, however, have been built over terrestrial networks, limiting their applications during natural disasters and in remote areas that desperately call for real-time visual data delivery and scene analysis. With the recent advent of space networking, in particular, Low Earth Orbit (LEO) satellite constellations such as Starlink, high-speed truly global Internet access is becoming available and affordable. This paper examines the challenges and potentials of LVA over modern LEO satellite networking (LSN). Using Starlink as the testbed, we have carried out extensive in-the-wild measurements to gain insights into its achievable performance for LVA. The results reveal that the uplink bottleneck in today's LSN, together with the volatile network conditions, can significantly affect the service quality of LVA and necessitate prompt adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming framework for LVA. At its core, StarStream is empowered by a Transformer-based network performance predictor tailored for LSN and a content-aware configuration optimizer. We discuss a series of key design and implementation issues of StarStream and demonstrate its effectiveness and superiority through trace-driven experiments with real-world network and video processing data.",
    "comments": "Accepted by MM'24",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14231",
    "url": "https://arxiv.org/abs/2508.14231",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "Incident Analysis for AI Agents",
    "authors": "Carson Ezell, Xavier Roberts-Gaal, Alan Chan",
    "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
    "abstract": "As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks.",
    "comments": "16 pages (10 pages main text), 4 figures, 3 tables. To be published in the Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics, & Society (AIES)",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14235",
    "url": "https://arxiv.org/abs/2508.14235",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "SLAM-based Safe Indoor Exploration Strategy",
    "authors": "Omar Mostafa, Nikolaos Evangeliou, Anthony Tzes",
    "subjects": "Robotics (cs.RO)",
    "abstract": "This paper suggests a 2D exploration strategy for a planar space cluttered with obstacles. Rather than using point robots capable of adjusting their position and altitude instantly, this research is tailored to classical agents with circular footprints that cannot control instantly their pose. Inhere, a self-balanced dual-wheeled differential drive system is used to explore the place. The system is equipped with linear accelerometers and angular gyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs RTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop closures. The mobile agent explores the planar space using a safe skeleton approach that places the agent as far as possible from the static obstacles. During the exploration strategy, the heading is towards any offered openings of the space. This space exploration strategy has as its highest priority the agent's safety in avoiding the obstacles followed by the exploration of undetected space. Experimental studies with a ROS-enabled mobile agent are presented indicating the path planning strategy while exploring the space.",
    "comments": "5 pages, 8 figures. Published in the 2025 11th International Conference on Automation, Robotics, and Applications (ICARA)",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2508.14237",
    "url": "https://arxiv.org/abs/2508.14237",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.344902",
    "title": "OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos",
    "authors": "Miao Zhang, Yifei Zhu, Linfeng Shen, Fangxin Wang, Jiangchuan Liu",
    "subjects": "Networking and Internet Architecture (cs.NI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
    "abstract": "With the reduced hardware costs of omnidirectional cameras and the proliferation of various extended reality applications, more and more $360^\\circ$ videos are being captured. To fully unleash their potential, advanced video analytics is expected to extract actionable insights and situational knowledge without blind spots from the videos. In this paper, we present OmniSense, a novel edge-assisted framework for online immersive video analytics. OmniSense achieves both low latency and high accuracy, combating the significant computation and network resource challenges of analyzing $360^\\circ$ videos. Motivated by our measurement insights into $360^\\circ$ videos, OmniSense introduces a lightweight spherical region of interest (SRoI) prediction algorithm to prune redundant information in $360^\\circ$ frames. Incorporating the video content and network dynamics, it then smartly scales vision models to analyze the predicted SRoIs with optimized resource utilization. We implement a prototype of OmniSense with commodity devices and evaluate it on diverse real-world collected $360^\\circ$ videos. Extensive evaluation results show that compared to resource-agnostic baselines, it improves the accuracy by $19.8\\%$ -- $114.6\\%$ with similar end-to-end latencies. Meanwhile, it hits $2.0\\times$ -- $2.4\\times$ speedups while keeping the accuracy on par with the highest accuracy of baselines.",
    "comments": "10 pages; Accepted by INFOCOM'23",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14273",
    "url": "https://arxiv.org/abs/2508.14273",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.351352",
    "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper",
    "authors": "Krishna Garg, Firoz Shaikh, Sambaran Bandyopadhyay, Cornelia Caragea",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMs' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.",
    "comments": "20 pages, 15 figures",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14275",
    "url": "https://arxiv.org/abs/2508.14275",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.351352",
    "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders",
    "authors": "Cliff O'Reilly, Ernesto Jimenez-Ruiz, Tillman Weyde",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14279",
    "url": "https://arxiv.org/abs/2508.14279",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.352461",
    "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs",
    "authors": "Adrian-Marius Dumitran, Alexandra-Mihaela Danila, Angela-Liliana Dumitran",
    "subjects": "Computation and Language (cs.CL); Computers and Society (cs.CY)",
    "abstract": "LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.",
    "comments": "Accepted as long paper @RANLP2025",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14285",
    "url": "https://arxiv.org/abs/2508.14285",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.353512",
    "title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models",
    "authors": "Liyi Zhang, Jake Snell, Thomas L. Griffiths",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
    "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.",
    "comments": "6 pages, 2 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14287",
    "url": "https://arxiv.org/abs/2508.14287",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.353512",
    "title": "Nearly Tight Bounds for the Online Sorting Problem",
    "authors": "Yossi Azar, Debmalya Panigrahi, Or Vardi",
    "subjects": "Data Structures and Algorithms (cs.DS)",
    "abstract": "In the online sorting problem, a sequence of $n$ numbers in $[0, 1]$ (including $\\{0,1\\}$) have to be inserted in an array of size $m \\ge n$ so as to minimize the sum of absolute differences between pairs of numbers occupying consecutive non-empty cells. Previously, Aamand {\\em et al.} (SODA 2023) gave a deterministic $2^{\\sqrt{\\log n} \\sqrt{\\log \\log n + \\log (1/\\varepsilon)}}$-competitive algorithm when $m = (1+\\varepsilon) n$ for any $\\varepsilon \\ge \\Omega(\\log n/n)$. They also showed a lower bound: with $m = \\gamma n$ space, the competitive ratio of any deterministic algorithm is at least $\\frac{1}{\\gamma}\\cdot\\Omega(\\log n / \\log \\log n)$. This left an exponential gap between the upper and lower bounds for the problem.\nIn this paper, we bridge this exponential gap and almost completely resolve the online sorting problem. First, we give a deterministic $O(\\log^2 n / \\varepsilon)$-competitive algorithm with $m = (1+\\varepsilon) n$, for any $\\varepsilon \\ge \\Omega(\\log n / n)$. Next, for $m = \\gamma n$ where $\\gamma = [O(1), O(\\log^2 n)]$, we give a deterministic $O(\\log^2 n / \\gamma)$-competitive algorithm. In particular, this implies an $O(1)$-competitive algorithm with $O(n \\log^2 n)$ space, which is within an $O(\\log n\\cdot \\log \\log n)$ factor of the lower bound of $\\Omega(n \\log n / \\log \\log n)$. Combined, the two results imply a close to optimal tradeoff between space and competitive ratio for the entire range of interest: specifically, an upper bound of $O(\\log^2 n)$ on the product of the competitive ratio and $\\gamma$ while the lower bound on this product is $\\Omega(\\log n / \\log\\log n)$. We also show that these results can be extended to the case when the range of the numbers is not known in advance, for an additional $O(\\log n)$ factor in the competitive ratio.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14288",
    "url": "https://arxiv.org/abs/2508.14288",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.354518",
    "title": "Measuring LLM Code Generation Stability via Structural Entropy",
    "authors": "Yewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Rajput, Tegawende F. Bissyande, Jacques Klein",
    "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL)",
    "abstract": "Assessing the stability of code generation from large language models (LLMs) is essential for judging their reliability in real-world development. We extend prior \"structural-entropy concepts\" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural overlap, and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading LLMs on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit.",
    "comments": "ASE-NIER",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14289",
    "url": "https://arxiv.org/abs/2508.14289",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.354518",
    "title": "\"They Aren't Built For Me\": An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics",
    "authors": "Areen Khalaila, Lane Harrison, Nam Wook Kim, Dylan Cashman",
    "subjects": "Human-Computer Interaction (cs.HC)",
    "abstract": "Advancements in accessibility technologies such as low-cost swell form printers or refreshable tactile displays promise to allow blind or low-vision (BLV) people to analyze data by transforming visual representations directly to tactile representations. However, it is possible that design guidelines derived from experiments on the visual perception system may not be suited for the tactile perception system. We investigate the potential mismatch between familiar visual encodings and tactile perception in an exploratory study into the strategies employed by BLV people to measure common graphical primitives converted to tactile representations. First, we replicate the Cleveland and McGill study on graphical perception using swell form printing with eleven BLV subjects. Then, we present results from a group interview in which we describe the strategies used by our subjects to read four common chart types. While our results suggest that familiar encodings based on visual perception studies can be useful in tactile graphics, our subjects also expressed a desire to use encodings designed explicitly for BLV people. Based on this study, we identify gaps between the perceptual expectations of common charts and the perceptual tools available in tactile perception. Then, we present a set of guidelines for the design of tactile graphics that accounts for these gaps. Supplemental material is available at this https URL.",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2508.14297",
    "url": "https://arxiv.org/abs/2508.14297",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.355517",
    "title": "Grid-Edge Energy-Flexible Technologies: A Comparative Analysis Across Generators, Loads, and Energy Storage Systems",
    "authors": "Jesus Silva-Rodriguez, Tianxia Zhao, Ran Mo, Xingpeng Li",
    "subjects": "Systems and Control (eess.SY)",
    "abstract": "This review analysis presents a comprehensive exploration of energy flexibility in modern power systems. It examines the roles and mechanisms of flexible technologies across three main categories: generators, energy storage systems (ESS), and loads. Energy flexibility is defined as the ability to dynamically adjust supply and/or demand in response to grid conditions to maintain balance and stability. This is of particular importance to facilitate the integration of the growing variable renewable energy sources (RES) into modern power grids. Additionally, traditional supply-side mechanisms to maintain balance and stability are complemented by advancements in demand-side management and demand response strategies, which enable loads to adjust consumption patterns and schedules in response to grid requirements. ESS are also explored to further enhance flexibility by absorbing excess generation and/or supplying large load increases that are not able to be met by the less flexible resources. This paper also explores specific flexibility technologies, examining their characteristics, control strategies, advantages, and limitations. Energy flexibility services are also categorized into intermittency mitigation, peak shaving, and energy reserve provisioning. Each service is supported by case studies and examples demonstrating how different resources respond to varying conditions. Ultimately, the findings and reviews of the various flexible resources in this paper provide a roadmap for optimizing energy flexibility across diverse resource types, paving the way for a more sustainable and resilient energy future.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14300",
    "url": "https://arxiv.org/abs/2508.14300",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.355517",
    "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing",
    "authors": "Youssef Maklad, Fares Wael, Ali Hamdi, Wael Elsersy, Khaled Shaban",
    "subjects": "Cryptography and Security (cs.CR); Computation and Language (cs.CL); Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI)",
    "abstract": "Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14302",
    "url": "https://arxiv.org/abs/2508.14302",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.355517",
    "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation",
    "authors": "Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14313",
    "url": "https://arxiv.org/abs/2508.14313",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.357515",
    "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS",
    "authors": "Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14314",
    "url": "https://arxiv.org/abs/2508.14314",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.357515",
    "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency",
    "authors": "Aman Goel, Daniel Schwartz, Yanjun Qi",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14327",
    "url": "https://arxiv.org/abs/2508.14327",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.359581",
    "title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation",
    "authors": "Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video generation has recently shown superiority in urban scene synthesis for autonomous driving. Existing video generation approaches to autonomous driving primarily focus on RGB video generation and lack the ability to support multi-modal video generation. However, multi-modal data, such as depth maps and semantic maps, are crucial for holistic urban scene understanding in autonomous driving. Although it is feasible to use multiple models to generate different modalities, this increases the difficulty of model deployment and does not leverage complementary cues for multi-modal data generation. To address this problem, in this work, we propose a novel multi-modal multi-view video generation approach to autonomous driving. Specifically, we construct a unified diffusion transformer model composed of modal-shared components and modal-specific components. Then, we leverage diverse conditioning inputs to encode controllable scene structure and content cues into the unified diffusion model for multi-modal multi-view video generation. In this way, our approach is capable of generating multi-modal multi-view driving scene videos in a unified framework. Our experiments on the challenging real-world autonomous driving dataset, nuScenes, show that our approach can generate multi-modal multi-view urban scene videos with high fidelity and controllability, surpassing the state-of-the-art methods.",
    "comments": "Technical Report",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14335",
    "url": "https://arxiv.org/abs/2508.14335",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.359581",
    "title": "The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations",
    "authors": "Hailong Su, Jinshu Su, Yusheng Xia, Haibin Li",
    "subjects": "Networking and Internet Architecture (cs.NI); Social and Information Networks (cs.SI)",
    "abstract": "In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14340",
    "url": "https://arxiv.org/abs/2508.14340",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.361370",
    "title": "A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations",
    "authors": "Konur Tholl, Mariam El Mezouar, Ranwa Al Mallah",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to train agents to make effective decisions in the cybersecurity domain. However, existing ACO applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. While teacher-guided techniques have demonstrated promise in other domains, they have not yet been applied to ACO. In this study, we implement four distinct teacher-guided techniques in the simulated CybORG environment and conduct a comparative evaluation. Our results demonstrate that teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14344",
    "url": "https://arxiv.org/abs/2508.14344",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.362375",
    "title": "ISCA: A Framework for Interview-Style Conversational Agents",
    "authors": "Charles Welch, Allison Lahnala, Vasudha Varadarajan, Lucie Flek, Rada Mihalcea, J. Lomax Boyd, João Sedoc",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "We present a low-compute non-generative system for implementing interview-style conversational agents which can be used to facilitate qualitative data collection through controlled interactions and quantitative analysis. Use cases include applications to tracking attitude formation or behavior change, where control or standardization over the conversational flow is desired. We show how our system can be easily adjusted through an online administrative panel to create new interviews, making the tool accessible without coding. Two case studies are presented as example applications, one regarding the Expressive Interviewing system for COVID-19 and the other a semi-structured interview to survey public opinion on emerging neurotechnology. Our code is open-source, allowing others to build off of our work and develop extensions for additional functionality.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14357",
    "url": "https://arxiv.org/abs/2508.14357",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.364375",
    "title": "Organ-Agents: Virtual Human Physiology Simulator via LLMs",
    "authors": "Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14377",
    "url": "https://arxiv.org/abs/2508.14377",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.366375",
    "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities",
    "authors": "Wenhan Dong, Zhen Sun, Yuemeng Zhao, Zifan Peng, Jun Wu, Jingyi Zheng, Yule Liu, Xinlei He, Yu Wang, Ruiming Wang, Xinyi Huang, Lei Mo",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
    "abstract": "Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14383",
    "url": "https://arxiv.org/abs/2508.14383",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.367376",
    "title": "Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations",
    "authors": "Haitong Ma, Bo Dai, Zhaolin Ren, Yebin Wang, Na Li",
    "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
    "abstract": "Limited data has become a major bottleneck in scaling up offline imitation learning (IL). In this paper, we propose enhancing IL performance under limited expert data by introducing a pre-training stage that learns dynamics representations, derived from factorizations of the transition dynamics. We first theoretically justify that the optimal decision variable of offline IL lies in the representation space, significantly reducing the parameters to learn in the downstream IL. Moreover, the dynamics representations can be learned from arbitrary data collected with the same dynamics, allowing the reuse of massive non-expert data and mitigating the limited data issues. We present a tractable loss function inspired by noise contrastive estimation to learn the dynamics representations at the pre-training stage. Experiments on MuJoCo demonstrate that our proposed algorithm can mimic expert policies with as few as a single trajectory. Experiments on real quadrupeds show that we can leverage pre-trained dynamics representations from simulator data to learn to walk from a few real-world demonstrations.",
    "comments": "7 pages, 5 figures",
    "matched_keyword": "pre-training",
    "matched_category": "预训练"
  },
  {
    "id": "2508.14387",
    "url": "https://arxiv.org/abs/2508.14387",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.368374",
    "title": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models",
    "authors": "Yuxiao Zhu, Junfeng Chen, Xintong Zhang, Meng Guo, Zhongkui Li",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Online coordination of multi-robot systems in open and unknown environments faces significant challenges, particularly when semantic features detected during operation dynamically trigger new tasks. Recent large language model (LLMs)-based approaches for scene reasoning and planning primarily focus on one-shot, end-to-end solutions in known environments, lacking both dynamic adaptation capabilities for online operation and explainability in the processes of planning. To address these issues, a novel framework (DEXTER-LLM) for dynamic task planning in unknown environments, integrates four modules: (i) a mission comprehension module that resolves partial ordering of tasks specified by natural languages or linear temporal logic formulas (LTL); (ii) an online subtask generator based on LLMs that improves the accuracy and explainability of task decomposition via multi-stage reasoning; (iii) an optimal subtask assigner and scheduler that allocates subtasks to robots via search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop verification module that implements multi-rate, event-based updates for both subtasks and their assignments, to cope with new features and tasks detected online. The framework effectively combines LLMs' open-world reasoning capabilities with the optimality of model-based assignment methods, simultaneously addressing the critical issue of online adaptability and explainability. Experimental evaluations demonstrate exceptional performances, with 100% success rates across all scenarios, 160 tasks and 480 subtasks completed on average (3 times the baselines), 62% less queries to LLMs during adaptation, and superior plan quality (2 times higher) for compound tasks. Project page at this https URL",
    "comments": "submitted to IROS 2025",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14390",
    "url": "https://arxiv.org/abs/2508.14390",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.368374",
    "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play",
    "authors": "Ke Fang, Tianyi Zhao, Lu Cheng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at this https URL.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14395",
    "url": "https://arxiv.org/abs/2508.14395",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.369377",
    "title": "NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding",
    "authors": "Running Zhao, Zhihan Jiang, Xinchen Zhang, Chirui Chang, Handi Chen, Weipeng Deng, Luyao Jin, Xiaojuan Qi, Xun Qian, Edith C.H. Ngai",
    "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
    "abstract": "Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users' expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt's interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt. Project website: this https URL",
    "comments": "Accepted to UIST 2025. Project website: this https URL",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14408",
    "url": "https://arxiv.org/abs/2508.14408",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.370375",
    "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs",
    "authors": "Yinghan Zhou, Weifeng Zhu, Juan Wen, Wanli Peng, Zhengxian Wu, Yiming Xue",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14410",
    "url": "https://arxiv.org/abs/2508.14410",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.371375",
    "title": "Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning",
    "authors": "Beinuo Yang, Qishen Zhou, Junyi Li, Xingchen Su, Simon Hu",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42\\%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14411",
    "url": "https://arxiv.org/abs/2508.14411",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.371375",
    "title": "A Real-world Display Inverse Rendering Dataset",
    "authors": "Seokjun Choi, Hoon-Gyu Chung, Yujin Jeon, Giljoo Nam, Seung-Hwan Baek",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Inverse rendering aims to reconstruct geometry and reflectance from captured images. Display-camera imaging systems offer unique advantages for this task: each pixel can easily function as a programmable point light source, and the polarized light emitted by LCD displays facilitates diffuse-specular separation. Despite these benefits, there is currently no public real-world dataset captured using display-camera systems, unlike other setups such as light stages. This absence hinders the development and evaluation of display-based inverse rendering methods. In this paper, we introduce the first real-world dataset for display-based inverse rendering. To achieve this, we construct and calibrate an imaging system comprising an LCD display and stereo polarization cameras. We then capture a diverse set of objects with diverse geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We also provide high-quality ground-truth geometry. Our dataset enables the synthesis of captured images under arbitrary display patterns and different noise levels. Using this dataset, we evaluate the performance of existing photometric stereo and inverse rendering methods, and provide a simple, yet effective baseline for display inverse rendering, outperforming state-of-the-art inverse rendering methods. Code and dataset are available on our project page at this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14415",
    "url": "https://arxiv.org/abs/2508.14415",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.372375",
    "title": "The Agent Behavior: Model, Governance and Challenges in the AI Digital Age",
    "authors": "Qiang Zhang, Pei Yan, Yijia Xu, Chuanpo Fu, Yong Fang, Yang Liu",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Advancements in AI have led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues such as data contamination and unclear accountability. To address these challenges, this paper proposes the \"Network Behavior Lifecycle\" model, which divides network behavior into 6 stages and systematically analyzes the behavioral differences between humans and agents at each stage. Based on these insights, the paper further introduces the \"Agent for Agent (A4A)\" paradigm and the \"Human-Agent Behavioral Disparity (HABD)\" model, which examine the fundamental distinctions between human and agent behaviors across 5 dimensions: decision mechanism, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense. Finally, the paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14419",
    "url": "https://arxiv.org/abs/2508.14419",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.372375",
    "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness",
    "authors": "Scott Blyth, Sherlock A. Licorish, Christoph Treude, Markus Wagner",
    "subjects": "Software Engineering (cs.SE)",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in code generation, achieving high scores on benchmarks such as HumanEval and MBPP. However, these benchmarks primarily assess functional correctness and neglect broader dimensions of code quality, including security, reliability, readability, and maintainability. In this work, we systematically evaluate the ability of LLMs to generate high-quality code across multiple dimensions using the PythonSecurityEval benchmark. We introduce an iterative static analysis-driven prompting algorithm that leverages Bandit and Pylint to identify and resolve code quality issues. Our experiments with GPT-4o show substantial improvements: security issues reduced from >40% to 13%, readability violations from >80% to 11%, and reliability warnings from >50% to 11% within ten iterations. These results demonstrate that LLMs, when guided by static analysis feedback, can significantly enhance code quality beyond functional correctness.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14423",
    "url": "https://arxiv.org/abs/2508.14423",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.373226",
    "title": "MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing",
    "authors": "Jeahun Sung, Changhyun Roh, Chanho Eom, Jihyong Oh",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Recent advances in portable imaging have made camera-based screen capture ubiquitous. Unfortunately, frequency aliasing between the camera's color filter array (CFA) and the display's sub-pixels induces moiré patterns that severely degrade captured photos and videos. Although various demoiréing models have been proposed to remove such moiré patterns, these approaches still suffer from several limitations: (i) spatially varying artifact strength within a frame, (ii) large-scale and globally spreading structures, (iii) channel-dependent statistics and (iv) rapid temporal fluctuations across frames. We address these issues with the Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former), which comprises two key components: Decoupled Moiré Adaptive Demoiréing (DMAD) and Spatio-Temporal Adaptive Demoiréing (STAD). DMAD separates moiré and content via a Moiré Decoupling Block (MDB) and a Detail Decoupling Block (DDB), then produces moiré-adaptive features using a Moiré Conditioning Block (MCB) for targeted restoration. STAD introduces a Spatial Fusion Block (SFB) with window attention to capture large-scale structures, and a Feature Channel Attention (FCA) to model channel dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs implicit frame alignment without any explicit alignment module. We analyze moiré characteristics through qualitative and quantitative studies, and evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.",
    "comments": "Please visit our project page at [this http URL link](this https URL)",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14427",
    "url": "https://arxiv.org/abs/2508.14427",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.373226",
    "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models",
    "authors": "Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14460",
    "url": "https://arxiv.org/abs/2508.14460",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.377950",
    "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization",
    "authors": "Shuaijie She, Yu Bao, Yu Lu, Lu Xu, Tao Li, Wenhao Zhu, Shujian Huang, Shanbo Cheng, Lu Lu, Yuxuan Wang",
    "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
    "abstract": "We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.",
    "comments": "18 pages, 4 figures,",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14465",
    "url": "https://arxiv.org/abs/2508.14465",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.379135",
    "title": "DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing",
    "authors": "Weitao Wang, Zichen Wang, Hongdeng Shen, Yulei Lu, Xirui Fan, Suhui Wu, Jun Zhang, Haoqian Wang, Hao Zhang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "With the rapid progress of video generation, demand for customized video editing is surging, where subject swapping constitutes a key component yet remains under-explored. Prevailing swapping approaches either specialize in narrow domains--such as human-body animation or hand-object interaction--or rely on some indirect editing paradigm or ambiguous text prompts that compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided, subject-agnostic, end-to-end framework that swaps any subject in any video for customization with a user-specified mask and reference image. To inject fine-grained guidance, we introduce multiple conditions and a dedicated condition fusion module that integrates them efficiently. In addition, an adaptive mask strategy is designed to accommodate subjects of varying scales and attributes, further improving interactions between the swapped subject and its surrounding context. Through our elaborate two-phase dataset construction and training scheme, our DreamSwapV outperforms existing methods, as validated by comprehensive experiments on VBench indicators and our first introduced DreamSwapV-Benchmark.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14466",
    "url": "https://arxiv.org/abs/2508.14466",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.379135",
    "title": "LookOut: Real-World Humanoid Egocentric Navigation",
    "authors": "Boxiao Pan, Adam W. Harley, C. Karen Liu, Leonidas J. Guibas",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "The ability to predict collision-free future trajectories from egocentric observations is crucial in applications such as humanoid robotics, VR / AR, and assistive navigation. In this work, we introduce the challenging problem of predicting a sequence of future 6D head poses from an egocentric video. In particular, we predict both head translations and rotations to learn the active information-gathering behavior expressed through head-turning events. To solve this task, we propose a framework that reasons over temporally aggregated 3D latent features, which models the geometric and semantic constraints for both the static and dynamic parts of the environment. Motivated by the lack of training data in this space, we further contribute a data collection pipeline using the Project Aria glasses, and present a dataset collected through this approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4 hours of recording of users navigating in real-world scenarios. It includes diverse situations and navigation behaviors, providing a valuable resource for learning real-world egocentric navigation policies. Extensive experiments show that our model learns human-like navigation behaviors such as waiting / slowing down, rerouting, and looking around for traffic while generalizing to unseen environments. Check out our project webpage at this https URL.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14483",
    "url": "https://arxiv.org/abs/2508.14483",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.380667",
    "title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration",
    "authors": "Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at this https URL.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14485",
    "url": "https://arxiv.org/abs/2508.14485",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.380667",
    "title": "Distribution-Guided Auto-Encoder for User Multimodal Interest Cross Fusion",
    "authors": "Moyu Zhang, Yongxiang Tang, Yujun Jin, Jinxin Hu, Yu Zhang",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Traditional recommendation methods rely on correlating the embedding vectors of item IDs to capture implicit collaborative filtering signals to model the user's interest in the target item. Consequently, traditional ID-based methods often encounter data sparsity problems stemming from the sparse nature of ID features. To alleviate the problem of item ID sparsity, recommendation models incorporate multimodal item information to enhance recommendation accuracy. However, existing multimodal recommendation methods typically employ early fusion approaches, which focus primarily on combining text and image features, while neglecting the contextual influence of user behavior sequences. This oversight prevents dynamic adaptation of multimodal interest representations based on behavioral patterns, consequently restricting the model's capacity to effectively capture user multimodal interests. Therefore, this paper proposes the Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves the cross fusion of user multimodal interest at the behavioral this http URL, extensive experiments demonstrate the superiority of DMAE.",
    "comments": "Accepted by CIKM 2025, 11 pages, 4 figures, 4 tables",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14496",
    "url": "https://arxiv.org/abs/2508.14496",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.381227",
    "title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy",
    "authors": "Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, Haifeng Wang",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14504",
    "url": "https://arxiv.org/abs/2508.14504",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.382296",
    "title": "PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments",
    "authors": "Bernd Hofmann, Albert Scheck, Joerg Franke, Patrick Bruendl",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14523",
    "url": "https://arxiv.org/abs/2508.14523",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.385441",
    "title": "Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism",
    "authors": "Kevin Riehl, Shaimaa K. El-Baklish, Anastasios Kouvelas, Michail A. Makridis",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Accurate prediction of road user movement is increasingly required by many applications ranging from advanced driver assistance systems to autonomous driving, and especially crucial for road safety. Even though most traffic accident fatalities account to bicycles, they have received little attention, as previous work focused mainly on pedestrians and motorized vehicles. In this work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal trajectory prediction framework for bicycles. The model incorporates both physics-based modeling (inspired by motorized vehicles) and social-based modeling (inspired by pedestrian movements) to explicitly account for the dual nature of bicycle movement. The social interactions are modeled with a graph attention network, and include decayed historical, but also anticipated, future trajectory data of a bicycles neighborhood, following recent insights from psychological and social studies. The results indicate that the proposed ensemble of physics models -- performing well in the short-term predictions -- and social models -- performing well in the long-term predictions -- exceeds state-of-the-art performance. We also conducted a controlled mass-cycling experiment to demonstrate the framework's performance when forecasting bicycle trajectories and modeling social interactions with road users.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14537",
    "url": "https://arxiv.org/abs/2508.14537",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.387460",
    "title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion",
    "authors": "Yonghan Shin, SeungKyu Kim, Won-Ki Jeong",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Whole slide images (WSIs) in computational pathology (CPath) pose a major computational challenge due to their gigapixel scale, often requiring the processing of tens to hundreds of thousands of high-resolution patches per slide. This results in prohibitive encoding costs, with preprocessing and training times extending to days or even weeks-making WSI encoding the most significant bottleneck in real-world deployment. In this work, we propose WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain vision-language models and large language models to address this challenge by selectively processing diagnostically relevant regions. WISE-FUSE first computes similarity scores between low-resolution patches and class-specific textual descriptions using a knowledge distillation mechanism that preserves fine-grained diagnostic features. Based on these similarity scores, we select a small subset of informative regions for the target task, which quickly eliminates irrelevant patches at the coarse level. The corresponding high-resolution patches are then selectively encoded and fused with textual embeddings to reinforce diagnostic context. Extensive experiments demonstrate that WISE-FUSE reduces WSI encoding time by over threefold while achieving diagnostic performance comparable to or surpassing that of exhaustive patch processing, offering a scalable and practical solution for CPath.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14540",
    "url": "https://arxiv.org/abs/2508.14540",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.388470",
    "title": "Post-hoc LLM-Supported Debugging of Distributed Processes",
    "authors": "Dennis Schiese, Andreas Both",
    "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
    "abstract": "In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.",
    "comments": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14544",
    "url": "https://arxiv.org/abs/2508.14544",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.388470",
    "title": "Adaptively Robust LLM Inference Optimization under Prediction Uncertainty",
    "authors": "Zixi Chen, Yinyu Ye, Zijie Zhou",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
    "abstract": "We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.\nWe first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14553",
    "url": "https://arxiv.org/abs/2508.14553",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.389478",
    "title": "Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems",
    "authors": "Dennis Schiese, Aleksandr Perevalov, Andreas Both",
    "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
    "abstract": "Over time, software systems have reached a level of complexity that makes it difficult for their developers and users to explain particular decisions made by them. In this paper, we focus on the explainability of component-based systems for Question Answering (QA). These components often conduct processes driven by AI methods, in which behavior and decisions cannot be clearly explained or justified, s.t., even for QA experts interpreting the executed process and its results is hard. To address this challenge, we present an approach that considers the components' input and output data flows as a source for representing the behavior and provide explanations for the components, enabling users to comprehend what happened. In the QA framework used here, the data flows of the components are represented as SPARQL queries (inputs) and RDF triples (outputs). Hence, we are also providing valuable insights on verbalization regarding these data types. In our experiments, the approach generates explanations while following template-based settings (baseline) or via the use of Large Language Models (LLMs) with different configurations (automatic generation). Our evaluation shows that the explanations generated via LLMs achieve high quality and mostly outperform template-based approaches according to the users' ratings. Therefore, it enables us to automatically explain the behavior and decisions of QA components to humans while using RDF and SPARQL as a context for explanations.",
    "comments": "Presented at ICWI 2024, Zagreb. Released with ISBN: 978-989-8704-62-7. Data source: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14564",
    "url": "https://arxiv.org/abs/2508.14564",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.391496",
    "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs",
    "authors": "Luca Annese, Sabrina Patania, Silvia Serino, Tom Foulsham, Silvia Rossi, Azzurra Ruggeri, Dimitri Ognibene",
    "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
    "abstract": "Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.",
    "comments": "Accepted at ICSR25",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14581",
    "url": "https://arxiv.org/abs/2508.14581",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.393515",
    "title": "FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics",
    "authors": "Chen Chen, Runze Li, Zejun Zhang, Pukun Zhao, Fanqing Zhou, Longxiang Wang, Haojian Huang",
    "subjects": "Multimedia (cs.MM); Image and Video Processing (eess.IV)",
    "abstract": "FakeHunter is a multimodal deepfake detection framework that combines memory-guided retrieval, chain-of-thought (Observation-Thought-Action) reasoning, and tool-augmented verification to provide accurate and interpretable video forensics. FakeHunter encodes visual content using CLIP and audio using CLAP, generating joint audio-visual embeddings that retrieve semantically similar real exemplars from a FAISS-indexed memory bank for contextual grounding. Guided by the retrieved context, the system iteratively reasons over evidence to localize manipulations and explain them. When confidence is low, it automatically invokes specialized tools-such as zoom-in image forensics or mel-spectrogram inspection-for fine-grained verification. Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that specify what was modified, where it occurs, and why it is judged fake. We also introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos (950+ min) annotated with manipulation type, region/entity, violated reasoning category, and free-form justification. On X-AVFake, FakeHunter achieves an accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87 percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies reveal that memory retrieval contributes a 7.75 percentage point gain, and tool-based inspection improves low-confidence cases to 46.50%. Despite its multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x), demonstrating practical deployability.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14604",
    "url": "https://arxiv.org/abs/2508.14604",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.395529",
    "title": "UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling",
    "authors": "Peiming Li, Ziyi Wang, Yulin Yuan, Hong Liu, Xiangming Meng, Junsong Yuan, Mengyuan Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at this https URL.",
    "comments": "8 pages, 5 figures, Accepted to ICCV2025",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14606",
    "url": "https://arxiv.org/abs/2508.14606",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.395529",
    "title": "Approximating 1-in-3 SAT by linearly ordered hypergraph 3-colouring is NP-hard",
    "authors": "Andrei Krokhin, Danny Vagnozzi",
    "subjects": "Computational Complexity (cs.CC)",
    "abstract": "Given a satisfiable instance of 1-in-3 SAT, it is NP-hard to find a satisfying assignment for it, but it may be possible to efficiently find a solution subject to a weaker (not necessarily Boolean) predicate than `1-in-3'. There is a folklore conjecture predicting which choices of weaker predicates lead to tractability and for which the task remains \\NP-hard. One specific predicate, corresponding to the problem of linearly ordered $3$-colouring of 3-uniform hypergraphs, has been mentioned in several recent papers as an obstacle to further progress in proving this conjecture. We prove that the problem for this predicate is NP-hard, as predicted by the conjecture.\nWe use the Promise CSP framework, where the complexity analysis is performed via the algebraic approach, by studying the structure of polymorphisms, which are multidimensional invariants of the problem at hand. The analysis of polymorphisms is in general a highly non-trivial task, and topological combinatorics was recently discovered to provide a useful tool for this. There are two distinct ways in which it was used: one is based on variations of the Borsuk-Ulam theorem, and the other aims to classify polymorphisms up to certain reconfigurations (homotopy). Our proof, whilst combinatorial in nature, shows that our problem is the first example where the features behind the two uses of topology appear together. Thus, it is likely to be useful in guiding further development of the topological method aimed at classifying Promise CSPs. An easy consequence of our result is the hardness of another specific Promise CSP, which was recently proved by Filakovský et al. by employing a deep topological analysis of polymorphisms.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14607",
    "url": "https://arxiv.org/abs/2508.14607",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.395529",
    "title": "SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos",
    "authors": "Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14609",
    "url": "https://arxiv.org/abs/2508.14609",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.396539",
    "title": "AnchorSync: Global Consistency Optimization for Long Video Editing",
    "authors": "Zichi Liu, Yinggui Wang, Tao Wei, Chao Ma",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Editing long videos remains a challenging task due to the need for maintaining both global consistency and temporal coherence across thousands of frames. Existing methods often suffer from structural drift or temporal artifacts, particularly in minute-long sequences. We introduce AnchorSync, a novel diffusion-based framework that enables high-quality, long-term video editing by decoupling the task into sparse anchor frame editing and smooth intermediate frame interpolation. Our approach enforces structural consistency through a progressive denoising process and preserves temporal dynamics via multimodal guidance. Extensive experiments show that AnchorSync produces coherent, high-fidelity edits, surpassing prior methods in visual quality and temporal stability.",
    "comments": "ACM MM 2025; Code is released at this https URL",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14631",
    "url": "https://arxiv.org/abs/2508.14631",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.398553",
    "title": "Towards a DSL to Formalize Multimodal Requirements",
    "authors": "Marcos Gomez-Vazquez, Jordi Cabot",
    "subjects": "Software Engineering (cs.SE)",
    "abstract": "Multimodal systems, which process multiple input types such as text, audio, and images, are becoming increasingly prevalent in software systems, enabled by the huge advancements in Machine Learning. This triggers the need to easily define the requirements linked to these new types of user interactions, potentially involving more than one modality at the same time. This remains an open challenge due to the lack of languages and methods adapted to the diverse nature of multimodal interactions, with the risk of implementing AI-enhanced systems that do not properly satisfy the user needs.\nIn this sense, this paper presents MERLAN, a Domain-Specific Language (DSL) to specify the requirements for these new types of multimodal interfaces. We present the metamodel for such language together with a textual syntax implemented as an ANTLR grammar. A prototype tool enabling requirements engineers to write such requirements and automatically generate a possible implementation of a system compliant with them on top of an agentic framework is also provided.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14635",
    "url": "https://arxiv.org/abs/2508.14635",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.398553",
    "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination",
    "authors": "João Vitor de Carvalho Silva, Douglas G. Macharet",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
    "abstract": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14654",
    "url": "https://arxiv.org/abs/2508.14654",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.400599",
    "title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration",
    "authors": "Peilin Ji, Xiao Xue, Simeng Wang, Wenhao Yan",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response.",
    "comments": "17 pages including appendix, 6 figures",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14676",
    "url": "https://arxiv.org/abs/2508.14676",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.401607",
    "title": "Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach",
    "authors": "Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani",
    "subjects": "Networking and Internet Architecture (cs.NI)",
    "abstract": "Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14679",
    "url": "https://arxiv.org/abs/2508.14679",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.402614",
    "title": "Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach",
    "authors": "Parham Soltani, Mehrshad Eskandarpour, Amir Ahmadizad, Hossein Soleimani",
    "subjects": "Networking and Internet Architecture (cs.NI)",
    "abstract": "Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14692",
    "url": "https://arxiv.org/abs/2508.14692",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.403623",
    "title": "Sociotechnical Imaginaries of ChatGPT in Higher Education: The Evolving Media Discourse",
    "authors": "Yinan Sun, Ali Unlu, Aditya Johri",
    "subjects": "Computers and Society (cs.CY)",
    "abstract": "This study investigates how U.S. news media framed the use of ChatGPT in higher education from November 2022 to October 2024. Employing Framing Theory and combining temporal and sentiment analysis of 198 news articles, we trace the evolving narratives surrounding generative AI. We found that the media discourse largely centered on institutional responses; policy changes and teaching practices showed the most consistent presence and positive sentiment over time. Conversely, coverage of topics such as human-centered learning, the job market, and skill development appeared more sporadically, with initially uncertain portrayals gradually shifting toward cautious optimism. Importantly, media sentiment toward ChatGPT's role in college admissions remained predominantly negative. Our findings suggest that media narratives prioritize institutional responses to generative AI over long-term, broader ethical, social, and labor-related implications, shaping an emerging sociotechnical imaginary that frames generative AI in education primarily through the lens of adaptation and innovation.",
    "comments": "Under review at a conference",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14704",
    "url": "https://arxiv.org/abs/2508.14704",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.404631",
    "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers",
    "authors": "Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li",
    "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.",
    "comments": "Website: this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14706",
    "url": "https://arxiv.org/abs/2508.14706",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.405638",
    "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
    "authors": "Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)",
    "abstract": "Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14718",
    "url": "https://arxiv.org/abs/2508.14718",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.406646",
    "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation",
    "authors": "Shubham Pundhir, Ganesh Bagler",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural language generation. We present a comprehensive comparative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB. Our key contribution is a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers. This approach addresses a critical limitation of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby enhancing domain specificity. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for integrating real-world constraints and multi-modal inputs in advanced recipe generation research.",
    "comments": "8 pages, 4 figures. Code is available at: this https URL",
    "matched_keyword": "fine-tuning",
    "matched_category": "微调"
  },
  {
    "id": "2508.14729",
    "url": "https://arxiv.org/abs/2508.14729",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.407654",
    "title": "Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving",
    "authors": "Leila Cheshmi, Mennatullah Siam",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video transformers capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video transformer trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory decoding and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional decoders that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics.",
    "comments": "6 pages, 2 figures, 1 table",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14735",
    "url": "https://arxiv.org/abs/2508.14735",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.408662",
    "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference",
    "authors": "Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: this https URL",
    "comments": "Under review",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14751",
    "url": "https://arxiv.org/abs/2508.14751",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.409670",
    "title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents",
    "authors": "Thomas Carta, Clément Romac, Loris Gaven, Pierre-Yves Oudeyer, Olivier Sigaud, Sylvain Lamprier",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Open-ended AI agents need to be able to learn efficiently goals of increasing complexity, abstraction and heterogeneity over their lifetime. Beyond sampling efficiently their own goals, autotelic agents specifically need to be able to keep the growing complexity of goals under control, limiting the associated growth in sample and computational complexity. To adress this challenge, recent approaches have leveraged hierarchical reinforcement learning (HRL) and language, capitalizing on its compositional and combinatorial generalization capabilities to acquire temporally extended reusable behaviours. Existing approaches use expert defined spaces of subgoals over which they instantiate a hierarchy, and often assume pre-trained associated low-level policies. Such designs are inadequate in open-ended scenarios, where goal spaces naturally diversify across a broad spectrum of difficulties. We introduce HERAKLES, a framework that enables a two-level hierarchical autotelic agent to continuously compile mastered goals into the low-level policy, executed by a small, fast neural network, dynamically expanding the set of subgoals available to the high-level policy. We train a Large Language Model (LLM) to serve as the high-level controller, exploiting its strengths in goal decomposition and generalization to operate effectively over this evolving subgoal space. We evaluate HERAKLES in the open-ended Crafter environment and show that it scales effectively with goal complexity, improves sample efficiency through skill compilation, and enables the agent to adapt robustly to novel challenges over time.",
    "comments": "42 pages",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14765",
    "url": "https://arxiv.org/abs/2508.14765",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.409670",
    "title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning",
    "authors": "Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14782",
    "url": "https://arxiv.org/abs/2508.14782",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.411675",
    "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting",
    "authors": "Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at this https URL.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14786",
    "url": "https://arxiv.org/abs/2508.14786",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.411675",
    "title": "Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns",
    "authors": "Veronika Ivanova, Evgeny Frolov, Alexey Vasilev",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "We consider the task of learning from both positive and negative feedback in a sequential recommendation scenario, as both types of feedback are often present in user interactions. Meanwhile, conventional sequential learning models usually focus on considering and predicting positive interactions, ignoring that reducing items with negative feedback in recommendations improves user satisfaction with the service. Moreover, the negative feedback can potentially provide a useful signal for more accurate identification of true user interests. In this work, we propose to train two transformer encoders on separate positive and negative interaction sequences. We incorporate both types of feedback into the training objective of the sequential recommender using a composite loss function that includes positive and negative cross-entropy as well as a cleverly crafted contrastive term, that helps better modeling opposing patterns. We demonstrate the effectiveness of this approach in terms of increasing true-positive metrics compared to state-of-the-art sequential recommendation methods while reducing the number of wrongly promoted negative items.",
    "matched_keyword": "ppo",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14787",
    "url": "https://arxiv.org/abs/2508.14787",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.411675",
    "title": "Challenges and Opportunities for Participatory Design of Conversational Agents for Young People's Wellbeing",
    "authors": "Natalia Kucirkova, Alexis Hiniker, Megumi Ishikawa, Sho Tsuji, Aayushi Dangol, Robert Wolfe",
    "subjects": "Human-Computer Interaction (cs.HC); Computers and Society (cs.CY)",
    "abstract": "This paper outlines the challenges and opportunities of research on conversational agents with children and young people across four countries, exploring the ways AI technologies can support children's well-being across social and cultural contexts.",
    "comments": "Presented at the AI4CW workshop at ACM IDC 2025",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14812",
    "url": "https://arxiv.org/abs/2508.14812",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.415679",
    "title": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives",
    "authors": "Haoyu Zhao, Jiaxi Gu, Shicong Wang, Xing Zhang, Hang Xu, Zuxuan Wu, Yu-Gang Jiang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "The explosive growth of video streaming presents challenges in achieving high accuracy and low training costs for video-language retrieval. However, existing methods rely on large-scale pre-training to improve video retrieval performance, resulting in significant computational demands. Additionally, the fine-grained information in videos and texts remains underexplored. To alleviate these problems, we propose a novel framework to learn fine-grained features for better alignment and introduce an inference pipeline to improve performance without additional training. Specifically, we employ coarse-to-fine objectives to understand the semantic information of video-text pairs, including contrastive and matching learning. The fine-grained data used for training is obtained through the Granularity-Aware Representation module, which is designed based on similarity analysis between video frames and words in captions. Furthermore, we observe that the repetition of keywords in the original captions, referred to as \"Repetition\", can enhance retrieval performance and improve alignment between video and text. Based on this insight, we propose a novel and effective inference pipeline that incorporates a voting mechanism and a new Matching Entropy metric to achieve better retrieval performance without requiring additional pre-training. Experimental results on four benchmarks demonstrate that the proposed method outperforms previous approaches. Additionally, our inference pipeline achieves significant performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT dataset and a 1.6% increase on the DiDeMo dataset.",
    "comments": "11 pages, 4 figures",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14817",
    "url": "https://arxiv.org/abs/2508.14817",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.415679",
    "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs",
    "authors": "Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14825",
    "url": "https://arxiv.org/abs/2508.14825",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.415679",
    "title": "From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning",
    "authors": "Lixiang Yan",
    "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
    "abstract": "The role of Artificial Intelligence (AI) in education is undergoing a rapid transformation, moving beyond its historical function as an instructional tool towards a new potential as an active participant in the learning process. This shift is driven by the emergence of agentic AI, autonomous systems capable of proactive, goal-directed action. However, the field lacks a robust conceptual framework to understand, design, and evaluate this new paradigm of human-AI interaction in learning. This paper addresses this gap by proposing a novel conceptual framework (the APCP framework) that charts the transition from AI as a tool to AI as a collaborative partner. We present a four-level model of escalating AI agency within human-AI collaborative learning: (1) the AI as an Adaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a Co-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural theories of learning and Computer-Supported Collaborative Learning (CSCL), this framework provides a structured vocabulary for analysing the shifting roles and responsibilities between human and AI agents. The paper further engages in a critical discussion of the philosophical underpinnings of collaboration, examining whether an AI, lacking genuine consciousness or shared intentionality, can be considered a true collaborator. We conclude that while AI may not achieve authentic phenomenological partnership, it can be designed as a highly effective functional collaborator. This distinction has significant implications for pedagogy, instructional design, and the future research agenda for AI in education, urging a shift in focus towards creating learning environments that harness the complementary strengths of both human and AI.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.14832",
    "url": "https://arxiv.org/abs/2508.14832",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "On Defining Neural Averaging",
    "authors": "Su Hyeong Lee, Richard Ngo",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "What does it even mean to average neural networks? We investigate the problem of synthesizing a single neural network from a collection of pretrained models, each trained on disjoint data shards, using only their final weights and no access to training data. In forming a definition of neural averaging, we take insight from model soup, which appears to aggregate multiple models into a singular model while enhancing generalization performance. In this work, we reinterpret model souping as a special case of a broader framework: Amortized Model Ensembling (AME) for neural averaging, a data-free meta-optimization approach that treats model differences as pseudogradients to guide neural weight updates. We show that this perspective not only recovers model soup but enables more expressive and adaptive ensembling strategies. Empirically, AME produces averaged neural solutions that outperform both individual experts and model soup baselines, especially in out-of-distribution settings. Our results suggest a principled and generalizable notion of data-free model weight aggregation and defines, in one sense, how to perform neural averaging.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14844",
    "url": "https://arxiv.org/abs/2508.14844",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations",
    "authors": "Murat Isik, Mandeep Kaur Saggi, Humaira Gowher, Sabre Kais",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Accurately predicting enzyme functionality remains one of the major challenges in computational biology, particularly for enzymes with limited structural annotations or sequence homology. We present a novel multimodal Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC) classification by integrating four complementary biochemical modalities: protein sequence embeddings, quantum-derived electronic descriptors, molecular graph structures, and 2D molecular image representations. Quantum Vision Transformer (QVT) backbone equipped with modality-specific encoders and a unified cross-attention fusion module. By integrating graph features and spatial patterns, our method captures key stereoelectronic interactions behind enzyme function. Experimental results demonstrate that our multimodal QVT model achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a substantial margin and achieving better performance results compared to other QML models.",
    "comments": "Accepted at IEEE International Conference on Quantum Artificial Intelligence (QAI) 2025",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14848",
    "url": "https://arxiv.org/abs/2508.14848",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach",
    "authors": "Qiao Zhang, Rabab Alomairy, Dali Wang, Zhuowei Gu, Qinglei Cao",
    "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)",
    "abstract": "General Matrix Multiplication (GEMM) is a critical operation underpinning a wide range of applications in high-performance computing (HPC) and artificial intelligence (AI). The emergence of hardware optimized for low-precision arithmetic necessitates a reevaluation of numerical algorithms to leverage mixed-precision computations, achieving improved performance and energy efficiency. This research introduces an adaptive mixed-precision GEMM framework that supports different precision formats at fine-grained tile/block levels. We utilize the PaRSEC runtime system to balance workloads across various architectures. The performance scales well on ARM CPU-based Fugaku supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier supercomputer. This research aims to enhance computational efficiency and accuracy by bridging algorithmic advancements and hardware innovations, driving transformative progress in various applications.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14853",
    "url": "https://arxiv.org/abs/2508.14853",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent",
    "authors": "Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14879",
    "url": "https://arxiv.org/abs/2508.14879",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "authors": "Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14881",
    "url": "https://arxiv.org/abs/2508.14881",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.419680",
    "title": "Compute-Optimal Scaling for Value-Based Deep RL",
    "authors": "Preston Fu, Oleh Rybkin, Zhiyuan Zhou, Michal Nauman, Pieter Abbeel, Sergey Levine, Aviral Kumar",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "As models grow larger and training them becomes expensive, it becomes increasingly important to scale training recipes not just to larger models and more data, but to do so in a compute-optimal manner that extracts maximal performance per unit of compute. While such scaling has been well studied for language modeling, reinforcement learning (RL) has received less attention in this regard. In this paper, we investigate compute scaling for online, value-based deep RL. These methods present two primary axes for compute allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed compute budget, we ask: how should resources be partitioned across these axes to maximize sample efficiency? Our analysis reveals a nuanced interplay between model size, batch size, and UTD. In particular, we identify a phenomenon we call TD-overfitting: increasing the batch quickly harms Q-function accuracy for small models, but this effect is absent in large models, enabling effective use of large batch size at scale. We provide a mental model for understanding this phenomenon and build guidelines for choosing batch size and UTD to optimize compute usage. Our findings provide a grounded starting point for compute-optimal scaling in deep RL, mirroring studies in supervised learning but adapted to TD learning.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14893",
    "url": "https://arxiv.org/abs/2508.14893",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.423681",
    "title": "Virtual Community: An Open World for Humans, Robots, and Society",
    "authors": "Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Robotics (cs.RO)",
    "abstract": "The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.",
    "comments": "website this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.14896",
    "url": "https://arxiv.org/abs/2508.14896",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.423681",
    "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
    "authors": "Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.",
    "comments": "Technical Report, Work in Progress",
    "matched_keyword": "post-training",
    "matched_category": "后训练"
  },
  {
    "id": "2508.14048",
    "url": "https://arxiv.org/abs/2508.14048",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.423681",
    "title": "RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition",
    "authors": "Pengcheng Wang, Sheng Li, Takahiro Shinozaki",
    "subjects": "Audio and Speech Processing (eess.AS); Computation and Language (cs.CL)",
    "abstract": "In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I) with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR hypothesis queries a vector store of audio-text pairs and domain terms, and the retrieved results are fused with the live ASR hypotheses to fix recognition errors. The fused hypotheses are passed to the LLM, yielding improved responses.",
    "comments": "accepted at Interspeech2025 MLC-SLM Challenge workshop (task I system description)",
    "matched_keyword": "retrieval-augmented generation",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2508.14130",
    "url": "https://arxiv.org/abs/2508.14130",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.427680",
    "title": "EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition",
    "authors": "Hugo Thimonier, Antony Perzo, Renaud Seguier",
    "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG)",
    "abstract": "Emotion recognition from speech is a challenging task that requires capturing both linguistic and paralinguistic cues, with critical applications in human-computer interaction and mental health monitoring. Recent works have highlighted the ability of Large Language Models (LLMs) to perform tasks outside of the sole natural language area. In particular, recent approaches have investigated coupling LLMs with other data modalities by using pre-trained backbones and different fusion mechanisms. This work proposes a novel approach that fine-tunes an LLM with audio and text representations for emotion prediction. Our method first extracts audio features using an audio feature extractor, which are then mapped into the LLM's representation space via a learnable interfacing module. The LLM takes as input (1) the transformed audio features, (2) additional features in the form of natural language (e.g., the transcript), and (3) a textual prompt describing the emotion prediction task. To efficiently adapt the LLM to this multimodal task, we employ Low-Rank Adaptation (LoRA), enabling parameter-efficient fine-tuning. Experimental results on standard emotion recognition benchmarks demonstrate that our model outperforms all but one existing Speech-Text LLMs in the literature, while requiring less than half the parameters of competing approaches. This highlights our approach's effectiveness in integrating multi-modal inputs for speech-based emotion understanding while maintaining significant computational efficiency.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14755",
    "url": "https://arxiv.org/abs/2508.14755",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.434454",
    "title": "Reliable generation of isomorphic physics problems using ChatGPT with prompt-chaining and tool use",
    "authors": "Zhongzhou Chen",
    "subjects": "Physics Education (physics.ed-ph); Artificial Intelligence (cs.AI)",
    "abstract": "We present a method for generating large numbers of isomorphic physics problems using ChatGPT through prompt chaining and tool use. This approach enables precise control over structural variations-such as numeric values and spatial relations-while supporting diverse contextual variations in the problem body. By utilizing the Python code interpreter, the method supports automatic solution validation and simple diagram generation, addressing key limitations in existing LLM-based methods. We generated two example isomorphic problem banks and compared the outcome against simpler prompt-based approaches. Results show that prompt-chaining produces significantly higher quality and more consistent outputs than simpler, non-chaining prompts. This work demonstrates a promising method for efficient problem creation accessible to the average instructor, which opens new possibilities for personalized adaptive testing and automated content development.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14869",
    "url": "https://arxiv.org/abs/2508.14869",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.435972",
    "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models",
    "authors": "Hend Al-Khalifa, Raneem Almansour, Layan Abdulrahman Alhuasini, Alanood Alsaleh, Mohamad-Hani Temsah, Mohamad-Hani_Temsah, Ashwag Rafea S Alruwaili",
    "subjects": "Neurons and Cognition (q-bio.NC); Computation and Language (cs.CL)",
    "abstract": "Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2208.07563",
    "url": "https://arxiv.org/abs/2208.07563",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.435972",
    "title": "Reinforcement Learning to Rank Using Coarse-grained Rewards",
    "authors": "Yiteng Tu, Zhichao Xu, Tao Yang, Weihang Su, Yujia Zhou, Yiqun Liu, Fen Lin, Qin Liu, Qingyao Ai",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Learning to rank (LTR) plays a crucial role in various Information Retrieval (IR) tasks. Although supervised LTR methods based on fine-grained relevance labels (e.g., document-level annotations) have achieved significant success, their reliance on costly and potentially biased annotations limits scalability and alignment with realistic goals. In contrast, coarse-grained feedback signals, such as duration time and session-level engagement, are more accessible and affordable. Reinforcement Learning (RL) offers a promising framework to directly optimize these objectives using reward signals, but most existing Reinforcement Learning to Rank (RLTR) approaches suffer from high variance and low sample efficiency. Motivated by recent advances in large language models (LLMs), we re-examine the problem of RLTR with coarse-grained rewards and propose new RLTR methods based on widely used RL algorithms for LLMs. We systematically compare supervised learning and RL-based methods across various model architectures and coarse-grained reward functions on large-scale LTR benchmarks. Experimental results demonstrate that advanced RL methods can directly learn from coarse-grained rewards and outperform strong supervised learning baselines even with fine-grained labels. This shows the great potential of RLTR for metric-agnostic ranking optimization.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2312.11370",
    "url": "https://arxiv.org/abs/2312.11370",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.435972",
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "authors": "Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.",
    "comments": "10 pages",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2402.00162",
    "url": "https://arxiv.org/abs/2402.00162",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.435972",
    "title": "Behind the Myth of Exploration in Policy Gradients",
    "authors": "Adrien Bolland, Gaspard Lambrechts, Damien Ernst",
    "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
    "abstract": "In order to compute near-optimal policies with policy-gradient algorithms, it is common in practice to include intrinsic exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis with the lens of numerical optimization. Two criteria are introduced on the learning objective and two others on its stochastic gradient estimates, and are afterwards used to discuss the quality of the policy after optimization. The analysis sheds light on two separate effects of exploration techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter updates eventually provide an optimal policy. We empirically illustrate these effects with exploration strategies based on entropy bonuses, identifying limitations and suggesting directions for future work.",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2402.03055",
    "url": "https://arxiv.org/abs/2402.03055",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.439982",
    "title": "Deep Exploration with PAC-Bayes",
    "authors": "Bahareh Tasdighi, Manuel Haussmann, Nicklas Werge, Yi-Shan Wu, Melih Kandemir",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Reinforcement learning (RL) for continuous control under delayed rewards is an under-explored problem despite its significance in real-world applications. Many complex skills are based on intermediate ones as prerequisites. For instance, a humanoid locomotor must learn how to stand before it can learn to walk. To cope with delayed reward, an agent must perform deep exploration. However, existing deep exploration methods are designed for small discrete action spaces, and their generalization to state-of-the-art continuous control remains unproven. We address the deep exploration problem for the first time from a PAC-Bayesian perspective in the context of actor-critic learning. To do this, we quantify the error of the Bellman operator through a PAC-Bayes bound, where a bootstrapped ensemble of critic networks represents the posterior distribution, and their targets serve as a data-informed function-space prior. We derive an objective function from this bound and use it to train the critic ensemble. Each critic trains an individual soft actor network, implemented as a shared trunk and critic-specific heads. The agent performs deep exploration by acting epsilon-softly on a randomly chosen actor head. Our proposed algorithm, named {\\it PAC-Bayesian Actor-Critic (PBAC)}, is the only algorithm to consistently discover delayed rewards on continuous control tasks with varying difficulty.",
    "comments": "ECAI camera-ready version; fixed acknowledgements; fixed github reference",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2403.17983",
    "url": "https://arxiv.org/abs/2403.17983",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.439982",
    "title": "Is The Watermarking Of LLM-Generated Code Robust?",
    "authors": "Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic",
    "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
    "abstract": "We present the first in depth study on the robustness of existing watermarking techniques applied to code generated by large language models (LLMs). As LLMs increasingly contribute to software development, watermarking has emerged as a potential solution for detecting AI generated code and mitigating misuse, such as plagiarism or the automated generation of malicious programs. While previous research has demonstrated the resilience of watermarking in the text setting, our work reveals that watermarking techniques are significantly more fragile in code-based contexts. Specifically, we show that simple semantic-preserving transformations, such as variable renaming and dead code insertion, can effectively erase watermarks without altering the program's functionality. To systematically evaluate watermark robustness, we develop an algorithm that traverses the Abstract Syntax Tree (AST) of a watermarked program and applies a sequence of randomized, semantics-preserving transformations. Our experimental results, conducted on Python code generated by different LLMs, indicate that even minor modifications can drastically reduce watermark detectability, with true positive rates (TPR) dropping below 50% in many cases. Our code is publicly available at this https URL.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2405.11032",
    "url": "https://arxiv.org/abs/2405.11032",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.439982",
    "title": "Game-theoretic Energy Management Strategies With Interacting Agents in Formula 1",
    "authors": "Giona Fieni, Marc-Philippe Neumann, Alessandro Zanardi, Alberto Cerofolini, Christopher H. Onder",
    "subjects": "Systems and Control (eess.SY)",
    "abstract": "This paper presents an interaction-aware energy management optimization framework for Formula 1 racing. The considered scenario involves two agents and a drag reduction model. Strategic interactions between the agents are captured by a Stackelberg game formulated as a bilevel program. To address the computational challenges associated with bilevel optimization, the problem is reformulated as a single-level nonlinear program employing the Karush-Kuhn-Tucker conditions. The proposed framework contributes towards the development of new energy management and allocation strategies, caused by the presence of another agent. For instance, it provides valuable insights on how to redistribute the energy in order to optimally exploit the wake effect, showcasing a notable difference with the behavior studied in previous works. Robust energy allocations can be identified to reduce the lap time loss associated with unexpected choices of the other agent. It allows to recognize the boundary conditions for the interaction to become relevant, impacting the system's behavior, and to assess if overtaking is possible and beneficial. Overall, the framework provides a comprehensive approach for a two-agent Formula 1 racing problem with strategic interactions, offering physically intuitive and practical results.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2405.17604",
    "url": "https://arxiv.org/abs/2405.17604",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.439982",
    "title": "LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters",
    "authors": "Klaudia Bałazy, Mohammadreza Banaei, Karl Aberer, Jacek Tabor",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "The growth of large language models underscores the need for parameter-efficient fine-tuning. Despite its popularity, LoRA encounters storage and computational challenges when deploying multiple task- or user-specific modules. To address this, we introduce LoRA-XS, a novel fine-tuning method backed by a theoretical derivation. LoRA-XS drastically reduces trainable parameters by incorporating a small, trainable weight matrix between frozen low-rank matrices derived from the Singular Value Decomposition of pre-trained weights. This design enables LoRA-XS to reduce storage requirements by over 100x in 7B models compared to LoRA. Additionally, unlike other methods, LoRA-XS imposes no lower bound on trainable parameters - it can scale from a single parameter per module to arbitrarily large values, adapting to any storage or computational constraint. Evaluations on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks across different model scales reveal that LoRA-XS consistently outperforms or matches LoRA and VeRA in accuracy, offering unmatched parameter efficiency. Our ablation studies highlight the significance of singular vectors in transformer weights, establishing LoRA-XS as a powerful, storage-efficient solution for scaling and personalizing large language models.",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2406.05881",
    "url": "https://arxiv.org/abs/2406.05881",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.443976",
    "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning",
    "authors": "Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri",
    "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Robotics (cs.RO)",
    "abstract": "Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2407.05311",
    "url": "https://arxiv.org/abs/2407.05311",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.443976",
    "title": "MMAD: Multi-label Micro-Action Detection in Videos",
    "authors": "Kun Li, Pengyu Liu, Dan Guo, Fei Wang, Zhiliang Wu, Hehe Fan, Meng Wang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Human body actions are an important form of non-verbal communication in social interactions. This paper specifically focuses on a subset of body actions known as micro-actions, which are subtle, low-intensity body movements with promising applications in human emotion analysis. In real-world scenarios, human micro-actions often temporally co-occur, with multiple micro-actions overlapping in time, such as concurrent head and hand movements. However, current research primarily focuses on recognizing individual micro-actions while overlooking their co-occurring nature. To address this gap, we propose a new task named Multi-label Micro-Action Detection (MMAD), which involves identifying all micro-actions in a given short video, determining their start and end times, and categorizing them. Accomplishing this requires a model capable of accurately capturing both long-term and short-term action relationships to detect multiple overlapping micro-actions. To facilitate the MMAD task, we introduce a new dataset named Multi-label Micro-Action-52 (MMA-52) and propose a baseline method equipped with a dual-path spatial-temporal adapter to address the challenges of subtle visual change in MMAD. We hope that MMA-52 can stimulate research on micro-action analysis in videos and prompt the development of spatio-temporal modeling in human-centric video understanding. The proposed MMA-52 dataset is available at: this https URL.",
    "comments": "Accepted by ICCV 2025",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2407.16661",
    "url": "https://arxiv.org/abs/2407.16661",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.443976",
    "title": "Regenerative Ulam-von Neumann Algorithm: An Innovative Markov chain Monte Carlo Method for Matrix Inversion",
    "authors": "Soumyadip Ghosh, Lior Horesh, Vassilis Kalantzis, Yingdong Lu, Tomasz Nowicki",
    "subjects": "Numerical Analysis (math.NA); Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)",
    "abstract": "This paper presents a regenerative variant of the classical Ulam-von Neumann Markov chain Monte Carlo algorithm for the approximation of the matrix inverse. The algorithm presented in this paper, termed regenerative Ulam-von Neumann algorithm, utilizes the regenerative structure of classical, non-truncated Neumann series defined by a non-singular matrix and produces an estimator of the matrix inverse via ratios of unbiased estimators of the regenerative quantities. The accuracy of the proposed algorithm depends on a single parameter that controls the total number of simulated Markov transitions, thus avoiding the challenge of balancing between the total number of Markov chain replications and their length as in the classical Ulam-von Neumann algorithm. To efficiently utilize Markov chain transition samples in the calculation of the regenerative variables, the proposed algorithm automatically quantifies the contribution of each Markov transition to all regenerative quantities by a carefully designed updating scheme that utilized three separate matrices containing the current weights, total weights, and regenerative cycle count, respectively. A probabilistic analysis of the performance of the algorithm, including the variance of the estimator, is provided. Finally, numerical experiments verify the effectiveness of the proposed scheme.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2408.06569",
    "url": "https://arxiv.org/abs/2408.06569",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.443976",
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "authors": "Harry Cheng, Yangyang Guo, Qingpei Guo, Ming Yang, Tian Gan, Weili Guan, Liqiang Nie",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks.",
    "comments": "Project page: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2410.03136",
    "url": "https://arxiv.org/abs/2410.03136",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model",
    "authors": "Siheng Xiong, Ali Payani, Yuan Yang, Faramarz Fekri",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.",
    "comments": "ACL25 (main)",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2410.03844",
    "url": "https://arxiv.org/abs/2410.03844",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "Projected Walk on Spheres: A Monte Carlo Closest Point Method for Surface PDEs",
    "authors": "Ryusuke Sugimoto, Nathan King, Toshiya Hachisuka, Christopher Batty",
    "subjects": "Numerical Analysis (math.NA); Graphics (cs.GR)",
    "abstract": "We present projected walk on spheres (PWoS), a novel pointwise and discretization-free Monte Carlo solver for surface PDEs with Dirichlet boundaries, as a generalization of the walk on spheres method (WoS) [Muller 1956; Sawhney and Crane 2020]. We adapt the recursive relationship of WoS designed for PDEs in volumetric domains to a volumetric neighborhood around the surface, and at the end of each recursion step, we project the sample point on the sphere back to the surface. We motivate this simple modification to WoS with the theory of the closest point extension used in the closest point method. To define the valid volumetric neighborhood domain for PWoS, we develop strategies to estimate the local feature size of the surface and to compute the distance to the Dirichlet boundaries on the surface extended in their normal directions. We also design a mean value filtering method for PWoS to improve the method's efficiency when the surface is represented as a polygonal mesh or a point cloud. Finally, we study the convergence of PWoS and demonstrate its application to graphics tasks, including diffusion curves, geodesic distance computation, and wave propagation animation. We show that our method works with various types of surfaces, including a surface of mixed codimension.",
    "comments": "Accepted to SIGGRAPH Asia 2024 (Conference Papers). See this https URL for updates",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2410.11571",
    "url": "https://arxiv.org/abs/2410.11571",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "SDS -- See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration",
    "authors": "Maria Stamatopoulou, Jeffrey Li, Dimitrios Kanoulas",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Imagine a robot learning locomotion skills from any single video, without labels or reward engineering. We introduce SDS (\"See it. Do it. Sorted.\"), an automated pipeline for skill acquisition from unstructured demonstrations. Using GPT-4o, SDS applies novel prompting techniques, in the form of spatio-temporal grid-based visual encoding ($G_{v}$) and structured input decomposition (SUS). These produce executable reward functions (RF) from the raw input videos. The RFs are used to train PPO policies and are optimized through closed-loop evolution, using training footage and performance metrics as self-supervised signals. SDS allows quadrupeds (e.g. Unitree Go1) to learn four gaits -- trot, bound, pace, and hop -- achieving 100% gait matching fidelity, Dynamic Time Warping (DTW) distance in the order of $10^{-6}$, and stable locomotion with zero failures, both in simulation and the real world. SDS generalizes to morphologically different quadrupeds (e.g. ANYmal) and outperforms prior work in data efficiency, training time and engineering effort. Further materials and the code are open-source under: this https URL.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2410.23805",
    "url": "https://arxiv.org/abs/2410.23805",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "UpANNS: Enhancing Billion-Scale ANNS Efficiency with Real-World PIM Architecture",
    "authors": "Sitian Chen, Amelie Chi Zhou, Yucheng Shi, Yusen Li, Xin Yao",
    "subjects": "Hardware Architecture (cs.AR)",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) is a critical component of modern AI systems, such as recommendation engines and retrieval-augmented large language models (RAG-LLMs). However, scaling ANNS to billion-entry datasets exposes critical inefficiencies: CPU-based solutions are bottlenecked by memory bandwidth limitations, while GPU implementations underutilize hardware resources, leading to suboptimal performance and energy consumption. To address these challenges, we introduce \\emph{UpANNS}, a novel framework leveraging Processing-in-Memory (PIM) architecture to accelerate billion-scale ANNS. UpANNS integrates four key innovations, including 1) architecture-aware data placement to minimize latency through workload balancing, 2) dynamic resource management for optimal PIM utilization, 3) co-occurrence optimized encoding to reduce redundant computations, and 4) an early-pruning strategy for efficient top-k selection. Evaluation on commercial UPMEM hardware demonstrates that UpANNS achieves 4.3x higher QPS than CPU-based Faiss, while matching GPU performance with 2.3x greater energy efficiency. Its near-linear scalability ensures practicality for growing datasets, making it ideal for applications like real-time LLM serving and large-scale retrieval systems.",
    "comments": "Accepted by SC 25",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2411.02433",
    "url": "https://arxiv.org/abs/2411.02433",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models",
    "authors": "Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (Gemma, Qwen, Mixtral, gpt-oss) and scales (from 1B to 45B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks and the results demonstrate that SLED consistently improves factual accuracy compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.",
    "comments": "Accepted at NeurIPS 2024; project page is available at this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2411.16748",
    "url": "https://arxiv.org/abs/2411.16748",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.447982",
    "title": "Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance",
    "authors": "Haojie Zhang, Zhihao Liang, Ruibo Fu, Bingyan Liu, Zhengqi Wen, Xuefei Liu, Jianhua Tao, Yaling Liang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Long-duration talking video synthesis faces persistent challenges in simultaneously achieving high video quality, portrait and temporal consistency, and computational efficiency. As video length increases, issues such as visual degradation, loss of identity consistency, temporal incoherence, and error accumulation become increasingly prominent, severely impacting the realism and reliability of generated results. To address these issues, we present LetsTalk, a diffusion transformer framework that incorporates multimodal guidance and a novel memory bank mechanism, explicitly maintaining contextual continuity and enabling robust, high-quality, and efficient long-duration talking video generation. Specifically, LetsTalk introduces a memory bank combined with a noise-regularized training strategy to mitigate error accumulation and sampling artifacts during long video generation. To further enhance efficiency and spatiotemporal consistency, LetsTalk employs a deep compression autoencoder and a spatiotemporal-aware transformer with linear attention for effective multimodal fusion. Furthermore, we systematically analyze three multimodal fusion schemes, adopting deep (Symbiotic Fusion) for portrait features to ensure visual consistency, and shallow (Direct Fusion) for audio to synchronize animation with speech while preserving motion diversity. Extensive experiments demonstrate that LetsTalk achieves state-of-the-art generation quality, producing temporally coherent and realistic talking videos with enhanced diversity and liveliness, while maintaining remarkable efficiency with 8 fewer parameters than previous approaches.",
    "comments": "13 pages, 11 figures",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2412.04465",
    "url": "https://arxiv.org/abs/2412.04465",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.451983",
    "title": "UnZipLoRA: Separating Content and Style from a Single Image",
    "authors": "Chang Liu, Viraj Shah, Aiyu Cui, Svetlana Lazebnik",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA.",
    "comments": "Project page: this https URL",
    "matched_keyword": "lora",
    "matched_category": "微调"
  },
  {
    "id": "2501.08096",
    "url": "https://arxiv.org/abs/2501.08096",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.451983",
    "title": "Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving",
    "authors": "Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, Chen Sun",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
    "abstract": "Reinforcement Learning (RL) has shown excellent performance in solving decision-making and control problems of autonomous driving, which is increasingly applied in diverse driving scenarios. However, driving is a multi-attribute problem, leading to challenges in achieving multi-objective compatibility for current RL methods, especially in both policy updating and policy execution. On the one hand, a single value evaluation network limits the policy updating in complex scenarios with coupled driving objectives. On the other hand, the common single-type action space structure limits driving flexibility or results in large behavior fluctuations during policy execution. To this end, we propose a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action for multi-objective compatible autonomous driving. Specifically, an advanced MORL architecture is constructed, in which the ensemble-critic focuses on different objectives through independent reward functions. The architecture integrates a hybrid parameterized action space structure, and the generated driving actions contain both abstract guidance that matches the hybrid road modality and concrete control commands. Additionally, an uncertainty-based exploration mechanism that supports hybrid actions is developed to learn multi-objective compatible policies more quickly. Experimental results demonstrate that, in both simulator-based and HighD dataset-based multi-lane highway scenarios, our method efficiently learns multi-objective compatible autonomous driving with respect to efficiency, action consistency, and safety.",
    "comments": "13 pages, 10 figures, 5 tables, Submitted to IEEE T-NNLS (under review, 2nd round)",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2501.13368",
    "url": "https://arxiv.org/abs/2501.13368",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.451983",
    "title": "MetaWild: A Multimodal Dataset for Animal Re-Identification with Environmental Metadata",
    "authors": "Yuzhuo Li, Di Zhao, Tingrui Qiao, Yihao Wu, Bo Pang, Yun Sing Koh",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
    "abstract": "Identifying individual animals within large wildlife populations is essential for effective wildlife monitoring and conservation efforts. Recent advancements in computer vision have shown promise in animal re-identification (Animal ReID) by leveraging data from camera traps. However, existing Animal ReID datasets rely exclusively on visual data, overlooking environmental metadata that ecologists have identified as highly correlated with animal behavior and identity, such as temperature and circadian rhythms. Moreover, the emergence of multimodal models capable of jointly processing visual and textual data presents new opportunities for Animal ReID, but existing datasets fail to leverage these models' text-processing capabilities, limiting their full potential. Additionally, to facilitate the use of metadata in existing ReID methods, we propose the Meta-Feature Adapter (MFA), a lightweight module that can be incorporated into existing vision-language model (VLM)-based Animal ReID methods, allowing ReID models to leverage both environmental metadata and visual information to improve ReID performance. Experiments on MetaWild show that combining baseline ReID models with MFA to incorporate metadata consistently improves performance compared to using visual information alone, validating the effectiveness of incorporating metadata in re-identification. We hope that our proposed dataset can inspire further exploration of multimodal approaches for Animal ReID.",
    "comments": "7 pages, 6 figures",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2502.02329",
    "url": "https://arxiv.org/abs/2502.02329",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.451983",
    "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs",
    "authors": "Yuan Tian, Chuhan Zhang, Xiaotong Wang, Sitong Pan, Weiwei Cui, Haidong Zhang, Dazhen Deng, Yingcai Wu",
    "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL)",
    "abstract": "Creating data reports is a labor-intensive task involving iterative data exploration, insight extraction, and narrative construction. A key challenge lies in composing the analysis logic-from defining objectives and transforming data to identifying and communicating insights. Manually crafting this logic can be cognitively demanding. While experienced analysts often reuse scripts from past projects, finding a perfect match for a new dataset is rare. Even when similar analyses are available online, they usually share only results or visualizations, not the underlying code, making reuse difficult. To address this, we present ReSpark, a system that leverages large language models (LLMs) to reverse-engineer analysis logic from existing reports and adapt it to new datasets. By generating draft analysis steps, ReSpark provides a warm start for users. It also supports interactive refinement, allowing users to inspect intermediate outputs, insert objectives, and revise content. We evaluate ReSpark through comparative and user studies, demonstrating its effectiveness in lowering the barrier to generating data reports without relying on existing analysis code.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2502.11277",
    "url": "https://arxiv.org/abs/2502.11277",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.455983",
    "title": "\"When I lost it, they dragged me out\": How Care Encounters Empower Marginalized Young Adults' Aspiration and Mental Health Care-Seeking",
    "authors": "Jiaying \"Lizzy\" Liu, Yan Zhang",
    "subjects": "Human-Computer Interaction (cs.HC)",
    "abstract": "Mental health care-seeking among marginalized young adults has received limited attention in CSCW research. Through in-depth interviews and visual elicitation methods with 18 diverse U.S. participants, our study reveals how marginalized identities shape mental health care-seeking journeys, often characterized by low aspirations and passive care-seeking influenced by lived experiences of marginalization. However, we found the transformative function of \"care encounters\" - serendipitous interactions with mental health resources that occur when individuals are not actively seeking support. These encounters serve as critical turning points, catalyzing shifts in aspiration and enabling more proactive care-seeking behaviors. Our analysis identifies both the infrastructural conditions that enable transformative care encounters and the aspiration breakdowns that impede care-seeking processes. This work makes conceptual contributions by supplementing traditional motivation-based care-seeking models with a reconceptualization of \"care encounters\" that accounts for the infrastructural and serendipitous nature of mental health access. We advance understanding of how marginalized identity uniquely influences care-seeking behaviors while providing actionable design implications for embedding technology-mediated \"care encounters\" into socio-technical interventions that can better support mental health care access for vulnerable populations.",
    "comments": "Accepted by CSCW 25. arXiv admin note: text overlap with arXiv:2401.08994",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2502.13953",
    "url": "https://arxiv.org/abs/2502.13953",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.455983",
    "title": "Benchmarking graph construction by large language models for coherence-driven inference",
    "authors": "Steve Huntsman, Jewell Thomas",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "We devise an algorithm to generate propositions that objectively instantiate graphs supporting coherence-driven inference. We also benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a simple transformation of) propositions expressed in natural language, with promising results from a single prompt to reasoning-optimized LLMs. For example, o1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs. Coherence-driven inference on consistency evaluations by LLMs may advance machine cognition capabilities.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2502.14496",
    "url": "https://arxiv.org/abs/2502.14496",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.455983",
    "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization",
    "authors": "Zhitao He, Zijun Liu, Peng Li, Yi R. Fung, Ming Yan, Ji Zhang, Fei Huang, Yang Liu",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.",
    "comments": "Published in COLM2025",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2503.21755",
    "url": "https://arxiv.org/abs/2503.21755",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.455983",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness",
    "authors": "Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored to individual dimensions, our evaluation framework integrates generalists such as SOTA VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive human annotations to ensure evaluation alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.",
    "comments": "Equal contributions from first two authors. Project page: this https URL Code: this https URL",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2503.24157",
    "url": "https://arxiv.org/abs/2503.24157",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.459983",
    "title": "LLM4FS: Leveraging Large Language Models for Feature Selection",
    "authors": "Jianhao Li, Xianchao Xiu",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making. Our code is available at this https URL.",
    "comments": "CAC 2025",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.01519",
    "url": "https://arxiv.org/abs/2504.01519",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.461475",
    "title": "Chain of Correction for Full-text Speech Recognition with Large Language Models",
    "authors": "Zhiyuan Tang, Dong Wang, Zhikai Zhou, Yong Liu, Shen Huang, Shidong Shang",
    "subjects": "Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)",
    "abstract": "Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) is attracting increased attention for its ability to address a wide range of error types, such as punctuation restoration and inverse text normalization, across long context. However, challenges remain regarding stability, controllability, completeness, and fluency. To mitigate these issues, this paper proposes the Chain of Correction (CoC), which uses a multi-turn chat format to correct errors segment by segment, guided by pre-recognized text and full-text context for better semantic understanding. Utilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to evaluate CoC's performance. Experiments show that CoC significantly outperforms baseline and benchmark systems in correcting full-text ASR outputs. We also analyze correction thresholds to balance under-correction and over-rephrasing, extrapolate CoC on extra-long ASR outputs, and explore using other types of information to guide error correction.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.02906",
    "url": "https://arxiv.org/abs/2504.02906",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.461475",
    "title": "Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement",
    "authors": "Zhihan Zhang, Yixin Cao, Lizi Liao",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at this https URL.",
    "comments": "Accepted by ACM MM 2025",
    "matched_keyword": "mllm",
    "matched_category": "多模态"
  },
  {
    "id": "2504.05846",
    "url": "https://arxiv.org/abs/2504.05846",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.462483",
    "title": "PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models",
    "authors": "Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Path recommendation (PR) aims to generate travel paths that are customized to a user's specific preferences and constraints. Conventional approaches often employ explicit optimization objectives or specialized machine learning architectures; however, these methods typically exhibit limited flexibility and generalizability, necessitating costly retraining to accommodate new scenarios. This paper introduces an alternative paradigm that conceptualizes PR as a natural language generation task. We present PathGPT, a retrieval-augmented large language model (LLM) system that leverages historical trajectory data and natural language user constraints to generate plausible paths. The proposed methodology first converts raw trajectory data into a human-interpretable textual format, which is then stored in a database. Subsequently, a hybrid retrieval system extracts path-specific context from this database to inform a pretrained LLM. The primary contribution of this work is a novel framework that demonstrates how integrating established information retrieval and generative model components can enable adaptive, zero-shot path generation across diverse scenarios. Extensive experiments on large-scale trajectory datasets indicate that PathGPT's performance is competitive with specialized, learning-based methods, underscoring its potential as a flexible and generalizable path generation system that avoids the need for retraining inherent in previous data-driven models.",
    "matched_keyword": "gpt",
    "matched_category": "大模型"
  },
  {
    "id": "2504.11695",
    "url": "https://arxiv.org/abs/2504.11695",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.463483",
    "title": "Interpreting the linear structure of vision-language model embedding spaces",
    "authors": "Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
    "abstract": "Vision-language models encode images and text in a joint space, minimizing the distance between corresponding image and text pairs. How are language and images organized in this joint space, and how do the models encode meaning and modality? To investigate this, we train and release sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, and AIMv2). SAEs approximate model embeddings as sparse linear combinations of learned directions, or \"concepts\". We find that, compared to other methods of linear feature learning, SAEs are better at reconstructing the real embeddings, while also able to retain the most sparsity. Retraining SAEs with different seeds or different data diet leads to two findings: the rare, specific concepts captured by the SAEs are liable to change drastically, but we also show that commonly-activating concepts are remarkably stable across runs. Interestingly, while most concepts activate primarily for one modality, we find they are not merely encoding modality per se. Many are almost orthogonal to the subspace that defines modality, and the concept directions do not function as good modality classifiers, suggesting that they encode cross-modal semantics. To quantify this bridging behavior, we introduce the Bridge Score, a metric that identifies concept pairs which are both co-activated across aligned image-text inputs and geometrically aligned in the shared space. This reveals that even single-modality concepts can collaborate to support cross-modal integration. We release interactive demos of the SAEs for all models, allowing researchers to explore the organization of the concept spaces. Overall, our findings uncover a sparse linear structure within VLM embedding spaces that is shaped by modality, yet stitched together through latent bridges, offering new insight into how multimodal meaning is constructed.",
    "comments": "COLM 2025",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2504.19061",
    "url": "https://arxiv.org/abs/2504.19061",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.464482",
    "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models",
    "authors": "Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
    "abstract": "Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, including admission reasons, major in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. Our results reveal that while the LLMs (e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission reasons and hospitalization events, they are generally less consistent when it comes to identifying follow-up recommendations, highlighting broader challenges in leveraging LLMs for comprehensive summarization.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2504.19254",
    "url": "https://arxiv.org/abs/2504.19254",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.464482",
    "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers",
    "authors": "Dylan Bouchard, Mohit Singh Chauhan",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.",
    "comments": "UQLM repository: this https URL",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2504.19959",
    "url": "https://arxiv.org/abs/2504.19959",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.465481",
    "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL Verification",
    "authors": "Junhao Ye, Yuchen Hu, Ke Xu, Dingrong Pan, Qichun Chen, Jie Zhou, Shuai Zhao, Xinwei Fang, Xi Wang, Nan Guan, Zhe Jiang",
    "subjects": "Hardware Architecture (cs.AR)",
    "abstract": "Verification presents a major bottleneck in Integrated Circuit (IC) development, consuming nearly 70% of the total development effort. While the Universal Verification Methodology (UVM) is widely used in industry to improve verification efficiency through structured and reusable testbenches, constructing these testbenches and generating sufficient stimuli remain challenging. These challenges arise from the considerable manual coding effort required, repetitive manual execution of multiple EDA tools, and the need for in-depth domain expertise to navigate complex this http URL, we present UVM^2, an automated verification framework that leverages Large Language Models (LLMs) to generate UVM testbenches and iteratively refine them using coverage feedback, significantly reducing manual effort while maintaining rigorous verification this http URL evaluate UVM^2, we introduce a benchmark suite comprising Register Transfer Level (RTL) designs of up to 1.6K lines of this http URL results show that UVM^2 reduces testbench setup time by up to UVM^2 compared to experienced engineers, and achieve average code and function coverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by 20.96% and 23.51%, respectively.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2505.07773",
    "url": "https://arxiv.org/abs/2505.07773",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.465481",
    "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving",
    "authors": "Xinji Mai, Haotian Xu, Zhong-Zhi Li, Xing W, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{this https URL}{this https URL\\_async\\_pipline}.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2505.18889",
    "url": "https://arxiv.org/abs/2505.18889",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.467481",
    "title": "Security Concerns for Large Language Models: A Survey",
    "authors": "Miles Q. Li, Benjamin C. M. Fung",
    "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: prompt injection and jailbreaking; adversarial attacks, including input perturbations and data poisoning; misuse by malicious actors to generate disinformation, phishing emails, and malware; and the worrisome risks inherent in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2505.19717",
    "url": "https://arxiv.org/abs/2505.19717",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.467481",
    "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning",
    "authors": "Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the minimum or maximum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: this https URL",
    "comments": "2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids), Sep 2025, Seoul, South Korea",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2505.24773",
    "url": "https://arxiv.org/abs/2505.24773",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.467481",
    "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption",
    "authors": "Yajie Zhou, Xiaoyi Pang, Zhibo Wang",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2506.03106",
    "url": "https://arxiv.org/abs/2506.03106",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.468482",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "authors": "Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.",
    "comments": "52 pages, updated with new experimental results and implementation details",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2506.04236",
    "url": "https://arxiv.org/abs/2506.04236",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.468482",
    "title": "Spore in the Wild: A Case Study of Spore.fun as an Open-Environment Evolution Experiment with Sovereign AI Agents on TEE-Secured Blockchains",
    "authors": "Botao Amber Hu, Helena Rong",
    "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Neural and Evolutionary Computing (cs.NE)",
    "abstract": "In Artificial Life (ALife) research, replicating Open-Ended Evolution (OEE)-the continuous emergence of novelty observed in biological life-has usually been pursued within isolated, closed system simulations, such as Tierra and Avida, which have typically plateaued after an initial burst of novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires an open-environment system that continually exchanges information or energy with its environment. A recent technological innovation in Decentralized Physical Infrastructure Network (DePIN), which provides permissionless computational substrates, enables the deployment of Large Language Model-based AI agents on blockchains integrated with Trusted Execution Environments (TEEs). This enables on-chain agents to operate autonomously \"in the wild,\" achieving self-sovereignty without human oversight. These agents can control their own social media accounts and cryptocurrency wallets, allowing them to interact directly with blockchain-based financial networks and broader human social media. Building on this new paradigm of on-chain agents, this http URL is a recent real-world AI evolution experiment that enables autonomous breeding and evolution of new on-chain agents. This paper presents a detailed case study of this http URL, examining agent behaviors and their evolutionary trajectories through digital ethology. We aim to spark discussion about whether open-environment ALife systems \"in the wild,\" based on permissionless computational substrates and driven by economic incentives to interact with their environment, could finally achieve the long-sought goal of OEE.",
    "comments": "Accepted by ALIFE 2025",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2506.08113",
    "url": "https://arxiv.org/abs/2506.08113",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.469481",
    "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting",
    "authors": "Timothée Hornek Amir Sartipi, Igor Tchappi, Gilbert Fridgen",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Statistical Finance (q-fin.ST)",
    "abstract": "Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it.",
    "matched_keyword": "pre-trained",
    "matched_category": "预训练"
  },
  {
    "id": "2506.11091",
    "url": "https://arxiv.org/abs/2506.11091",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.470481",
    "title": "Customizing Speech Recognition Model with Large Language Model Feedback",
    "authors": "Shaoshi Ling, Guoli Ye",
    "subjects": "Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
    "abstract": "Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\\% improvement on entity word error rate over conventional self-training methods.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2506.13790",
    "url": "https://arxiv.org/abs/2506.13790",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.470481",
    "title": "The NordDRG AI Benchmark for Large Language Models",
    "authors": "Tapio Pitkäranta",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Large language models (LLMs) are being piloted for clinical coding and decision support, yet no open benchmark targets the hospital-funding layer where Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD systems, DRGs route a substantial share of multi-trillion-dollar health spending through governed grouper software, making transparency and auditability first-order concerns. We release NordDRG-AI-Benchmark, the first public, rule-complete test bed for DRG reasoning. The package includes (i) machine-readable approximately 20-sheet NordDRG definition tables and (ii) expert manuals and change-log templates that capture governance workflows. It exposes two suites: a 13-task Logic benchmark (code lookup, cross-table inference, grouping features, multilingual terminology, and CC/MCC validity checks) and a 13-task Grouper benchmark that requires full DRG grouper emulation with strict exact-match scoring on both the DRG and the triggering this http URL. Lightweight reference agents (LogicAgent, GrouperAgent) enable artefact-only evaluation. Under an artefact-only (no web) setting, on the 13 Logic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier models (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining models score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5 Thinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13, and all other tested endpoints score 0/13. To our knowledge, this is the first public report of an LLM partially emulating the complete NordDRG grouper logic with governance-grade traceability. Coupling a rule-complete release with exact-match tasks and open scoring provides a reproducible yardstick for head-to-head and longitudinal evaluation in hospital funding. Benchmark materials available in Github.",
    "comments": "23 pages, 4 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2506.18897",
    "url": "https://arxiv.org/abs/2506.18897",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.470481",
    "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
    "authors": "Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo",
    "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
    "abstract": "Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.21205",
    "url": "https://arxiv.org/abs/2506.21205",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.471481",
    "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations",
    "authors": "Elia Trevisan, Khaled A. Mustafa, Godert Notten, Xinwei Wang, Javier Alonso-Mora",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Deploying mobile robots safely among humans requires the motion planner to account for the uncertainty in the other agents' predicted trajectories. This remains challenging in traditional approaches, especially with arbitrarily shaped predictions and real-time constraints. To address these challenges, we propose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI), a motion planner that incorporates uncertain future motions modelled with potentially non-Gaussian stochastic predictions. By leveraging MPPI's gradient-free nature, we propose a method that efficiently approximates the joint Collision Probability (CP) among multiple dynamic obstacles for several hundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This enables the rejection of samples exceeding a predefined CP threshold or the integration of CP as a weighted objective within the navigation cost function. Consequently, DRA-MPPI mitigates the freezing robot problem while enhancing safety. Real-world and simulated experiments with multiple dynamic obstacles demonstrate DRA-MPPI's superior performance compared to state-of-the-art approaches, including Scenario-based Model Predictive Control (S-MPC), Frenet planner, and vanilla MPPI.",
    "comments": "Accepted for presentation at IROS 2025. Accepted Version",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.22562",
    "url": "https://arxiv.org/abs/2506.22562",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.472481",
    "title": "Improving Token-based Object Detection with Video",
    "authors": "Abhineet Singh, Nilanjan Ray",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available.",
    "comments": "Published in IEEE Access",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2506.23036",
    "url": "https://arxiv.org/abs/2506.23036",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.472481",
    "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress",
    "authors": "Zain ul Abdeen, Ming Jin",
    "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
    "abstract": "This paper explores Reinforcement learning (RL) policy robustness by systematically analyzing network parameters under internal and external stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering introduces internal stress by selectively perturbing parameters, while adversarial attacks apply external stress through modified agent observations. This dual approach enables the classification of parameters as fragile, robust, or antifragile, based on their influence on policy performance in clean and adversarial settings. Parameter scores are defined to quantify these characteristics, and the framework is validated on PPO-trained agents in Mujoco continuous control environments. The results highlight the presence of antifragile parameters that enhance policy performance under stress, demonstrating the potential of targeted filtering techniques to improve RL policy adaptability. These insights provide a foundation for future advancements in the design of robust and antifragile RL systems.",
    "comments": "Withdrawn pending a review of attribution and overlap with Pravin et al., Artificial Intelligence (2024), DOI: https://doi.org/10.1016/j.artint.2023.104060. Further dissemination is paused while we determine appropriate next steps",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2507.03047",
    "url": "https://arxiv.org/abs/2507.03047",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.472481",
    "title": "Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning",
    "authors": "Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
    "abstract": "Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.\nTo address this critical gap, we propose \\underline{C}ounterfactual \\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based \\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at this https URL.",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2507.03608",
    "url": "https://arxiv.org/abs/2507.03608",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.472481",
    "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)",
    "authors": "Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi",
    "subjects": "Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)",
    "abstract": "Generative AI (GenAI) is expected to play a pivotal role in enabling autonomous optimization in future wireless networks. Within the ORAN architecture, Large Language Models (LLMs) can be specialized to generate xApps and rApps by leveraging specifications and API definitions from the RAN Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for telecom-specific tasks remains expensive and resource-intensive. Retrieval-Augmented Generation (RAG) offers a practical alternative through in-context learning, enabling domain adaptation without full retraining. While traditional RAG systems rely on vector-based retrieval, emerging variants such as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval strategies to support multi-hop reasoning and improve factual grounding. Despite their promise, these methods lack systematic, metric-driven evaluations, particularly in high-stakes domains such as ORAN. In this study, we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid GraphRAG using ORAN specifications. We assess performance across varying question complexities using established generation metrics: faithfulness, answer relevance, context relevance, and factual correctness. Results show that both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG improves factual correctness by 8%, while GraphRAG improves context relevance by 11%.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2507.04487",
    "url": "https://arxiv.org/abs/2507.04487",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.473482",
    "title": "LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization",
    "authors": "Xujia Wang, Yunjia Qi, Bin Xu",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training. The source code is available at this https URL.",
    "comments": "Accepted to EMNLP 2025 (Main Conference); 18 pages, 12 figures",
    "matched_keyword": "fine-tuning",
    "matched_category": "微调"
  },
  {
    "id": "2507.04996",
    "url": "https://arxiv.org/abs/2507.04996",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.473482",
    "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems",
    "authors": "Jiangbo Yu",
    "subjects": "Computers and Society (cs.CY); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
    "abstract": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are viewed as vehicular systems capable of perceiving their environment and executing pre-programmed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 0 to 5); Examples of this outpace include the interaction with humans with natural language, goal adaptation, contextual reasoning, external tool use, and unseen ethical dilemma handling, largely empowered by multi-modal large language models (LLMs). These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this gap, this paper introduces the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. This paper proposes the term AgVs and their distinguishing characteristics from conventional AuVs. It synthesizes relevant advances in integrating LLMs and AuVs and highlights how AgVs might transform future mobility systems and ensure the systems are human-centered. The paper concludes by identifying key challenges in the development and governance of AgVs, and how they can play a significant role in future agentic transportation systems.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2507.06484",
    "url": "https://arxiv.org/abs/2507.06484",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.473482",
    "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds",
    "authors": "Fan-Yun Sun, Shengguang Wu, Christian Jacobsen, Thomas Yim, Haoming Zou, Alex Zook, Shangru Li, Yu-Hsin Chou, Ethem Can, Xunlei Wu, Clemens Eppner, Valts Blukis, Jonathan Tremblay, Jiajun Wu, Stan Birchfield, Nick Haber",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Despite large-scale pretraining endowing models with language and vision reasoning capabilities, improving their spatial reasoning capability remains challenging due to the lack of data grounded in the 3D world. While it is possible for humans to manually create immersive and interactive worlds through 3D graphics, as seen in applications such as VR, gaming, and robotics, this process remains highly labor-intensive. In this paper, we propose a scalable method for generating high-quality 3D environments that can serve as training data for foundation models. We recast 3D environment building as a sequential decision-making problem, employing Vision-Language-Models (VLMs) as policies that output actions to jointly craft a 3D environment's layout, materials, lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to generate more prompt-aligned 3D environments via self-improvement fine-tuning. We demonstrate the effectiveness of 3D-Generalist and the proposed training strategy in generating simulation-ready 3D environments. Furthermore, we demonstrate its quality and scalability in synthetic data generation by pretraining a vision foundation model on the generated data. After fine-tuning the pre-trained model on downstream tasks, we show that it surpasses models pre-trained on meticulously human-crafted synthetic data and approaches results achieved with real data orders of magnitude larger.",
    "comments": "project website: this https URL",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2507.10016",
    "url": "https://arxiv.org/abs/2507.10016",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.474481",
    "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents",
    "authors": "Lixu Wang, Kaixiang Yao, Xinfeng Li, Dong Yang, Haoyang Li, Xiaofeng Wang, Wei Dong",
    "subjects": "Cryptography and Security (cs.CR); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
    "abstract": "Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.",
    "comments": "22 pages, 4 figures",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2507.12911",
    "url": "https://arxiv.org/abs/2507.12911",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.475619",
    "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR",
    "authors": "Hayeon Oh",
    "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
    "abstract": "Out-of-distribution (OOD) scenarios in autonomous driving pose critical challenges, as planners often fail to generalize beyond their training experience, leading to unsafe or unexpected behavior. Vision-Language Models (VLMs) have shown promise in handling such scenarios by providing high-level scene understanding and user-aligned decisions. However, existing VLMs often exhibit a misalignment between their language-based reasoning and the low-level trajectories required for action-level planning. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to fine-tune VLMs using planning-oriented metrics. Experimental results show that LaViPlan improves planning performance across both in-domain and out-of-domain datasets. While linguistic fidelity slightly decreases after RLVR-based fine-tuning, qualitative evaluation indicates that the outputs remain coherent. We also conduct ablation studies to analyze the effects of sampling ratio and reasoning guidance, highlighting how these design choices influence performance. These findings demonstrate the potential of RLVR as a post-training paradigm for aligning language-guided reasoning with action-level planning in autonomous driving.",
    "comments": "Accepted to the 2nd ICCV 2025 Workshop on the Challenge of Out-of-Label Hazards in Autonomous Driving (13 pages, 6 figures)",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2507.17442",
    "url": "https://arxiv.org/abs/2507.17442",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.476505",
    "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
    "authors": "Shiting Chen, Zijian Zhao, Jinsong Chen",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.",
    "matched_keyword": "rag",
    "matched_category": "检索增强生成"
  },
  {
    "id": "2507.23479",
    "url": "https://arxiv.org/abs/2507.23479",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.476505",
    "title": "Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning",
    "authors": "Julia Werner, Oliver Bause, Julius Oexle, Maxime Le Floch, Franz Brinkmann, Jochen Hampe, Oliver Bringmann",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video capsule endoscopy has become increasingly important for investigating the small intestine within the gastrointestinal tract. However, a persistent challenge remains the short battery lifetime of such compact sensor edge devices. Integrating artificial intelligence can help overcome this limitation by enabling intelligent real-time decision-making, thereby reducing the energy consumption and prolonging the battery life. However, this remains challenging due to data sparsity and the limited resources of the device restricting the overall model size. In this work, we introduce a multi-task neural network that combines the functionalities of precise self-localization within the gastrointestinal tract with the ability to detect anomalies in the small intestine within a single model. Throughout the development process, we consistently restricted the total number of parameters to ensure the feasibility to deploy such model in a small capsule. We report the first multi-task results using the recently published Galar dataset, integrating established multi-task methods and Viterbi decoding for subsequent time-series analysis. This outperforms current single-task models and represents a significant advance in AI-based approaches in this field. Our model achieves an accuracy of 93.63% on the localization task and an accuracy of 87.48% on the anomaly detection task. The approach requires only 1 million parameters while surpassing the current baselines.",
    "comments": "Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025 (submitted version)",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.00288",
    "url": "https://arxiv.org/abs/2508.00288",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.478000",
    "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents",
    "authors": "Jianqiang Xiao, Yuexuan Sun, Yixin Shao, Boxi Gan, Rongqiang Liu, Yanjing Wu, Weili Gua, Xiang Deng",
    "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments.",
    "comments": "Accepted to ACM MM Dataset Track 2025",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.02028",
    "url": "https://arxiv.org/abs/2508.02028",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.478987",
    "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving",
    "authors": "Tianyuan Zhang, Ting Jin, Lu Wang, Jiangfan Liu, Siyuan Liang, Mingchuan Zhang, Aishan Liu, Xianglong Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions.",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2508.02085",
    "url": "https://arxiv.org/abs/2508.02085",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.478987",
    "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
    "authors": "Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents' interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at this https URL.",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.02091",
    "url": "https://arxiv.org/abs/2508.02091",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.479509",
    "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
    "authors": "Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)",
    "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at this https URL",
    "comments": "Preprint Version",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.02866",
    "url": "https://arxiv.org/abs/2508.02866",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.479509",
    "title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows",
    "authors": "Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva",
    "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Databases (cs.DB)",
    "abstract": "Large Language Models (LLMs) and other foundation models are increasingly used as the core of AI agents. In agentic workflows, these agents plan tasks, interact with humans and peers, and influence scientific outcomes across federated and heterogeneous environments. However, agents can hallucinate or reason incorrectly, propagating errors when one agent's output becomes another's input. Thus, assuring that agents' actions are transparent, traceable, reproducible, and reliable is critical to assess hallucination risks and mitigate their workflow impacts. While provenance techniques have long supported these principles, existing methods fail to capture and relate agent-centric metadata such as prompts, responses, and decisions with the broader workflow context and downstream outcomes. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) and data observability to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis.",
    "comments": "Paper accepted for publication in the Proceedings of the 2025 IEEE 21st International Conference on e-Science. Cite it as: R. Souza, A. Gueroudji, S. DeWitt, D. Rosendo, T. Ghosal, R. Ross, P. Balaprakash, R. F. da Silva, \"PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows,\" IEEE International Conference on e-Science, Chicago, IL, USA, 2025",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.03082",
    "url": "https://arxiv.org/abs/2508.03082",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.479509",
    "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design",
    "authors": "Fei Liu, Yilu Liu, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan",
    "subjects": "Artificial Intelligence (cs.AI)",
    "abstract": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a small-sized complementary heuristic set to serve diverse problem instances, such that each problem instance could be optimized by at least one heuristic in this set. We show that the objective function of AHSD is monotone and supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary population management and complementary-aware memetic search, EoH-S could effectively generate a set of high-quality and complementary heuristics. Comprehensive experimental results on three AHD tasks with diverse instances spanning various sizes and distributions demonstrate that EoH-S consistently outperforms existing state-of-the-art AHD methods and achieves up to 60\\% performance improvements.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.04941",
    "url": "https://arxiv.org/abs/2508.04941",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.480533",
    "title": "Toward Errorless Training ImageNet-1k",
    "authors": "Bo Deng, Levi Heath",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
    "abstract": "In this paper, we describe a feedforward artificial neural network trained on the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are perfectly classified over the 10 batch partitions of the dataset. The best performing model uses 322,430,160 parameters, with 4 decimal places precision. We conjecture that the reason our model does not achieve a 100% accuracy rate is due to a double-labeling problem, by which there are duplicate images in the dataset with different labels.",
    "comments": "14 pages, 2 figures, 5 tables",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.05253",
    "url": "https://arxiv.org/abs/2508.05253",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.481556",
    "title": "Congestion Mitigation Path Planning for Large-Scale Multi-Agent Navigation in Dense Environments",
    "authors": "Takuro Kato, Keisuke Okumura, Yoko Sasaki, Naoya Yokomachi",
    "subjects": "Multiagent Systems (cs.MA)",
    "abstract": "In high-density environments where numerous autonomous agents move simultaneously in a distributed manner, streamlining global flows to mitigate local congestion is crucial to maintain overall navigation efficiency. This paper introduces a novel path-planning problem, congestion mitigation path planning (CMPP), which embeds congestion directly into the cost function, defined by the usage of incoming edges along agents' paths. CMPP assigns a flow-based multiplicative penalty to each vertex of a sparse graph, which grows steeply where frequently-traversed paths intersect, capturing the intuition that congestion intensifies where many agents enter the same area from different directions. Minimizing the total cost yields a set of coarse-level, time-independent routes that autonomous agents can follow while applying their own local collision avoidance. We formulate the problem and develop two solvers: (i) an exact mixed-integer nonlinear programming solver for small instances, and (ii) a scalable two-layer search algorithm, A-CMTS, which quickly finds suboptimal solutions for large-scale instances and iteratively refines them toward the optimum. Empirical studies show that augmenting state-of-the-art collision-avoidance planners with CMPP significantly reduces local congestion and enhances system throughput in both discrete- and continuous-space scenarios. These results indicate that CMPP improves the performance of multi-agent systems in real-world applications such as logistics and autonomous-vehicle operations.",
    "comments": "Published in IEEE Robotics and Automation Letters (RA-L), 2025. Supplementary videos are accessible via IEEE Xplore",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.08066",
    "url": "https://arxiv.org/abs/2508.08066",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.483601",
    "title": "ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
    "authors": "Weitai Kang, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
    "abstract": "Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
    "comments": "8 pages for the main paper",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.08487",
    "url": "https://arxiv.org/abs/2508.08487",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.483601",
    "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling",
    "authors": "Qian Wang, Ziqi Huang, Ruoxi Jia, Paul Debevec, Ning Yu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
    "abstract": "Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.",
    "comments": "Video Generation Agent",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.09128",
    "url": "https://arxiv.org/abs/2508.09128",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.483601",
    "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions",
    "authors": "Dhruv Singh Kushwaha, Zoleikha Abdollahi Biron",
    "subjects": "Systems and Control (eess.SY)",
    "abstract": "Reinforcement learning (RL) has proven to be particularly effective in solving complex decision-making problems for a wide range of applications. From a control theory perspective, RL can be considered as an adaptive optimal control scheme. Lyapunov and barrier functions are the most commonly used certificates to guarantee system stability for a proposed/derived controller and constraint satisfaction guarantees, respectively, in control theoretic approaches. However, compared to theoretical guarantees available in control theoretic methods, RL lacks closed-loop stability of a computed policy and constraint satisfaction guarantees. Safe reinforcement learning refers to a class of constrained problems where the constraint violations lead to partial or complete system failure. The goal of this review is to provide an overview of safe RL techniques using Lyapunov and barrier functions to guarantee this notion of safety discussed (stability of the system in terms of a computed policy and constraint satisfaction during training and deployment). The different approaches employed are discussed in detail along with their shortcomings and benefits to provide critique and possible future research directions. Key motivation for this review is to discuss current theoretical approaches for safety and stability guarantees in RL similar to control theoretic approaches using Lyapunov and barrier functions. The review provides proven potential and promising scope of providing safety guarantees for complex dynamical systems with operational constraints using model-based and model-free RL.",
    "comments": "pages - 19, figures - 9, Submitted to IEEE TAI",
    "matched_keyword": "reinforcement learning",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.10142",
    "url": "https://arxiv.org/abs/2508.10142",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.484620",
    "title": "Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs",
    "authors": "Kartikeya Badola, Jonathan Simon, Arian Hosseini, Sara Marie Mc Carthy, Tsendsuren Munkhdalai, Abhimanyu Goyal, Tomáš Kočiský, Shyam Upadhyay, Bahare Fatemi, Mehran Kazemi",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.11870",
    "url": "https://arxiv.org/abs/2508.11870",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.485641",
    "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition",
    "authors": "Ying Huang, Yuanbin Man, Wenqi Jia, Zhengzhong Tu, Junzhou Huang, Miao Yin",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Adapter-based fine-tuning has gained remarkable attention in adapting large pre-trained vision language models (VLMs) for a wide range of downstream tasks efficiently. In this paradigm, only the inserted adapters are fine-tuned, without the need for training the original VLM backbone. Existing works scale adapters by integrating them into every layer of VLMs to increase the capacity of adapters. However, these methods face two primary limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters. In this paper, we propose a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing, achieving ultra-light parameter-efficient adaptation of VLMs on various tasks. To remove the high redundancy that exists among adapters across layers, we exploit the tensor-level low-rankness to formulate adapters as layer-shared tensor cores and layer-specific slices. Moreover, guided by generalization-aware fine-tuning, diverse rank-driven adapters cooperate to handle tasks that require different representations. Our experiments show that the proposed AdaRing achieves the state-of-the-art performance while reducing average training parameters by 90%.",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2508.12096",
    "url": "https://arxiv.org/abs/2508.12096",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.485641",
    "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples",
    "authors": "Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
    "abstract": "Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \\textbf{S}tructured \\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \\textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.",
    "comments": "Submit to AAAI 2026",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.12645",
    "url": "https://arxiv.org/abs/2508.12645",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.485641",
    "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation",
    "authors": "Hongyang Liu, Zhu Sun, Tianjun Wei, Yan Wang, Jiajie Zhu, Xinghua Qu",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Recent advances in large language models (LLMs) have enabled realistic user simulators for developing and evaluating recommender systems (RSs). However, existing LLM-based simulators for RSs face two major limitations: (1) static and single-step prompt-based inference that leads to inaccurate and incomplete user profile construction; (2) unrealistic and single-round recommendation-feedback interaction pattern that fails to capture real-world scenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided Dynamic Profile Optimization), a novel framework that constructs user profile through a dynamic and iterative optimization process to enhance the simulation fidelity. Specifically, DGDPO incorporates two core modules within each optimization loop: firstly, a specialized LLM-based diagnostic module, calibrated through our novel training strategy, accurately identifies specific defects in the user profile. Subsequently, a generalized LLM-based treatment module analyzes the diagnosed defect and generates targeted suggestions to refine the profile. Furthermore, unlike existing LLM-based user simulators that are limited to single-round interactions, we are the first to integrate DGDPO with sequential recommenders, enabling a bidirectional evolution where user profiles and recommendation strategies adapt to each other over multi-round interactions. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of our proposed framework.",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  },
  {
    "id": "2508.12769",
    "url": "https://arxiv.org/abs/2508.12769",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.485641",
    "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
    "authors": "Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Peng",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at this https URL",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2508.13404",
    "url": "https://arxiv.org/abs/2508.13404",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.489653",
    "title": "TASER: Table Agents for Schema-guided Extraction and Recommendation",
    "authors": "Nicole Cho, Kirsty Fielding, William Watson, Sumitra Ganesh, Manuela Veloso",
    "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
    "abstract": "Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.",
    "comments": "Withdrawn due to missing key sections in the paper",
    "matched_keyword": "agent",
    "matched_category": "智能体"
  },
  {
    "id": "2508.13968",
    "url": "https://arxiv.org/abs/2508.13968",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.489653",
    "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
    "authors": "Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
    "abstract": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.",
    "comments": "20 pages. Code and data: this https URL",
    "matched_keyword": "large language model",
    "matched_category": "大模型"
  },
  {
    "id": "2508.14029",
    "url": "https://arxiv.org/abs/2508.14029",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.489653",
    "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR",
    "authors": "Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, Weizhu Chen",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2407.11353",
    "url": "https://arxiv.org/abs/2407.11353",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.493658",
    "title": "Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early-Stopping",
    "authors": "Yingzhen Yang, Ping Li",
    "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
    "abstract": "We study nonparametric regression using an over-parameterized two-layer neural networks trained with algorithmic guarantees in this paper. We consider the setting where the training features are drawn uniformly from the unit sphere in $\\mathbb{R}^d$, and the target function lies in an interpolation space commonly studied in statistical learning theory. We demonstrate that training the neural network with a novel Preconditioned Gradient Descent (PGD) algorithm, equipped with early stopping, achieves a sharp regression rate of $\\mathcal O(n^{-\\frac{2\\alpha s'}{2\\alpha s'+1}})$ when the target function is in the interpolation space $[\\mathcal H_K]^{s'}$ with $s' \\ge 3$. This rate is even sharper than the currently known nearly-optimal rate of $\\mathcal O(n^{-\\frac{2\\alpha s'}{2\\alpha s'+1}})\\log^2(1/\\delta)$~\\citep{Li2024-edr-general-domain}, where $n$ is the size of the training data and $\\delta \\in (0,1)$ is a small probability. This rate is also sharper than the standard kernel regression rate of $\\mathcal O(n^{-\\frac{2\\alpha}{2\\alpha+1}})$ obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent (GD), where $2\\alpha = d/(d-1)$. Our analysis is based on two key technical contributions. First, we present a principled decomposition of the network output at each PGD step into a function in the reproducing kernel Hilbert space (RKHS) of a newly induced integral kernel, and a residual function with small $L^{\\infty}$-norm. Second, leveraging this decomposition, we apply local Rademacher complexity theory to tightly control the complexity of the function class comprising all the neural network functions obtained in the PGD iterates. Our results further suggest that PGD enables the neural network to escape the linear NTK regime and achieve improved generalization by inducing a new integral kernel of lower kernel complexity.",
    "matched_keyword": "rl",
    "matched_category": "强化学习"
  },
  {
    "id": "2506.07614",
    "url": "https://arxiv.org/abs/2506.07614",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.493658",
    "title": "Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds",
    "authors": "Rishikesh Srinivasan, Dheeraj Nagaraj",
    "subjects": "Probability (math.PR); Machine Learning (cs.LG); Statistics Theory (math.ST)",
    "abstract": "We study the problem of sampling from strongly log-concave distributions over $\\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.",
    "matched_keyword": "dpo",
    "matched_category": "强化学习"
  },
  {
    "id": "2507.02755",
    "url": "https://arxiv.org/abs/2507.02755",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.493658",
    "title": "Multi-agent Auditory Scene Analysis",
    "authors": "Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón",
    "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI)",
    "abstract": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a publicly available framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.",
    "comments": "Submitted to Applied Soft Computing",
    "matched_keyword": "multi-agent",
    "matched_category": "智能体"
  },
  {
    "id": "2507.07060",
    "url": "https://arxiv.org/abs/2507.07060",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.497662",
    "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning",
    "authors": "Shreyas Vinaya Sathyanarayana, Sharanabasava D. Hiremath, Rahil Shah, Rishikesh Panda, Rahul Jana, Riya Singh, Rida Irfan, Ashwin Murali, Bharath Ramsundar",
    "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Biomolecules (q-bio.BM); Molecular Networks (q-bio.MN)",
    "abstract": "The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.",
    "comments": "64 pages,",
    "matched_keyword": "llm",
    "matched_category": "大模型"
  }
]