[
  {
    "id": "2508.14094",
    "url": "https://arxiv.org/abs/2508.14094",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.332906",
    "title": "Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets",
    "authors": "Benjamin Pikus, Pratyush Ranjan Tiwari, Burton Ye",
    "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
    "abstract": "Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate a critical question for resource-constrained alignment: under a fixed acquisition budget, should practitioners prioritize examples that are easy, medium, hard, or of random difficulty? We study Group Relative Policy Optimization (GRPO) fine-tuning across different model sizes and families, comparing four subset selection policies chosen from the same unlabeled pool using base-model difficulty estimates obtained via multi-sample evaluation. Our experiments reveal that training on the hardest examples yields the largest performance gains, up to 47%, while training on easy examples yield the smallest gains. Analysis reveals that this effect arises from harder examples providing more learnable opportunities during GRPO training. These findings provide practical guidance for budget-constrained post-training: prioritizing hard examples yields substantial performance gains on reasoning tasks when using GRPO.",
    "matched_keyword": "post-training",
    "matched_category": "后训练"
  },
  {
    "id": "2508.14896",
    "url": "https://arxiv.org/abs/2508.14896",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T02:00:09.423681",
    "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
    "authors": "Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.",
    "comments": "Technical Report, Work in Progress",
    "matched_keyword": "post-training",
    "matched_category": "后训练"
  }
]