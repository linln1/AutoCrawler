[
  {
    "id": "2508.14058",
    "url": "https://arxiv.org/abs/2508.14058",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.137540",
    "title": "Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks",
    "authors": "Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang",
    "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
    "abstract": "The explosive growth of the video game industry has created an urgent need for recommendation systems that can scale with expanding catalogs and maintain user engagement. While prior work has explored accuracy and diversity in recommendations, existing models underutilize playtime, a rich behavioral signal unique to gaming platforms, and overlook the potential of multimodal information to enhance diversity. In this paper, we propose DP2Rec, a novel Dual-Phase Playtime-guided Recommendation model designed to jointly optimize accuracy and diversity. First, we introduce a playtime-guided interest intensity exploration module that separates strong and weak preferences via dual-beta modeling, enabling fine-grained user profiling and more accurate recommendations. Second, we present a playtime-guided multimodal random walks module that simulates player exploration using transitions guided by both playtime-derived interest similarity and multimodal semantic similarity. This mechanism preserves core preferences while promoting cross-category discovery through latent semantic associations and adaptive category balancing. Extensive experiments on a real-world game dataset show that DP2Rec outperforms existing methods in both recommendation accuracy and diversity.",
    "comments": "Accepted for publication at ACM Multimedia (ACM MM) 2025. 10 pages, 5 figures. Code and dataset: this https URL",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14146",
    "url": "https://arxiv.org/abs/2508.14146",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.153711",
    "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation",
    "authors": "Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu",
    "subjects": "Computation and Language (cs.CL)",
    "abstract": "With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.",
    "comments": "Work in progress",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14160",
    "url": "https://arxiv.org/abs/2508.14160",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.154706",
    "title": "RynnEC: Bringing MLLMs into Embodied World",
    "authors": "Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
    "abstract": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: this https URL",
    "comments": "The technical report of RynnEC, an embodied cognition MLLM",
    "matched_keyword": "mllm",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14203",
    "url": "https://arxiv.org/abs/2508.14203",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.154706",
    "title": "A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment",
    "authors": "Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Video Anomaly Detection (VAD) has emerged as a pivotal task in computer vision, with broad relevance across multiple fields. Recent advances in deep learning have driven significant progress in this area, yet the field remains fragmented across domains and learning paradigms. This survey offers a comprehensive perspective on VAD, systematically organizing the literature across various supervision levels, as well as adaptive learning methods such as online, active, and continual learning. We examine the state of VAD across three major application categories: human-centric, vehicle-centric, and environment-centric scenarios, each with distinct challenges and design considerations. In doing so, we identify fundamental contributions and limitations of current methodologies. By consolidating insights from subfields, we aim to provide the community with a structured foundation for advancing both theoretical understanding and real-world applicability of VAD systems. This survey aims to support researchers by providing a useful reference, while also drawing attention to the broader set of open challenges in anomaly detection, including both fundamental research questions and practical obstacles to real-world deployment.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14222",
    "url": "https://arxiv.org/abs/2508.14222",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.158729",
    "title": "StarStream: Live Video Analytics over Space Networking",
    "authors": "Miao Zhang, Jiaxing Li, Haoyuan Zhao, Linfeng Shen, Jiangchuan Liu",
    "subjects": "Networking and Internet Architecture (cs.NI); Multimedia (cs.MM)",
    "abstract": "Streaming videos from resource-constrained front-end devices over networks to resource-rich cloud servers has long been a common practice for surveillance and analytics. Most existing live video analytics (LVA) systems, however, have been built over terrestrial networks, limiting their applications during natural disasters and in remote areas that desperately call for real-time visual data delivery and scene analysis. With the recent advent of space networking, in particular, Low Earth Orbit (LEO) satellite constellations such as Starlink, high-speed truly global Internet access is becoming available and affordable. This paper examines the challenges and potentials of LVA over modern LEO satellite networking (LSN). Using Starlink as the testbed, we have carried out extensive in-the-wild measurements to gain insights into its achievable performance for LVA. The results reveal that the uplink bottleneck in today's LSN, together with the volatile network conditions, can significantly affect the service quality of LVA and necessitate prompt adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming framework for LVA. At its core, StarStream is empowered by a Transformer-based network performance predictor tailored for LSN and a content-aware configuration optimizer. We discuss a series of key design and implementation issues of StarStream and demonstrate its effectiveness and superiority through trace-driven experiments with real-world network and video processing data.",
    "comments": "Accepted by MM'24",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14237",
    "url": "https://arxiv.org/abs/2508.14237",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.159741",
    "title": "OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos",
    "authors": "Miao Zhang, Yifei Zhu, Linfeng Shen, Fangxin Wang, Jiangchuan Liu",
    "subjects": "Networking and Internet Architecture (cs.NI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
    "abstract": "With the reduced hardware costs of omnidirectional cameras and the proliferation of various extended reality applications, more and more $360^\\circ$ videos are being captured. To fully unleash their potential, advanced video analytics is expected to extract actionable insights and situational knowledge without blind spots from the videos. In this paper, we present OmniSense, a novel edge-assisted framework for online immersive video analytics. OmniSense achieves both low latency and high accuracy, combating the significant computation and network resource challenges of analyzing $360^\\circ$ videos. Motivated by our measurement insights into $360^\\circ$ videos, OmniSense introduces a lightweight spherical region of interest (SRoI) prediction algorithm to prune redundant information in $360^\\circ$ frames. Incorporating the video content and network dynamics, it then smartly scales vision models to analyze the predicted SRoIs with optimized resource utilization. We implement a prototype of OmniSense with commodity devices and evaluate it on diverse real-world collected $360^\\circ$ videos. Extensive evaluation results show that compared to resource-agnostic baselines, it improves the accuracy by $19.8\\%$ -- $114.6\\%$ with similar end-to-end latencies. Meanwhile, it hits $2.0\\times$ -- $2.4\\times$ speedups while keeping the accuracy on par with the highest accuracy of baselines.",
    "comments": "10 pages; Accepted by INFOCOM'23",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14327",
    "url": "https://arxiv.org/abs/2508.14327",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.170763",
    "title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation",
    "authors": "Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video generation has recently shown superiority in urban scene synthesis for autonomous driving. Existing video generation approaches to autonomous driving primarily focus on RGB video generation and lack the ability to support multi-modal video generation. However, multi-modal data, such as depth maps and semantic maps, are crucial for holistic urban scene understanding in autonomous driving. Although it is feasible to use multiple models to generate different modalities, this increases the difficulty of model deployment and does not leverage complementary cues for multi-modal data generation. To address this problem, in this work, we propose a novel multi-modal multi-view video generation approach to autonomous driving. Specifically, we construct a unified diffusion transformer model composed of modal-shared components and modal-specific components. Then, we leverage diverse conditioning inputs to encode controllable scene structure and content cues into the unified diffusion model for multi-modal multi-view video generation. In this way, our approach is capable of generating multi-modal multi-view driving scene videos in a unified framework. Our experiments on the challenging real-world autonomous driving dataset, nuScenes, show that our approach can generate multi-modal multi-view urban scene videos with high fidelity and controllability, surpassing the state-of-the-art methods.",
    "comments": "Technical Report",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14395",
    "url": "https://arxiv.org/abs/2508.14395",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.178762",
    "title": "NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding",
    "authors": "Running Zhao, Zhihan Jiang, Xinchen Zhang, Chirui Chang, Handi Chen, Weipeng Deng, Luyao Jin, Xiaojuan Qi, Xun Qian, Edith C.H. Ngai",
    "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
    "abstract": "Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users' expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt's interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt. Project website: this https URL",
    "comments": "Accepted to UIST 2025. Project website: this https URL",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14423",
    "url": "https://arxiv.org/abs/2508.14423",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.182837",
    "title": "MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing",
    "authors": "Jeahun Sung, Changhyun Roh, Chanho Eom, Jihyong Oh",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Recent advances in portable imaging have made camera-based screen capture ubiquitous. Unfortunately, frequency aliasing between the camera's color filter array (CFA) and the display's sub-pixels induces moiré patterns that severely degrade captured photos and videos. Although various demoiréing models have been proposed to remove such moiré patterns, these approaches still suffer from several limitations: (i) spatially varying artifact strength within a frame, (ii) large-scale and globally spreading structures, (iii) channel-dependent statistics and (iv) rapid temporal fluctuations across frames. We address these issues with the Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former), which comprises two key components: Decoupled Moiré Adaptive Demoiréing (DMAD) and Spatio-Temporal Adaptive Demoiréing (STAD). DMAD separates moiré and content via a Moiré Decoupling Block (MDB) and a Detail Decoupling Block (DDB), then produces moiré-adaptive features using a Moiré Conditioning Block (MCB) for targeted restoration. STAD introduces a Spatial Fusion Block (SFB) with window attention to capture large-scale structures, and a Feature Channel Attention (FCA) to model channel dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs implicit frame alignment without any explicit alignment module. We analyze moiré characteristics through qualitative and quantitative studies, and evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.",
    "comments": "Please visit our project page at [this http URL link](this https URL)",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14465",
    "url": "https://arxiv.org/abs/2508.14465",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.190825",
    "title": "DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing",
    "authors": "Weitao Wang, Zichen Wang, Hongdeng Shen, Yulei Lu, Xirui Fan, Suhui Wu, Jun Zhang, Haoqian Wang, Hao Zhang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "With the rapid progress of video generation, demand for customized video editing is surging, where subject swapping constitutes a key component yet remains under-explored. Prevailing swapping approaches either specialize in narrow domains--such as human-body animation or hand-object interaction--or rely on some indirect editing paradigm or ambiguous text prompts that compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided, subject-agnostic, end-to-end framework that swaps any subject in any video for customization with a user-specified mask and reference image. To inject fine-grained guidance, we introduce multiple conditions and a dedicated condition fusion module that integrates them efficiently. In addition, an adaptive mask strategy is designed to accommodate subjects of varying scales and attributes, further improving interactions between the swapped subject and its surrounding context. Through our elaborate two-phase dataset construction and training scheme, our DreamSwapV outperforms existing methods, as validated by comprehensive experiments on VBench indicators and our first introduced DreamSwapV-Benchmark.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14483",
    "url": "https://arxiv.org/abs/2508.14483",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.190825",
    "title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration",
    "authors": "Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at this https URL.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14485",
    "url": "https://arxiv.org/abs/2508.14485",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.190825",
    "title": "Distribution-Guided Auto-Encoder for User Multimodal Interest Cross Fusion",
    "authors": "Moyu Zhang, Yongxiang Tang, Yujun Jin, Jinxin Hu, Yu Zhang",
    "subjects": "Information Retrieval (cs.IR)",
    "abstract": "Traditional recommendation methods rely on correlating the embedding vectors of item IDs to capture implicit collaborative filtering signals to model the user's interest in the target item. Consequently, traditional ID-based methods often encounter data sparsity problems stemming from the sparse nature of ID features. To alleviate the problem of item ID sparsity, recommendation models incorporate multimodal item information to enhance recommendation accuracy. However, existing multimodal recommendation methods typically employ early fusion approaches, which focus primarily on combining text and image features, while neglecting the contextual influence of user behavior sequences. This oversight prevents dynamic adaptation of multimodal interest representations based on behavioral patterns, consequently restricting the model's capacity to effectively capture user multimodal interests. Therefore, this paper proposes the Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves the cross fusion of user multimodal interest at the behavioral this http URL, extensive experiments demonstrate the superiority of DMAE.",
    "comments": "Accepted by CIKM 2025, 11 pages, 4 figures, 4 tables",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14504",
    "url": "https://arxiv.org/abs/2508.14504",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.194828",
    "title": "PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments",
    "authors": "Bernd Hofmann, Albert Scheck, Joerg Franke, Patrick Bruendl",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14523",
    "url": "https://arxiv.org/abs/2508.14523",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.194828",
    "title": "Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism",
    "authors": "Kevin Riehl, Shaimaa K. El-Baklish, Anastasios Kouvelas, Michail A. Makridis",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Accurate prediction of road user movement is increasingly required by many applications ranging from advanced driver assistance systems to autonomous driving, and especially crucial for road safety. Even though most traffic accident fatalities account to bicycles, they have received little attention, as previous work focused mainly on pedestrians and motorized vehicles. In this work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal trajectory prediction framework for bicycles. The model incorporates both physics-based modeling (inspired by motorized vehicles) and social-based modeling (inspired by pedestrian movements) to explicitly account for the dual nature of bicycle movement. The social interactions are modeled with a graph attention network, and include decayed historical, but also anticipated, future trajectory data of a bicycles neighborhood, following recent insights from psychological and social studies. The results indicate that the proposed ensemble of physics models -- performing well in the short-term predictions -- and social models -- performing well in the long-term predictions -- exceeds state-of-the-art performance. We also conducted a controlled mass-cycling experiment to demonstrate the framework's performance when forecasting bicycle trajectories and modeling social interactions with road users.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14581",
    "url": "https://arxiv.org/abs/2508.14581",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.202757",
    "title": "FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics",
    "authors": "Chen Chen, Runze Li, Zejun Zhang, Pukun Zhao, Fanqing Zhou, Longxiang Wang, Haojian Huang",
    "subjects": "Multimedia (cs.MM); Image and Video Processing (eess.IV)",
    "abstract": "FakeHunter is a multimodal deepfake detection framework that combines memory-guided retrieval, chain-of-thought (Observation-Thought-Action) reasoning, and tool-augmented verification to provide accurate and interpretable video forensics. FakeHunter encodes visual content using CLIP and audio using CLAP, generating joint audio-visual embeddings that retrieve semantically similar real exemplars from a FAISS-indexed memory bank for contextual grounding. Guided by the retrieved context, the system iteratively reasons over evidence to localize manipulations and explain them. When confidence is low, it automatically invokes specialized tools-such as zoom-in image forensics or mel-spectrogram inspection-for fine-grained verification. Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that specify what was modified, where it occurs, and why it is judged fake. We also introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos (950+ min) annotated with manipulation type, region/entity, violated reasoning category, and free-form justification. On X-AVFake, FakeHunter achieves an accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87 percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies reveal that memory retrieval contributes a 7.75 percentage point gain, and tool-based inspection improves low-confidence cases to 46.50%. Despite its multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x), demonstrating practical deployability.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14604",
    "url": "https://arxiv.org/abs/2508.14604",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.202757",
    "title": "UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling",
    "authors": "Peiming Li, Ziyi Wang, Yulin Yuan, Hong Liu, Xiangming Meng, Junsong Yuan, Mengyuan Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at this https URL.",
    "comments": "8 pages, 5 figures, Accepted to ICCV2025",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14607",
    "url": "https://arxiv.org/abs/2508.14607",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.206757",
    "title": "SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos",
    "authors": "Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14609",
    "url": "https://arxiv.org/abs/2508.14609",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.206757",
    "title": "AnchorSync: Global Consistency Optimization for Long Video Editing",
    "authors": "Zichi Liu, Yinggui Wang, Tao Wei, Chao Ma",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Editing long videos remains a challenging task due to the need for maintaining both global consistency and temporal coherence across thousands of frames. Existing methods often suffer from structural drift or temporal artifacts, particularly in minute-long sequences. We introduce AnchorSync, a novel diffusion-based framework that enables high-quality, long-term video editing by decoupling the task into sparse anchor frame editing and smooth intermediate frame interpolation. Our approach enforces structural consistency through a progressive denoising process and preserves temporal dynamics via multimodal guidance. Extensive experiments show that AnchorSync produces coherent, high-fidelity edits, surpassing prior methods in visual quality and temporal stability.",
    "comments": "ACM MM 2025; Code is released at this https URL",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14631",
    "url": "https://arxiv.org/abs/2508.14631",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.206757",
    "title": "Towards a DSL to Formalize Multimodal Requirements",
    "authors": "Marcos Gomez-Vazquez, Jordi Cabot",
    "subjects": "Software Engineering (cs.SE)",
    "abstract": "Multimodal systems, which process multiple input types such as text, audio, and images, are becoming increasingly prevalent in software systems, enabled by the huge advancements in Machine Learning. This triggers the need to easily define the requirements linked to these new types of user interactions, potentially involving more than one modality at the same time. This remains an open challenge due to the lack of languages and methods adapted to the diverse nature of multimodal interactions, with the risk of implementing AI-enhanced systems that do not properly satisfy the user needs.\nIn this sense, this paper presents MERLAN, a Domain-Specific Language (DSL) to specify the requirements for these new types of multimodal interfaces. We present the metamodel for such language together with a textual syntax implemented as an ANTLR grammar. A prototype tool enabling requirements engineers to write such requirements and automatically generate a possible implementation of a system compliant with them on top of an agentic framework is also provided.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14706",
    "url": "https://arxiv.org/abs/2508.14706",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.214757",
    "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
    "authors": "Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)",
    "abstract": "Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14729",
    "url": "https://arxiv.org/abs/2508.14729",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.214757",
    "title": "Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving",
    "authors": "Leila Cheshmi, Mennatullah Siam",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video transformers capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video transformer trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory decoding and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional decoders that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics.",
    "comments": "6 pages, 2 figures, 1 table",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14812",
    "url": "https://arxiv.org/abs/2508.14812",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.225104",
    "title": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives",
    "authors": "Haoyu Zhao, Jiaxi Gu, Shicong Wang, Xing Zhang, Hang Xu, Zuxuan Wu, Yu-Gang Jiang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "The explosive growth of video streaming presents challenges in achieving high accuracy and low training costs for video-language retrieval. However, existing methods rely on large-scale pre-training to improve video retrieval performance, resulting in significant computational demands. Additionally, the fine-grained information in videos and texts remains underexplored. To alleviate these problems, we propose a novel framework to learn fine-grained features for better alignment and introduce an inference pipeline to improve performance without additional training. Specifically, we employ coarse-to-fine objectives to understand the semantic information of video-text pairs, including contrastive and matching learning. The fine-grained data used for training is obtained through the Granularity-Aware Representation module, which is designed based on similarity analysis between video frames and words in captions. Furthermore, we observe that the repetition of keywords in the original captions, referred to as \"Repetition\", can enhance retrieval performance and improve alignment between video and text. Based on this insight, we propose a novel and effective inference pipeline that incorporates a voting mechanism and a new Matching Entropy metric to achieve better retrieval performance without requiring additional pre-training. Experimental results on four benchmarks demonstrate that the proposed method outperforms previous approaches. Additionally, our inference pipeline achieves significant performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT dataset and a 1.6% increase on the DiDeMo dataset.",
    "comments": "11 pages, 4 figures",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.14844",
    "url": "https://arxiv.org/abs/2508.14844",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.229547",
    "title": "Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations",
    "authors": "Murat Isik, Mandeep Kaur Saggi, Humaira Gowher, Sabre Kais",
    "subjects": "Machine Learning (cs.LG)",
    "abstract": "Accurately predicting enzyme functionality remains one of the major challenges in computational biology, particularly for enzymes with limited structural annotations or sequence homology. We present a novel multimodal Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC) classification by integrating four complementary biochemical modalities: protein sequence embeddings, quantum-derived electronic descriptors, molecular graph structures, and 2D molecular image representations. Quantum Vision Transformer (QVT) backbone equipped with modality-specific encoders and a unified cross-attention fusion module. By integrating graph features and spatial patterns, our method captures key stereoelectronic interactions behind enzyme function. Experimental results demonstrate that our multimodal QVT model achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a substantial margin and achieving better performance results compared to other QML models.",
    "comments": "Accepted at IEEE International Conference on Quantum Artificial Intelligence (QAI) 2025",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2407.05311",
    "url": "https://arxiv.org/abs/2407.05311",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.254291",
    "title": "MMAD: Multi-label Micro-Action Detection in Videos",
    "authors": "Kun Li, Pengyu Liu, Dan Guo, Fei Wang, Zhiliang Wu, Hehe Fan, Meng Wang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Human body actions are an important form of non-verbal communication in social interactions. This paper specifically focuses on a subset of body actions known as micro-actions, which are subtle, low-intensity body movements with promising applications in human emotion analysis. In real-world scenarios, human micro-actions often temporally co-occur, with multiple micro-actions overlapping in time, such as concurrent head and hand movements. However, current research primarily focuses on recognizing individual micro-actions while overlooking their co-occurring nature. To address this gap, we propose a new task named Multi-label Micro-Action Detection (MMAD), which involves identifying all micro-actions in a given short video, determining their start and end times, and categorizing them. Accomplishing this requires a model capable of accurately capturing both long-term and short-term action relationships to detect multiple overlapping micro-actions. To facilitate the MMAD task, we introduce a new dataset named Multi-label Micro-Action-52 (MMA-52) and propose a baseline method equipped with a dual-path spatial-temporal adapter to address the challenges of subtle visual change in MMAD. We hope that MMA-52 can stimulate research on micro-action analysis in videos and prompt the development of spatio-temporal modeling in human-centric video understanding. The proposed MMA-52 dataset is available at: this https URL.",
    "comments": "Accepted by ICCV 2025",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2410.11571",
    "url": "https://arxiv.org/abs/2410.11571",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.257360",
    "title": "SDS -- See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration",
    "authors": "Maria Stamatopoulou, Jeffrey Li, Dimitrios Kanoulas",
    "subjects": "Robotics (cs.RO)",
    "abstract": "Imagine a robot learning locomotion skills from any single video, without labels or reward engineering. We introduce SDS (\"See it. Do it. Sorted.\"), an automated pipeline for skill acquisition from unstructured demonstrations. Using GPT-4o, SDS applies novel prompting techniques, in the form of spatio-temporal grid-based visual encoding ($G_{v}$) and structured input decomposition (SUS). These produce executable reward functions (RF) from the raw input videos. The RFs are used to train PPO policies and are optimized through closed-loop evolution, using training footage and performance metrics as self-supervised signals. SDS allows quadrupeds (e.g. Unitree Go1) to learn four gaits -- trot, bound, pace, and hop -- achieving 100% gait matching fidelity, Dynamic Time Warping (DTW) distance in the order of $10^{-6}$, and stable locomotion with zero failures, both in simulation and the real world. SDS generalizes to morphologically different quadrupeds (e.g. ANYmal) and outperforms prior work in data efficiency, training time and engineering effort. Further materials and the code are open-source under: this https URL.",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2411.16748",
    "url": "https://arxiv.org/abs/2411.16748",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.259916",
    "title": "Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance",
    "authors": "Haojie Zhang, Zhihao Liang, Ruibo Fu, Bingyan Liu, Zhengqi Wen, Xuefei Liu, Jianhua Tao, Yaling Liang",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Long-duration talking video synthesis faces persistent challenges in simultaneously achieving high video quality, portrait and temporal consistency, and computational efficiency. As video length increases, issues such as visual degradation, loss of identity consistency, temporal incoherence, and error accumulation become increasingly prominent, severely impacting the realism and reliability of generated results. To address these issues, we present LetsTalk, a diffusion transformer framework that incorporates multimodal guidance and a novel memory bank mechanism, explicitly maintaining contextual continuity and enabling robust, high-quality, and efficient long-duration talking video generation. Specifically, LetsTalk introduces a memory bank combined with a noise-regularized training strategy to mitigate error accumulation and sampling artifacts during long video generation. To further enhance efficiency and spatiotemporal consistency, LetsTalk employs a deep compression autoencoder and a spatiotemporal-aware transformer with linear attention for effective multimodal fusion. Furthermore, we systematically analyze three multimodal fusion schemes, adopting deep (Symbiotic Fusion) for portrait features to ensure visual consistency, and shallow (Direct Fusion) for audio to synchronize animation with speech while preserving motion diversity. Extensive experiments demonstrate that LetsTalk achieves state-of-the-art generation quality, producing temporally coherent and realistic talking videos with enhanced diversity and liveliness, while maintaining remarkable efficiency with 8 fewer parameters than previous approaches.",
    "comments": "13 pages, 11 figures",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2501.13368",
    "url": "https://arxiv.org/abs/2501.13368",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.262471",
    "title": "MetaWild: A Multimodal Dataset for Animal Re-Identification with Environmental Metadata",
    "authors": "Yuzhuo Li, Di Zhao, Tingrui Qiao, Yihao Wu, Bo Pang, Yun Sing Koh",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
    "abstract": "Identifying individual animals within large wildlife populations is essential for effective wildlife monitoring and conservation efforts. Recent advancements in computer vision have shown promise in animal re-identification (Animal ReID) by leveraging data from camera traps. However, existing Animal ReID datasets rely exclusively on visual data, overlooking environmental metadata that ecologists have identified as highly correlated with animal behavior and identity, such as temperature and circadian rhythms. Moreover, the emergence of multimodal models capable of jointly processing visual and textual data presents new opportunities for Animal ReID, but existing datasets fail to leverage these models' text-processing capabilities, limiting their full potential. Additionally, to facilitate the use of metadata in existing ReID methods, we propose the Meta-Feature Adapter (MFA), a lightweight module that can be incorporated into existing vision-language model (VLM)-based Animal ReID methods, allowing ReID models to leverage both environmental metadata and visual information to improve ReID performance. Experiments on MetaWild show that combining baseline ReID models with MFA to incorporate metadata consistently improves performance compared to using visual information alone, validating the effectiveness of incorporating metadata in re-identification. We hope that our proposed dataset can inspire further exploration of multimodal approaches for Animal ReID.",
    "comments": "7 pages, 6 figures",
    "matched_keyword": "multimodal",
    "matched_category": "多模态"
  },
  {
    "id": "2503.21755",
    "url": "https://arxiv.org/abs/2503.21755",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.267561",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness",
    "authors": "Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored to individual dimensions, our evaluation framework integrates generalists such as SOTA VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive human annotations to ensure evaluation alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.",
    "comments": "Equal contributions from first two authors. Project page: this https URL Code: this https URL",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2504.02906",
    "url": "https://arxiv.org/abs/2504.02906",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.269604",
    "title": "Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement",
    "authors": "Zhihan Zhang, Yixin Cao, Lizi Liao",
    "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
    "abstract": "Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at this https URL.",
    "comments": "Accepted by ACM MM 2025",
    "matched_keyword": "mllm",
    "matched_category": "多模态"
  },
  {
    "id": "2504.11695",
    "url": "https://arxiv.org/abs/2504.11695",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.271675",
    "title": "Interpreting the linear structure of vision-language model embedding spaces",
    "authors": "Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
    "abstract": "Vision-language models encode images and text in a joint space, minimizing the distance between corresponding image and text pairs. How are language and images organized in this joint space, and how do the models encode meaning and modality? To investigate this, we train and release sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, and AIMv2). SAEs approximate model embeddings as sparse linear combinations of learned directions, or \"concepts\". We find that, compared to other methods of linear feature learning, SAEs are better at reconstructing the real embeddings, while also able to retain the most sparsity. Retraining SAEs with different seeds or different data diet leads to two findings: the rare, specific concepts captured by the SAEs are liable to change drastically, but we also show that commonly-activating concepts are remarkably stable across runs. Interestingly, while most concepts activate primarily for one modality, we find they are not merely encoding modality per se. Many are almost orthogonal to the subspace that defines modality, and the concept directions do not function as good modality classifiers, suggesting that they encode cross-modal semantics. To quantify this bridging behavior, we introduce the Bridge Score, a metric that identifies concept pairs which are both co-activated across aligned image-text inputs and geometrically aligned in the shared space. This reveals that even single-modality concepts can collaborate to support cross-modal integration. We release interactive demos of the SAEs for all models, allowing researchers to explore the organization of the concept spaces. Overall, our findings uncover a sparse linear structure within VLM embedding spaces that is shaped by modality, yet stitched together through latent bridges, offering new insight into how multimodal meaning is constructed.",
    "comments": "COLM 2025",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2506.22562",
    "url": "https://arxiv.org/abs/2506.22562",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.279816",
    "title": "Improving Token-based Object Detection with Video",
    "authors": "Abhineet Singh, Nilanjan Ray",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available.",
    "comments": "Published in IEEE Access",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2507.06484",
    "url": "https://arxiv.org/abs/2507.06484",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.281851",
    "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds",
    "authors": "Fan-Yun Sun, Shengguang Wu, Christian Jacobsen, Thomas Yim, Haoming Zou, Alex Zook, Shangru Li, Yu-Hsin Chou, Ethem Can, Xunlei Wu, Clemens Eppner, Valts Blukis, Jonathan Tremblay, Jiajun Wu, Stan Birchfield, Nick Haber",
    "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Despite large-scale pretraining endowing models with language and vision reasoning capabilities, improving their spatial reasoning capability remains challenging due to the lack of data grounded in the 3D world. While it is possible for humans to manually create immersive and interactive worlds through 3D graphics, as seen in applications such as VR, gaming, and robotics, this process remains highly labor-intensive. In this paper, we propose a scalable method for generating high-quality 3D environments that can serve as training data for foundation models. We recast 3D environment building as a sequential decision-making problem, employing Vision-Language-Models (VLMs) as policies that output actions to jointly craft a 3D environment's layout, materials, lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to generate more prompt-aligned 3D environments via self-improvement fine-tuning. We demonstrate the effectiveness of 3D-Generalist and the proposed training strategy in generating simulation-ready 3D environments. Furthermore, we demonstrate its quality and scalability in synthetic data generation by pretraining a vision foundation model on the generated data. After fine-tuning the pre-trained model on downstream tasks, we show that it surpasses models pre-trained on meticulously human-crafted synthetic data and approaches results achieved with real data orders of magnitude larger.",
    "comments": "project website: this https URL",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2507.23479",
    "url": "https://arxiv.org/abs/2507.23479",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.285496",
    "title": "Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning",
    "authors": "Julia Werner, Oliver Bause, Julius Oexle, Maxime Le Floch, Franz Brinkmann, Jochen Hampe, Oliver Bringmann",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Video capsule endoscopy has become increasingly important for investigating the small intestine within the gastrointestinal tract. However, a persistent challenge remains the short battery lifetime of such compact sensor edge devices. Integrating artificial intelligence can help overcome this limitation by enabling intelligent real-time decision-making, thereby reducing the energy consumption and prolonging the battery life. However, this remains challenging due to data sparsity and the limited resources of the device restricting the overall model size. In this work, we introduce a multi-task neural network that combines the functionalities of precise self-localization within the gastrointestinal tract with the ability to detect anomalies in the small intestine within a single model. Throughout the development process, we consistently restricted the total number of parameters to ensure the feasibility to deploy such model in a small capsule. We report the first multi-task results using the recently published Galar dataset, integrating established multi-task methods and Viterbi decoding for subsequent time-series analysis. This outperforms current single-task models and represents a significant advance in AI-based approaches in this field. Our model achieves an accuracy of 93.63% on the localization task and an accuracy of 87.48% on the anomaly detection task. The approach requires only 1 million parameters while surpassing the current baselines.",
    "comments": "Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025 (submitted version)",
    "matched_keyword": "video",
    "matched_category": "多模态"
  },
  {
    "id": "2508.02028",
    "url": "https://arxiv.org/abs/2508.02028",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.288637",
    "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving",
    "authors": "Tianyuan Zhang, Ting Jin, Lu Wang, Jiangfan Liu, Siyuan Liang, Mingchuan Zhang, Aishan Liu, Xianglong Liu",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
    "abstract": "Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions.",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  },
  {
    "id": "2508.11870",
    "url": "https://arxiv.org/abs/2508.11870",
    "category": "cs",
    "source": "arxiv",
    "crawl_time": "2025-08-22T01:56:24.296904",
    "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition",
    "authors": "Ying Huang, Yuanbin Man, Wenqi Jia, Zhengzhong Tu, Junzhou Huang, Miao Yin",
    "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
    "abstract": "Adapter-based fine-tuning has gained remarkable attention in adapting large pre-trained vision language models (VLMs) for a wide range of downstream tasks efficiently. In this paradigm, only the inserted adapters are fine-tuned, without the need for training the original VLM backbone. Existing works scale adapters by integrating them into every layer of VLMs to increase the capacity of adapters. However, these methods face two primary limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters. In this paper, we propose a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing, achieving ultra-light parameter-efficient adaptation of VLMs on various tasks. To remove the high redundancy that exists among adapters across layers, we exploit the tensor-level low-rankness to formulate adapters as layer-shared tensor cores and layer-specific slices. Moreover, guided by generalization-aware fine-tuning, diverse rank-driven adapters cooperate to handle tasks that require different representations. Our experiments show that the proposed AdaRing achieves the state-of-the-art performance while reducing average training parameters by 90%.",
    "matched_keyword": "vision-language",
    "matched_category": "多模态"
  }
]