#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSè®ºæ–‡çˆ¬è™«å·¥å…·
åŸºäºMediaCrawleré¡¹ç›®ï¼Œå»æ‰æ•°æ®åº“å­˜å‚¨éƒ¨åˆ†ï¼Œä¸“é—¨ç”¨äºçˆ¬å–CSé¢†åŸŸçš„è®ºæ–‡
ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰è¿‡æ»¤ï¼Œæé«˜ç›¸å…³æ€§åˆ¤æ–­å‡†ç¡®æ€§
"""

import json
import logging
import os
import re
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Set
from urllib.request import urlopen
from urllib.error import HTTPError, URLError
from bs4 import BeautifulSoup

# å¯¼å…¥LLM API
try:
    from llm_api import get_kimi_client, analyze_paper_relevance
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›å¤‡ç”¨æ–¹æ¡ˆ
    analyze_paper_relevance = None

# é…ç½®
CRAWLER_CONFIG = {
    "request_delay": 2,  # è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
    "timeout": 10,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "enable_llm_filter": True,  # å¯ç”¨LLMè¯­ä¹‰è¿‡æ»¤ï¼Œæ™ºèƒ½ç­›é€‰ç›¸å…³è®ºæ–‡
    "llm_batch_size": 10,  # å¢åŠ LLMæ‰¹é‡å¤„ç†å¤§å°ï¼Œæé«˜æ•ˆç‡
    "relevance_threshold": 0.6,  # é™ä½ç›¸å…³æ€§é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šç›¸å…³è®ºæ–‡
}

# ArXiv CSé¢†åŸŸåˆ†ç±» - åªçˆ¬å–æ¯æ—¥è®ºæ–‡
ARXIV_CS_CATEGORIES = {
    "cs": "https://arxiv.org/list/cs/new",  # è®¡ç®—æœºç§‘å­¦æ¯æ—¥è®ºæ–‡
}

# ç ”ç©¶é¢†åŸŸå®šä¹‰ - ç”¨äºLLMåˆ¤æ–­
RESEARCH_AREAS = {
    "å¤§æ¨¡å‹ç®—æ³•": "ä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹çš„ç®—æ³•æ”¹è¿›ã€æ¶æ„ä¼˜åŒ–ã€è®­ç»ƒæ–¹æ³•ç­‰",
    "å¤§æ¨¡å‹åº”ç”¨": "å¤§æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€ä¼˜åŒ–ã€æ•ˆæœæå‡ç­‰ï¼Œä¸»è¦æŒ‡åœ¨é‡‘èã€æœç´¢æ¨èã€ç§‘å­¦ã€æ•°å­¦ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œä¸åŒ…æ‹¬åœ°ç†ä¿¡æ¯ã€èŠ¯ç‰‡è®¾è®¡ã€åŒ»ç–—ã€æ³•å¾‹ã€æ•™è‚²ç­‰é¢†åŸŸçš„åº”ç”¨",
    "æ™ºèƒ½ä½“ç³»ç»Ÿ": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€è‡ªä¸»æ™ºèƒ½ä½“ã€æ™ºèƒ½ä½“åä½œç­‰",
    "å¼ºåŒ–å­¦ä¹ ": "å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€ç­–ç•¥ä¼˜åŒ–ã€å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç­‰",
    "å¤šæ¨¡æ€å¤§æ¨¡å‹": "è§†è§‰è¯­è¨€æ¨¡å‹ã€è§†é¢‘æ¨¡å‹ã€éŸ³é¢‘æ¨¡å‹ã€å¤šæ¨¡æ€å¤§æ¨¡å‹ç­‰",
    "æ¨¡å‹å¾®è°ƒ": "LoRAã€QLoRAã€Adapterç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•",
    "æ£€ç´¢å¢å¼ºç”Ÿæˆ": "RAGç³»ç»Ÿã€çŸ¥è¯†æ£€ç´¢ã€æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆç­‰",
    "å¤§æ¨¡å‹è®­ç»ƒåŸºç¡€è®¾æ–½": "å¤§æ¨¡å‹è®­ç»ƒåŸºç¡€è®¾æ–½ã€å¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€å¤§æ¨¡å‹è®­ç»ƒå¹³å°ç­‰ï¼Œä¸åŒ…æ‹¬èŠ¯ç‰‡ç¡¬ä»¶ã€èŠ¯ç‰‡è®¾è®¡ç­‰",
    "å¤§æ¨¡å‹æ¨ç†åŸºç¡€è®¾æ–½": "å¤§æ¨¡å‹æ¨ç†åŸºç¡€è®¾æ–½ã€å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€å¤§æ¨¡å‹æ¨ç†å¹³å°ç­‰ï¼Œä¸åŒ…æ‹¬èŠ¯ç‰‡ç¡¬ä»¶ã€èŠ¯ç‰‡è®¾è®¡ç­‰",
    "å¤§æ¨¡å‹æ¨ç†ç®—æ³•": "å¤§æ¨¡å‹æ¨ç†ç®—æ³•ã€å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€å¤§æ¨¡å‹æ¨ç†å¹³å°ç­‰ï¼Œä¸åŒ…æ‹¬èŠ¯ç‰‡ç¡¬ä»¶ã€èŠ¯ç‰‡è®¾è®¡ç­‰",
    "å¤§æ¨¡å‹è®­ç»ƒæ•°æ®æ„é€ æ–¹æ³•":"é¢„è®­ç»ƒæ•°æ®æ„é€ æ–¹æ³•ã€æ•°æ®å¢å¼ºæ–¹æ³•ã€æ•°æ®æ¸…æ´—æ–¹æ³•ç­‰",
    "å¾®è°ƒæ•°æ®æ„é€ æ–¹æ³•":"å¾®è°ƒæ•°æ®æ„é€ æ–¹æ³•ã€æ•°æ®å¢å¼ºæ–¹æ³•ã€æ•°æ®æ¸…æ´—æ–¹æ³•ç­‰",
    "åè®­ç»ƒæ•°æ®æ„é€ æ–¹æ³•":"åè®­ç»ƒæ•°æ®æ„é€ æ–¹æ³•ã€æ•°æ®å¢å¼ºæ–¹æ³•ã€æ•°æ®æ¸…æ´—æ–¹æ³•ç­‰"
}

# ä¿ç•™å…³é”®è¯è¿‡æ»¤ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
CS_KEYWORDS = {
    "å¤§æ¨¡å‹": ["large language model", "LLM", "GPT"],
    "æ™ºèƒ½ä½“": ["agent", "intelligent agent", "multi-agent", "autonomous agent"],
    "å¼ºåŒ–å­¦ä¹ ": ["reinforcement learning", "RL", "PPO", "DPO"],
    "å¤šæ¨¡æ€": ["multimodal", "vision-language", "image-text", "audio-visual", "video", "VLM", "MLLM"],
    "å¾®è°ƒ": ["fine-tuning", "adapter", "LoRA", "QLoRA"],
    "é¢„è®­ç»ƒ": ["pre-training", "pre-trained"],
    "ä¼˜åŒ–ç®—æ³•": ["optimizer"],
    "æ£€ç´¢å¢å¼ºç”Ÿæˆ": ["RAG", "retrieval-augmented generation", "retrieval-augmented generation", "RAG", "retrieval-augmented generation", "retrieval-augmented generation"],
    "åè®­ç»ƒ": ["post-training"]
}


class CSPaperCrawler:
    """CSè®ºæ–‡çˆ¬è™«ç±»"""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.setup_logging()
        self.setup_keywords()
        self.setup_llm_filter_config()
        self.output_dir = self._create_output_dir()
        self.crawled_papers: Set[str] = set()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('cs_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_keywords(self):
        """è®¾ç½®å…³é”®è¯"""
        self.keywords = {}
        for category, keyword_list in CS_KEYWORDS.items():
            for keyword in keyword_list:
                self.keywords[keyword.lower()] = category
        
        self.logger.info(f"åŠ è½½äº† {len(self.keywords)} ä¸ªå…³é”®è¯ï¼Œæ¶µç›– {len(CS_KEYWORDS)} ä¸ªé¢†åŸŸ")
    
    def setup_llm_filter_config(self):
        """è®¾ç½®LLMè¿‡æ»¤é…ç½®"""
        # ä»é…ç½®æ–‡ä»¶è¯»å–è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        llm_config = self.config.get("crawler", {}).get("llm_filter", {})
        
        self.llm_filter_enabled = llm_config.get("enabled", CRAWLER_CONFIG["enable_llm_filter"])
        self.relevance_threshold = llm_config.get("relevance_threshold", CRAWLER_CONFIG["relevance_threshold"])
        self.llm_batch_size = llm_config.get("batch_size", CRAWLER_CONFIG["llm_batch_size"])
        self.request_interval = llm_config.get("request_interval", 0.5)  # å‡å°‘è¯·æ±‚é—´éš”ï¼Œæé«˜é€Ÿåº¦
        
        if self.llm_filter_enabled:
            print(f"ğŸ¤– LLMè¯­ä¹‰è¿‡æ»¤å·²å¯ç”¨")
            print(f"   ğŸ“Š ç›¸å…³æ€§é˜ˆå€¼: {self.relevance_threshold}")
            print(f"   ğŸ“¦ æ‰¹é‡å¤§å°: {self.llm_batch_size}")
            print(f"   â±ï¸  è¯·æ±‚é—´éš”: {self.request_interval}ç§’")
            self.logger.info(f"LLMè¯­ä¹‰è¿‡æ»¤å·²å¯ç”¨ï¼Œé˜ˆå€¼: {self.relevance_threshold}, æ‰¹é‡å¤§å°: {self.llm_batch_size}")
        else:
            print("ğŸ” LLMè¯­ä¹‰è¿‡æ»¤å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å…³é”®è¯è¿‡æ»¤")
            self.logger.info("LLMè¯­ä¹‰è¿‡æ»¤å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å…³é”®è¯è¿‡æ»¤")
    
    def _create_output_dir(self) -> str:
        """åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œæ ¼å¼ä¸ºYYMMDD"""
        today = datetime.now()
        dir_name = today.strftime("%y%m%d")
        
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
            self.logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {dir_name}")
        
        return dir_name
    
    def get_page(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"   ğŸŒ æ­£åœ¨è·å–é¡µé¢: {url}")
            
            # æ·»åŠ è¶…æ—¶æ§åˆ¶
            import urllib.request
            import socket
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º30ç§’
            socket.setdefaulttimeout(30)
            
            # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            req = urllib.request.Request(url, headers=headers)
            response = urllib.request.urlopen(req, timeout=30)
            
            content = response.read()
            print(f"   âœ… é¡µé¢è·å–æˆåŠŸï¼Œå¤§å°: {len(content)} bytes")
            
            return BeautifulSoup(content, 'html.parser')
            
        except urllib.error.HTTPError as e:
            print(f"   âŒ HTTPé”™è¯¯ {e.code}: {e.reason}")
            self.logger.warning(f"HTTPé”™è¯¯ {e.code}: {e.reason}")
            return None
        except urllib.error.URLError as e:
            print(f"   âŒ URLé”™è¯¯: {e.reason}")
            self.logger.warning(f"URLé”™è¯¯: {e.reason}")
            return None
        except socket.timeout:
            print(f"   â° è¯·æ±‚è¶…æ—¶")
            self.logger.warning("è¯·æ±‚è¶…æ—¶")
            return None
        except Exception as e:
            print(f"   âŒ è·å–é¡µé¢æ—¶å‡ºé”™: {e}")
            self.logger.error(f"è·å–é¡µé¢æ—¶å‡ºé”™: {e}")
            return None
    
    def start(self):
        """å¯åŠ¨çˆ¬è™«"""
        self.logger.info("å¼€å§‹çˆ¬å–CSé¢†åŸŸæ¯æ—¥è®ºæ–‡...")
        
        try:
            # çˆ¬å–ArXiv CSé¢†åŸŸæ¯æ—¥è®ºæ–‡
            self.crawl_arxiv_papers()
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            self.generate_summary_report()
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
        self.logger.info(f"çˆ¬å–å®Œæˆï¼è®ºæ–‡å·²ä¿å­˜åˆ° {self.output_dir} ç›®å½•")
    
    def crawl_arxiv_papers(self):
        """çˆ¬å–ArXivè®ºæ–‡"""
        print("ğŸŒ æ­£åœ¨çˆ¬å–ArXiv CSé¢†åŸŸæ¯æ—¥è®ºæ–‡...")
        self.logger.info("æ­£åœ¨çˆ¬å–ArXiv CSé¢†åŸŸæ¯æ—¥è®ºæ–‡...")
        
        all_papers = []
        for category, url in ARXIV_CS_CATEGORIES.items():
            print(f"ğŸ“š çˆ¬å–ç±»åˆ«: {category}")
            self.logger.info(f"çˆ¬å–ç±»åˆ«: {category}")
            try:
                print(f"   ğŸ”— è®¿é—®URL: {url}")
                papers = self.crawl_arxiv_category(url, category)
                if papers:
                    all_papers.extend(papers)
                    print(f"   âœ… ç±»åˆ« {category}: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                    self.logger.info(f"ç±»åˆ« {category}: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                else:
                    print(f"   âš ï¸  ç±»åˆ« {category}: æœªæ‰¾åˆ°è®ºæ–‡")
                print(f"   â±ï¸  ç­‰å¾… {CRAWLER_CONFIG['request_delay']} ç§’...")
                time.sleep(CRAWLER_CONFIG["request_delay"])
            except Exception as e:
                print(f"   âŒ çˆ¬å– {category} æ—¶å‡ºé”™: {e}")
                self.logger.error(f"çˆ¬å– {category} æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(all_papers)} ç¯‡è®ºæ–‡")
        
        if all_papers:
            print("ğŸ” å¼€å§‹è¿‡æ»¤è®ºæ–‡...")
            filtered_papers = self.filter_papers_by_keywords(all_papers)
            print(f"âœ… å…³é”®è¯ç­›é€‰åï¼Œå…±æ‰¾åˆ° {len(filtered_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            self.logger.info(f"å…³é”®è¯ç­›é€‰åï¼Œå…±æ‰¾åˆ° {len(filtered_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            self.save_papers(filtered_papers)
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
    
    def crawl_arxiv_category(self, url: str, category: str) -> List[Dict]:
        """çˆ¬å–ç‰¹å®šArXivç±»åˆ«çš„è®ºæ–‡"""
        papers = []
        
        try:
            bs = self.get_page(url)
            if bs:
                papers = self.extract_arxiv_papers(bs, category)
        except Exception as e:
            self.logger.error(f"çˆ¬å– {category} é¡µé¢æ—¶å‡ºé”™: {e}")
        
        return papers
    
    def extract_arxiv_papers(self, bs: BeautifulSoup, category: str) -> List[Dict]:
        """ä»ArXivé¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            print(f"   ğŸ” æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
            
            # æ ¹æ®å®é™…HTMLç»“æ„ï¼Œè®ºæ–‡ä¿¡æ¯åœ¨dtå’Œddæ ‡ç­¾å¯¹ä¸­
            dt_elements = bs.find_all("dt")
            print(f"   ğŸ“„ æ‰¾åˆ° {len(dt_elements)} ä¸ªè®ºæ–‡æ¡ç›®")
            
            processed_count = 0
            for dt_element in dt_elements:
                try:
                    # è·å–å¯¹åº”çš„ddå…ƒç´ ï¼ˆè®ºæ–‡è¯¦ç»†ä¿¡æ¯ï¼‰
                    dd_element = dt_element.find_next_sibling("dd")
                    if not dd_element:
                        continue
                    
                    # æå–è®ºæ–‡ID
                    link_element = dt_element.find("a", title="Abstract")
                    if not link_element:
                        continue
                    
                    paper_id = link_element.get("id")
                    if not paper_id:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–è¿‡
                    if paper_id in self.crawled_papers:
                        continue
                    
                    # åˆ›å»ºè®ºæ–‡ä¿¡æ¯å­—å…¸
                    paper_info = {
                        "id": paper_id,
                        "url": f"https://arxiv.org/abs/{paper_id}",
                        "category": category,
                        "source": "arxiv",
                        "crawl_time": datetime.now().isoformat()
                    }
                    
                    # æå–æ ‡é¢˜
                    title_element = dd_element.find("div", class_="list-title")
                    if title_element:
                        title_text = title_element.get_text(strip=True)
                        # ç§»é™¤"Title:"å‰ç¼€
                        if title_text.startswith("Title:"):
                            title_text = title_text[6:].strip()
                        paper_info["title"] = title_text
                    
                    # æå–æ‘˜è¦
                    abstract_element = dd_element.find("p", class_="mathjax")
                    if abstract_element:
                        paper_info["abstract"] = abstract_element.get_text(strip=True)
                    
                    # æå–ä½œè€…
                    authors_element = dd_element.find("div", class_="list-authors")
                    if authors_element:
                        authors_text = authors_element.get_text(strip=True)
                        # ç§»é™¤"Authors:"å‰ç¼€
                        if authors_text.startswith("Authors:"):
                            authors_text = authors_text[8:].strip()
                        paper_info["authors"] = authors_text
                    
                    # æå–ä¸»é¢˜
                    subjects_element = dd_element.find("div", class_="list-subjects")
                    if subjects_element:
                        subjects_text = subjects_element.get_text(strip=True)
                        # ç§»é™¤"Subjects:"å‰ç¼€
                        if subjects_text.startswith("Subjects:"):
                            subjects_text = subjects_text[9:].strip()
                        paper_info["subjects"] = subjects_text
                    
                    papers.append(paper_info)
                    processed_count += 1
                    
                    # æ¯å¤„ç†10ç¯‡è®ºæ–‡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if processed_count % 10 == 0:
                        print(f"   ğŸ“Š å·²å¤„ç† {processed_count}/{len(dt_elements)} ç¯‡è®ºæ–‡")
                    
                except Exception as e:
                    self.logger.warning(f"æå–è®ºæ–‡å…ƒç´ æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"   âœ… æˆåŠŸæå– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"   âŒ æå–è®ºæ–‡æ—¶å‡ºé”™: {e}")
            self.logger.error(f"æå–è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def filter_papers_by_keywords(self, papers: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨LLMè¯­ä¹‰è¿‡æ»¤è®ºæ–‡ï¼Œæé«˜ç›¸å…³æ€§åˆ¤æ–­å‡†ç¡®æ€§"""
        if not papers:
            return []
        
        if self.llm_filter_enabled and analyze_paper_relevance:
            return self._filter_papers_with_llm(papers)
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å…³é”®è¯è¿‡æ»¤
            self.logger.warning("LLMè¿‡æ»¤ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯è¿‡æ»¤ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            return self._filter_papers_with_keywords(papers)
    
    def _filter_papers_with_llm(self, papers: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨LLMè¯­ä¹‰è¿‡æ»¤è®ºæ–‡"""
        if not papers:
            return []
        
        print(f"ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¯­ä¹‰è¿‡æ»¤ {len(papers)} ç¯‡è®ºæ–‡...")
        self.logger.info(f"å¼€å§‹ä½¿ç”¨LLMè¯­ä¹‰è¿‡æ»¤ {len(papers)} ç¯‡è®ºæ–‡...")
        
        # è·å–LLMè¿‡æ»¤é…ç½®
        llm_config = self.config.get("llm_filter", {})
        batch_size = llm_config.get("batch_size", 10)  # å¢åŠ æ‰¹é‡å¤§å°
        relevance_threshold = llm_config.get("relevance_threshold", 0.6)
        request_interval = llm_config.get("request_interval", 0.5)
        
        # åˆ†æ‰¹å¤„ç†è®ºæ–‡
        filtered_papers = []
        total_batches = (len(papers) + batch_size - 1) // batch_size
        
        print(f"   ğŸ“¦ å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š {batch_size} ç¯‡è®ºæ–‡")
        
        for batch_idx in range(0, len(papers), batch_size):
            batch = papers[batch_idx:batch_idx + batch_size]
            current_batch = (batch_idx // batch_size) + 1
            
            print(f"   ğŸ”„ å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches}ï¼ŒåŒ…å« {len(batch)} ç¯‡è®ºæ–‡")
            self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches}ï¼ŒåŒ…å« {len(batch)} ç¯‡è®ºæ–‡")
            
            # æ‰¹é‡å¤„ç†è®ºæ–‡ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
            batch_titles = [paper.get('title', '') for paper in batch]
            batch_abstracts = [paper.get('abstract', '') for paper in batch]
            
            try:
                print(f"   ğŸ¤– è°ƒç”¨LLMåˆ†ææ‰¹æ¬¡ {current_batch}...")
                
                # æ‰¹é‡åˆ†æç›¸å…³æ€§
                relevance_results = self._analyze_batch_relevance(
                    batch_titles, batch_abstracts, batch
                )
                
                # å¤„ç†ç»“æœ
                for i, (paper, result) in enumerate(zip(batch, relevance_results)):
                    if result:
                        # è·å–ç›¸å…³æ€§åˆ†æ•°
                        relevance_score = float(result.get("relevance_score", 0))
                        best_match_area = result.get("best_match_area", "æœªçŸ¥")
                        reasoning = result.get("relevance_reasoning", "æ— æ¨ç†è¯´æ˜")
                        
                        # è®°å½•åˆ†æç»“æœ
                        paper['llm_analysis'] = {
                            'relevance_score': relevance_score,
                            'best_match_area': best_match_area,
                            'reasoning': reasoning,
                            'is_relevant': result.get("is_relevant", False),
                            'summary': result.get("summary", "")
                        }
                        
                        # åˆ¤æ–­æ˜¯å¦ç›¸å…³
                        if relevance_score >= relevance_threshold:
                            filtered_papers.append(paper)
                            print(f"      âœ… è®ºæ–‡ {i+1} ç›¸å…³ (åˆ†æ•°: {relevance_score:.2f}, é¢†åŸŸ: {best_match_area})")
                        else:
                            print(f"      âŒ è®ºæ–‡ {i+1} ä¸ç›¸å…³ (åˆ†æ•°: {relevance_score:.2f}, é¢†åŸŸ: {best_match_area})")
                    else:
                        print(f"      âš ï¸  è®ºæ–‡ {i+1} LLMåˆ†æå¤±è´¥")
                        # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨å…³é”®è¯è¿‡æ»¤ä½œä¸ºå¤‡é€‰
                        if self.config.get("llm_filter", {}).get("enable_fallback", True):
                            if self._check_paper_relevance_with_keywords(paper):
                                filtered_papers.append(paper)
                                print(f"      âœ… è®ºæ–‡ {i+1} å…³é”®è¯è¿‡æ»¤é€šè¿‡")
                            else:
                                print(f"      âŒ è®ºæ–‡ {i+1} å…³é”®è¯è¿‡æ»¤ä¸é€šè¿‡")
                
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…APIé™åˆ¶
                if request_interval > 0 and current_batch < total_batches:
                    print(f"   â±ï¸  ç­‰å¾… {request_interval} ç§’...")
                    time.sleep(request_interval)
                    
            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡ {current_batch} å¤„ç†å¤±è´¥: {e}")
                self.logger.error(f"æ‰¹æ¬¡ {current_batch} å¤„ç†å¤±è´¥: {e}")
                
                # å¦‚æœæ‰¹é‡å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å•ç¯‡å¤„ç†
                print(f"   ğŸ”„ å›é€€åˆ°å•ç¯‡å¤„ç†...")
                for paper in batch:
                    try:
                        relevance_result = analyze_paper_relevance(
                            paper_title=paper.get('title', ''),
                            paper_abstract=paper.get('abstract', ''),
                            research_areas=RESEARCH_AREAS
                        )
                        
                        if relevance_result:
                            relevance_score = float(relevance_result.get("relevance_score", 0))
                            if relevance_score >= relevance_threshold:
                                filtered_papers.append(paper)
                                print(f"      âœ… å›é€€å¤„ç†: è®ºæ–‡ç›¸å…³ (åˆ†æ•°: {relevance_score:.2f})")
                        else:
                            # ä½¿ç”¨å…³é”®è¯è¿‡æ»¤ä½œä¸ºå¤‡é€‰
                            if self._check_paper_relevance_with_keywords(paper):
                                filtered_papers.append(paper)
                                print(f"      âœ… å›é€€å¤„ç†: å…³é”®è¯è¿‡æ»¤é€šè¿‡")
                    except Exception as e2:
                        print(f"      âŒ å›é€€å¤„ç†å¤±è´¥: {e2}")
                        # æœ€åå¤‡é€‰ï¼šç›´æ¥é€šè¿‡
                        if self.config.get("llm_filter", {}).get("enable_fallback", True):
                            filtered_papers.append(paper)
                            print(f"      âœ… å›é€€å¤„ç†: ç›´æ¥é€šè¿‡ï¼ˆå¤‡é€‰ï¼‰")
        
        print(f"ğŸ¤– LLMè¯­ä¹‰è¿‡æ»¤å®Œæˆï¼Œä» {len(papers)} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(filtered_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        self.logger.info(f"LLMè¯­ä¹‰è¿‡æ»¤å®Œæˆï¼Œä» {len(papers)} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(filtered_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        return filtered_papers
    
    def _filter_papers_with_keywords(self, papers: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨å…³é”®è¯è¿‡æ»¤ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        self.logger.info("ä½¿ç”¨å…³é”®è¯è¿‡æ»¤è®ºæ–‡...")
        
        filtered_papers = []
        for paper in papers:
            if self._check_paper_relevance_with_keywords(paper):
                filtered_papers.append(paper)
        
        self.logger.info(f"å…³é”®è¯è¿‡æ»¤å®Œæˆï¼Œä» {len(papers)} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(filtered_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        return filtered_papers
    
    def _check_paper_relevance_with_keywords(self, paper: Dict) -> bool:
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦ä¸å…³é”®è¯ç›¸å…³"""
        title = paper.get("title", "").lower()
        abstract = paper.get("abstract", "").lower()
        text = f"{title} {abstract}"
        
        for keyword, category in self.keywords.items():
            if keyword.lower() in text:
                paper["matched_keyword"] = keyword
                paper["matched_category"] = category
                return True
        
        return False
    
    def save_papers(self, papers: List[Dict]):
        """ä¿å­˜è®ºæ–‡åˆ°æœ¬åœ°æ–‡ä»¶"""
        if not papers:
            return
        
        papers_by_category = {}
        for paper in papers:
            category = paper.get("matched_category", "å…¶ä»–")
            if category not in papers_by_category:
                papers_by_category[category] = []
            papers_by_category[category].append(paper)
        
        for category, paper_list in papers_by_category.items():
            safe_category = re.sub(r'[<>:"/\\|?*]', '_', category)
            filename = f"{self.output_dir}/{safe_category}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(paper_list, f, ensure_ascii=False, indent=2)
                self.logger.info(f"ä¿å­˜ {len(paper_list)} ç¯‡è®ºæ–‡åˆ° {filename}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        all_papers_filename = f"{self.output_dir}/all_papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(all_papers_filename, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ä¿å­˜æ‰€æœ‰è®ºæ–‡åˆ° {all_papers_filename}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ±‡æ€»æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        try:
            summary = {
                "crawl_time": datetime.now().isoformat(),
                "output_directory": self.output_dir,
                "total_papers": len(self.crawled_papers),
                "files": []
            }
            
            for filename in os.listdir(self.output_dir):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.output_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, list):
                                summary["files"].append({
                                    "filename": filename,
                                    "paper_count": len(data),
                                    "file_size": os.path.getsize(file_path)
                                })
                    except Exception as e:
                        self.logger.warning(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            
            summary_filename = f"{self.output_dir}/crawl_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(summary_filename, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š: {summary_filename}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")

    def _analyze_batch_relevance(self, titles: List[str], abstracts: List[str], papers: List[Dict]) -> List[Optional[Dict]]:
        """æ‰¹é‡åˆ†æè®ºæ–‡ç›¸å…³æ€§ï¼Œæé«˜APIè°ƒç”¨æ•ˆç‡"""
        try:
            # æ„å»ºæ‰¹é‡åˆ†ææç¤º
            batch_prompt = self._build_batch_analysis_prompt(titles, abstracts)
            
            # è°ƒç”¨LLMè¿›è¡Œæ‰¹é‡åˆ†æ
            from llm_api import analyze_paper_relevance
            
            # è¿™é‡Œå¯ä»¥ä¼˜åŒ–ä¸ºçœŸæ­£çš„æ‰¹é‡è°ƒç”¨ï¼Œä½†ç›®å‰å…ˆä½¿ç”¨å•ç¯‡è°ƒç”¨çš„æ–¹å¼
            # æœªæ¥å¯ä»¥ä¿®æ”¹llm_api.pyæ”¯æŒæ‰¹é‡åˆ†æ
            results = []
            for title, abstract in zip(titles, abstracts):
                try:
                    result = analyze_paper_relevance(
                        paper_title=title,
                        paper_abstract=abstract,
                        research_areas=RESEARCH_AREAS
                    )
                    results.append(result)
                except Exception as e:
                    print(f"      âš ï¸  å•ç¯‡åˆ†æå¤±è´¥: {e}")
                    results.append(None)
            
            return results
            
        except Exception as e:
            print(f"      âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            return [None] * len(titles)
    
    def _build_batch_analysis_prompt(self, titles: List[str], abstracts: List[str]) -> str:
        """æ„å»ºæ‰¹é‡åˆ†ææç¤º"""
        prompt = "è¯·åˆ†æä»¥ä¸‹è®ºæ–‡çš„ç›¸å…³æ€§ï¼Œæ¯ç¯‡è®ºæ–‡ç»™å‡ºç›¸å…³æ€§åˆ†æ•°å’Œé¢†åŸŸåŒ¹é…ï¼š\n\n"
        
        for i, (title, abstract) in enumerate(zip(titles, abstracts)):
            prompt += f"è®ºæ–‡ {i+1}:\n"
            prompt += f"æ ‡é¢˜: {title}\n"
            prompt += f"æ‘˜è¦: {abstract[:200]}...\n\n"
        
        prompt += "è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«æ¯ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§åˆ†æã€‚"
        return prompt


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    try:
        from bs4 import BeautifulSoup
        print("âœ“ BeautifulSoup å·²å®‰è£…")
    except ImportError:
        print("âœ— BeautifulSoup æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install beautifulsoup4")
        return False
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("CSè®ºæ–‡çˆ¬è™«å¯åŠ¨æ£€æŸ¥...")
    print("=" * 50)
    
    if not check_dependencies():
        print("\nè¯·å…ˆå®‰è£…æ‰€éœ€ä¾èµ–ï¼")
        sys.exit(1)
    
    print("\næ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹çˆ¬å–è®ºæ–‡...")
    print("=" * 50)
    
    try:
        crawler = CSPaperCrawler()
        crawler.start()
    except Exception as e:
        print(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        sys.exit(1) 