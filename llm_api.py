import os
import time
import json
import logging
import glob
import requests
import httpx
from datetime import datetime
from openai import OpenAI
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config_manager import get_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Tokenç»Ÿè®¡ç±»
class TokenUsageTracker:
    """Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_estimate = 0.0
        self.api_calls = 0
        self.start_time = datetime.now()
    
    def add_usage(self, input_tokens: int, output_tokens: int, model: str = "unknown"):
        """æ·»åŠ tokenä½¿ç”¨é‡"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.api_calls += 1
        
        # ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºå¸¸è§æ¨¡å‹çš„å®šä»·ï¼‰
        cost = self._estimate_cost(input_tokens, output_tokens, model)
        self.total_cost_estimate += cost
        
        logging.info(f"Tokenä½¿ç”¨é‡: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}, ä¼°ç®—æˆæœ¬=${cost:.4f}")
    
    def _estimate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        # åŸºäºå¸¸è§æ¨¡å‹çš„å®šä»·ï¼ˆæ¯1000 tokensçš„ä»·æ ¼ï¼‰
        pricing = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
            "kimi-k2-0711-preview": {"input": 0.002, "output": 0.004},  # ä¼°ç®—
            "deepseek-chat": {"input": 0.001, "output": 0.002},  # ä¼°ç®—
            "unknown": {"input": 0.002, "output": 0.004}  # é»˜è®¤ä¼°ç®—
        }
        
        model_pricing = pricing.get(model, pricing["unknown"])
        input_cost = (input_tokens / 1000) * model_pricing["input"]
        output_cost = (output_tokens / 1000) * model_pricing["output"]
        
        return input_cost + output_cost
    
    def get_summary(self) -> Dict:
        """è·å–ä½¿ç”¨é‡æ‘˜è¦"""
        duration = datetime.now() - self.start_time
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "api_calls": self.api_calls,
            "total_cost_estimate": self.total_cost_estimate,
            "duration_seconds": duration.total_seconds(),
            "start_time": self.start_time.isoformat(),
            "end_time": datetime.now().isoformat()
        }
    
    def print_summary(self):
        """æ‰“å°ä½¿ç”¨é‡æ‘˜è¦"""
        summary = self.get_summary()
        print("\n" + "="*60)
        print("ğŸ“Š Tokenä½¿ç”¨é‡ç»Ÿè®¡")
        print("="*60)
        print(f"æ€»è¾“å…¥Token: {summary['total_input_tokens']:,}")
        print(f"æ€»è¾“å‡ºToken: {summary['total_output_tokens']:,}")
        print(f"æ€»Token: {summary['total_tokens']:,}")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {summary['api_calls']}")
        print(f"ä¼°ç®—æ€»æˆæœ¬: ${summary['total_cost_estimate']:.4f}")
        print(f"è¿è¡Œæ—¶é•¿: {summary['duration_seconds']:.1f} ç§’")
        print(f"å¼€å§‹æ—¶é—´: {summary['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {summary['end_time']}")
        print("="*60)
    
    def save_summary(self, filename: str = None):
        """ä¿å­˜ä½¿ç”¨é‡æ‘˜è¦åˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"token_usage_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        summary = self.get_summary()
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logging.info(f"Tokenä½¿ç”¨é‡ç»Ÿè®¡å·²ä¿å­˜åˆ°: {filename}")

# å…¨å±€tokenè·Ÿè¸ªå™¨
token_tracker = TokenUsageTracker()

# ==================== Configuration ====================
# ä»é…ç½®æ–‡ä»¶è¯»å–APIé…ç½®ï¼Œä¸å†ç¡¬ç¼–ç 
config = get_config()

# è®ºæ–‡è§£è¯»ç›¸å…³çš„é…ç½®
PAPER_DATA_DIR = './'  # æ ¹ç›®å½•ï¼Œç”¨äºåˆ›å»ºæ—¥æœŸå­ç›®å½•

# è®ºæ–‡è§£è¯»çš„æç¤ºæ¨¡æ¿
ABSTRACT_TRANSLATION_PROMPT = '''
è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒå­¦æœ¯æ€§å’Œå‡†ç¡®æ€§ï¼š

{abstract}

è¯·ç›´æ¥è¾“å‡ºä¸­æ–‡ç¿»è¯‘ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚
'''

PAPER_ANALYSIS_PROMPT = '''
è¯·åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ‘˜è¦ï¼š{abstract}
å­¦ç§‘åˆ†ç±»ï¼š{subjects}
urlï¼š{url}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚
'''

# è®ºæ–‡è§£è¯»çš„é—®é¢˜åˆ—è¡¨
ANALYSIS_QUESTIONS = [
    "æ€»ç»“ä¸€ä¸‹è®ºæ–‡çš„ä¸»è¦å†…å®¹",
    "è¿™ç¯‡è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "æœ‰å“ªäº›ç›¸å…³ç ”ç©¶ï¼Ÿå¼•ç”¨ä¸èƒ½åªç»™å‡ºåºå·ï¼Œéœ€è¦ç»“åˆpdf referenceç« èŠ‚ç»™å‡ºç›¸å…³ç ”ç©¶çš„è®ºæ–‡æ ‡é¢˜ã€‚",
    "è®ºæ–‡å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ",
    "è®ºæ–‡åšäº†å“ªäº›å®éªŒï¼Ÿå®éªŒç»“è®ºå¦‚ä½•ï¼Ÿ",
    "æœ‰ä»€ä¹ˆå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„ç‚¹ï¼Ÿ"
]

# è®¾ç½®æ—¥å¿—è®°å½•
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ==================== è·å–APIé…ç½®çš„å‡½æ•° ====================
def get_api_config():
    """ä»é…ç½®æ–‡ä»¶è·å–APIé…ç½®"""
    try:
        llm_config = config.get("llm", {})
        provider = llm_config.get("provider", "kimi")
        
        if provider == "kimi":
            kimi_config = llm_config.get("kimi", {})
            return {
                "api_key": kimi_config.get("api_key"),
                "base_url": kimi_config.get("base_url", "https://api.moonshot.cn/v1"),
                "model": kimi_config.get("model", "kimi-k2-0711-preview"),
                "temperature": kimi_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "openai":
            openai_config = llm_config.get("openai", {})
            return {
                "api_key": openai_config.get("api_key"),
                "base_url": openai_config.get("base_url", "https://api.openai.com/v1"),
                "model": openai_config.get("model", "gpt-4-turbo"),
                "temperature": openai_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "deepseek":
            deepseek_config = llm_config.get("deepseek", {})
            return {
                "api_key": deepseek_config.get("api_key"),
                "base_url": deepseek_config.get("base_url", "https://api.deepseek.com/v1"),
                "model": deepseek_config.get("model", "deepseek-chat"),
                "temperature": deepseek_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return None
            
    except Exception as e:
        logging.error(f"è·å–APIé…ç½®å¤±è´¥: {e}")
        return None

def get_kimi_client():
    """è·å–Kimiå®¢æˆ·ç«¯"""
    try:
        api_config = get_api_config()
        if not api_config or not api_config.get("api_key"):
            logging.error("æœªæ‰¾åˆ°æœ‰æ•ˆçš„APIé…ç½®")
            return None
            
        return OpenAI(
            api_key=api_config["api_key"],
            base_url=api_config["base_url"]
        )
    except Exception as e:
        logging.error(f"åˆ›å»ºKimiå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def get_deepseek_client():
    """è·å–DeepSeekå®¢æˆ·ç«¯"""
    try:
        config = get_config()
        deepseek_config = config.get("llm", {}).get("deepseek", {})
        
        if not deepseek_config.get("api_key"):
            logging.error("DeepSeek APIå¯†é’¥æœªé…ç½®")
            return None
        
        # DeepSeek R1ä½¿ç”¨ä¸åŒçš„base_urlæ ¼å¼
        base_url = deepseek_config.get("base_url", "https://api.deepseek.com")
        if not base_url.endswith("/v1"):
            base_url = f"{base_url}/v1"
        
        client = OpenAI(
            api_key=deepseek_config["api_key"],
            base_url=base_url
        )
        
        logging.info("DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        return client
        
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def get_temperature_for_scenario(scenario: str) -> float:
    """
    æ ¹æ®ä½¿ç”¨åœºæ™¯è·å–åˆé€‚çš„temperatureå€¼
    
    Args:
        scenario: ä½¿ç”¨åœºæ™¯
        
    Returns:
        float: åˆé€‚çš„temperatureå€¼
    """
    config = get_config()
    temperature_config = config.get("llm", {}).get("temperature_by_scenario", {})
    
    # åœºæ™¯åŒ–temperatureé…ç½®
    scenario_temperatures = {
        "paper_relevance": 0.1,      # è®ºæ–‡ç›¸å…³æ€§åˆ†æ - éœ€è¦ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
        "paper_analysis": 0.3,       # è®ºæ–‡å†…å®¹åˆ†æ - éœ€è¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
        "report_generation": 0.7,    # æŠ¥å‘Šç”Ÿæˆ - éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ä½†ä¿æŒå‡†ç¡®æ€§
        "general_conversation": 1.3, # é€šç”¨å¯¹è¯ - éœ€è¦çµæ´»æ€§å’Œåˆ›é€ æ€§
        "creative_writing": 1.5,     # åˆ›æ„å†™ä½œ - éœ€è¦é«˜åˆ›é€ æ€§
        "code_generation": 0.0,      # ä»£ç ç”Ÿæˆ - éœ€è¦ç²¾ç¡®æ€§
        "data_extraction": 1.0,      # æ•°æ®æŠ½å– - éœ€è¦å‡†ç¡®æ€§
        "translation": 1.3,          # ç¿»è¯‘ä»»åŠ¡ - éœ€è¦çµæ´»æ€§
    }
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    temperature = temperature_config.get(scenario, scenario_temperatures.get(scenario, 1.0))
    
    logging.info(f"åœºæ™¯ '{scenario}' ä½¿ç”¨ temperature: {temperature}")
    return temperature

def get_api_config_with_scenario(scenario: str = "general"):
    """
    æ ¹æ®åœºæ™¯è·å–APIé…ç½®ï¼ŒåŒ…æ‹¬åˆé€‚çš„temperature
    
    Args:
        scenario: ä½¿ç”¨åœºæ™¯
        
    Returns:
        Dict: APIé…ç½®å­—å…¸
    """
    try:
        llm_config = config.get("llm", {})
        provider = llm_config.get("provider", "kimi")
        
        if provider == "kimi":
            kimi_config = llm_config.get("kimi", {})
            return {
                "api_key": kimi_config.get("api_key"),
                "base_url": kimi_config.get("base_url", "https://api.moonshot.cn/v1"),
                "model": kimi_config.get("model", "kimi-k2-0711-preview"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "openai":
            openai_config = llm_config.get("openai", {})
            return {
                "api_key": openai_config.get("api_key"),
                "base_url": openai_config.get("base_url", "https://api.openai.com/v1"),
                "model": openai_config.get("model", "gpt-4-turbo"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": openai_config.get("max_tokens", 4000)
            }
        elif provider == "deepseek":
            deepseek_config = llm_config.get("deepseek", {})
            return {
                "api_key": deepseek_config.get("api_key"),
                "base_url": deepseek_config.get("base_url", "https://api.deepseek.com/v1"),
                "model": deepseek_config.get("model", "deepseek-chat"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": deepseek_config.get("max_tokens", 4000)
            }
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return None
    except Exception as e:
        logging.error(f"è·å–APIé…ç½®å¤±è´¥: {e}")
        return None

# ==================== è®ºæ–‡è§£è¯»ç›¸å…³å‡½æ•° ====================
def analyze_paper_with_questions(paper_title: str, paper_abstract: str, pdf_content: str = None) -> Dict:
    """
    ä½¿ç”¨LLMåˆ†æè®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜ï¼Œå‡å°‘tokenæ¶ˆè€—
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
        paper_abstract: è®ºæ–‡æ‘˜è¦
        pdf_content: PDFå†…å®¹ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        Dict: åŒ…å«æ‰€æœ‰é—®é¢˜ç­”æ¡ˆçš„å­—å…¸
    """
    try:
        # æ ¹æ®é…ç½®é€‰æ‹©å®¢æˆ·ç«¯
        config = get_config()
        provider = config.get("llm", {}).get("provider", "deepseek")
        
        if provider == "deepseek":
            client = get_deepseek_client()
        elif provider == "kimi":
            client = get_kimi_client()
        elif provider == "openai":
            client = get_openai_client()
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return {}
        
        if not client:
            logging.error("æ— æ³•è·å–LLMå®¢æˆ·ç«¯")
            return {}
        
        # æ„å»ºä¼˜åŒ–çš„promptï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜
        if pdf_content:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰6ä¸ªé—®é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}
PDFå†…å®¹: {pdf_content[:2000]}...  # é™åˆ¶PDFå†…å®¹é•¿åº¦

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”æ‰€æœ‰é—®é¢˜:
{{
    "q1_main_content": "è®ºæ–‡ä¸»è¦å†…å®¹æ€»ç»“",
    "q2_problem": "è®ºæ–‡è¯•å›¾è§£å†³çš„å…·ä½“é—®é¢˜",
    "q3_related_work": "ç›¸å…³ç ”ç©¶ï¼ˆç»“åˆPDF referenceç« èŠ‚ï¼Œç»™å‡ºå…·ä½“è®ºæ–‡æ ‡é¢˜ï¼‰",
    "q4_solution": "è®ºæ–‡çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•",
    "q5_experiments": "å®éªŒè®¾è®¡å’Œç»“è®º",
    "q6_future_work": "å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. æ¯ä¸ªç­”æ¡ˆè¦ç®€æ´ä½†å®Œæ•´
3. ç›¸å…³ç ”ç©¶è¦ç»“åˆPDFä¸­çš„å…·ä½“å¼•ç”¨
4. ä¸è¦æ·»åŠ åºå·ã€æ ‡é¢˜ç­‰é¢å¤–æ ¼å¼
"""
        else:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰6ä¸ªé—®é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”æ‰€æœ‰é—®é¢˜:
{{
    "q1_main_content": "è®ºæ–‡ä¸»è¦å†…å®¹æ€»ç»“",
    "q2_problem": "è®ºæ–‡è¯•å›¾è§£å†³çš„å…·ä½“é—®é¢˜",
    "q3_related_work": "ç›¸å…³ç ”ç©¶ï¼ˆåŸºäºæ‘˜è¦å†…å®¹åˆ†æï¼‰",
    "q4_solution": "è®ºæ–‡çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•",
    "q5_experiments": "å®éªŒè®¾è®¡å’Œç»“è®º",
    "q6_future_work": "å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. æ¯ä¸ªç­”æ¡ˆè¦ç®€æ´ä½†å®Œæ•´
3. ä¸è¦æ·»åŠ åºå·ã€æ ‡é¢˜ç­‰é¢å¤–æ ¼å¼
"""

        # æ ¹æ®æä¾›å•†æ„å»ºä¸åŒçš„APIè°ƒç”¨å‚æ•°
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†
            api_params = {
                "model": get_api_config_with_scenario("paper_analysis")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": get_api_config_with_scenario("paper_analysis").get("max_tokens", 32000)
                # æ³¨æ„ï¼šDeepSeek R1ä¸æ”¯æŒtemperatureã€top_pç­‰å‚æ•°
            }
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨æ ‡å‡†å‚æ•°
            api_params = {
                "model": get_api_config_with_scenario("paper_analysis")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": get_temperature_for_scenario("paper_analysis"),
                "max_tokens": get_api_config_with_scenario("paper_analysis").get("max_tokens", 4000)
            }

        # è°ƒç”¨LLM
        response = client.chat.completions.create(**api_params)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if hasattr(response, 'usage') and response.usage:
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            model_name = get_api_config_with_scenario("paper_analysis")["model"] if get_api_config_with_scenario("paper_analysis") else "unknown"
            token_tracker.add_usage(input_tokens, output_tokens, model_name)
        
        # è§£æå“åº”
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåŒæ—¶è·å–reasoning_contentå’Œcontent
            reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
            content = response.choices[0].message.content.strip()
            
            if reasoning_content:
                logging.info(f"DeepSeek R1æ¨ç†è¿‡ç¨‹: {reasoning_content[:200]}...")
        else:
            content = response.choices[0].message.content.strip()
        
        # æå–JSONéƒ¨åˆ†
        try:
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                
                # éªŒè¯æ‰€æœ‰é—®é¢˜éƒ½æœ‰ç­”æ¡ˆ
                required_keys = [
                    "q1_main_content", "q2_problem", "q3_related_work",
                    "q4_solution", "q5_experiments", "q6_future_work"
                ]
                
                for key in required_keys:
                    if key not in result or not result[key]:
                        result[key] = "æœªæä¾›ç­”æ¡ˆ"
                
                # å¦‚æœæ˜¯DeepSeek R1ï¼Œæ·»åŠ æ¨ç†è¿‡ç¨‹
                if provider == "deepseek" and reasoning_content:
                    result["reasoning_process"] = reasoning_content
                
                return result
            else:
                logging.warning("æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£ææ–‡æœ¬")
                return _parse_text_response(content)
                
        except json.JSONDecodeError as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•è§£ææ–‡æœ¬")
            return _parse_text_response(content)
            
    except Exception as e:
        logging.error(f"LLMåˆ†æè®ºæ–‡å¤±è´¥: {e}")
        return {}

def _parse_text_response(content: str) -> Dict:
    """ä»LLMçš„æ–‡æœ¬å“åº”ä¸­è§£æç­”æ¡ˆ"""
    try:
        # å°è¯•ä»æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ
        lines = content.split('\n')
        result = {}
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        current_question = None
        current_answer = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜
            if any(q in line.lower() for q in ["ä¸»è¦å†…å®¹", "é—®é¢˜", "ç›¸å…³ç ”ç©¶", "è§£å†³æ–¹æ¡ˆ", "å®éªŒ", "æ¢ç´¢"]):
                # ä¿å­˜ä¹‹å‰çš„ç­”æ¡ˆ
                if current_question and current_answer:
                    result[current_question] = " ".join(current_answer).strip()
                
                # å¼€å§‹æ–°é—®é¢˜
                if "ä¸»è¦å†…å®¹" in line:
                    current_question = "q1_main_content"
                elif "é—®é¢˜" in line:
                    current_question = "q2_problem"
                elif "ç›¸å…³ç ”ç©¶" in line:
                    current_question = "q3_related_work"
                elif "è§£å†³æ–¹æ¡ˆ" in line:
                    current_question = "q4_solution"
                elif "å®éªŒ" in line:
                    current_question = "q5_experiments"
                elif "æ¢ç´¢" in line:
                    current_question = "q6_future_work"
                
                current_answer = []
            else:
                # ç´¯ç§¯ç­”æ¡ˆå†…å®¹
                if current_question:
                    current_answer.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç­”æ¡ˆ
        if current_question and current_answer:
            result[current_question] = " ".join(current_answer).strip()
        
        # ç¡®ä¿æ‰€æœ‰é—®é¢˜éƒ½æœ‰ç­”æ¡ˆ
        required_keys = [
            "q1_main_content", "q2_problem", "q3_related_work",
            "q4_solution", "q5_experiments", "q6_future_work"
        ]
        
        for key in required_keys:
            if key not in result:
                result[key] = "æ— æ³•è§£æç­”æ¡ˆ"
        
        return result
        
    except Exception as e:
        logging.error(f"æ–‡æœ¬è§£æå¤±è´¥: {e}")
        return {
            "q1_main_content": "è§£æå¤±è´¥",
            "q2_problem": "è§£æå¤±è´¥",
            "q3_related_work": "è§£æå¤±è´¥",
            "q4_solution": "è§£æå¤±è´¥",
            "q5_experiments": "è§£æå¤±è´¥",
            "q6_future_work": "è§£æå¤±è´¥"
        }

# ==================== è®ºæ–‡ç›¸å…³æ€§åˆ†æ ====================
def analyze_paper_relevance(paper_title: str, paper_abstract: str, research_areas: Dict[str, str]) -> Dict:
    """
    ä½¿ç”¨LLMåˆ†æè®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
        paper_abstract: è®ºæ–‡æ‘˜è¦
        research_areas: ç ”ç©¶é¢†åŸŸå®šä¹‰
    
    Returns:
        Dict: åŒ…å«ç›¸å…³æ€§åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # æ ¹æ®é…ç½®é€‰æ‹©å®¢æˆ·ç«¯
        config = get_config()
        provider = config.get("llm", {}).get("provider", "deepseek")
        
        if provider == "deepseek":
            client = get_deepseek_client()
        elif provider == "kimi":
            client = get_kimi_client()
        elif provider == "openai":
            client = get_openai_client()
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return {}
        
        if not client:
            logging.error("æ— æ³•è·å–LLMå®¢æˆ·ç«¯")
            return {}
        
        # æ„å»ºç ”ç©¶é¢†åŸŸæè¿°
        areas_description = "\n".join([f"- {area}: {desc}" for area, desc in research_areas.items()])
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ä¸æˆ‘ä»¬å…³æ³¨çš„ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}

æˆ‘ä»¬å…³æ³¨çš„ç ”ç©¶é¢†åŸŸ:
{areas_description}

è¯·åˆ†æè¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸æˆ‘ä»¬çš„ç ”ç©¶é¢†åŸŸç›¸å…³ï¼Œå¹¶ç»™å‡ºç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-10åˆ†ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ï¼‰ã€‚

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡º:
{{
    "relevance_score": ç›¸å…³æ€§è¯„åˆ†(0-10),
    "relevance_reasoning": "ç›¸å…³æ€§åˆ†ææ¨ç†è¿‡ç¨‹",
    "best_match_area": "æœ€åŒ¹é…çš„ç ”ç©¶é¢†åŸŸ",
    "is_relevant": true/false,
    "summary": "è®ºæ–‡å†…å®¹ç®€è¦æ€»ç»“"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. ç›¸å…³æ€§è¯„åˆ†è¦å®¢è§‚å‡†ç¡®
3. æ¨ç†è¿‡ç¨‹è¦è¯¦ç»†è¯´æ˜åˆ¤æ–­ä¾æ®
4. å¦‚æœè®ºæ–‡æ¶‰åŠç¡¬ä»¶ã€èŠ¯ç‰‡è®¾è®¡ã€ç”µè·¯ç­‰éAIç®—æ³•å†…å®¹ï¼Œè¯·ç»™å‡ºè¾ƒä½è¯„åˆ†
"""
        
        # æ ¹æ®æä¾›å•†æ„å»ºä¸åŒçš„APIè°ƒç”¨å‚æ•°
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†
            api_params = {
                "model": get_api_config_with_scenario("paper_relevance")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­è®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": get_api_config_with_scenario("paper_relevance").get("max_tokens", 32000)
                # æ³¨æ„ï¼šDeepSeek R1ä¸æ”¯æŒtemperatureã€top_pç­‰å‚æ•°
            }
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨æ ‡å‡†å‚æ•°
            api_params = {
                "model": get_api_config_with_scenario("paper_relevance")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­è®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": get_temperature_for_scenario("paper_relevance"),
                "max_tokens": get_api_config_with_scenario("paper_relevance").get("max_tokens", 4000)
            }
        
        # è°ƒç”¨LLM
        response = client.chat.completions.create(**api_params)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if hasattr(response, 'usage') and response.usage:
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            model_name = get_api_config_with_scenario("paper_relevance")["model"] if get_api_config_with_scenario("paper_relevance") else "unknown"
            token_tracker.add_usage(input_tokens, output_tokens, model_name)
        
        # è§£æå“åº”
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåŒæ—¶è·å–reasoning_contentå’Œcontent
            reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
            content = response.choices[0].message.content.strip()
            
            if reasoning_content:
                logging.info(f"DeepSeek R1æ¨ç†è¿‡ç¨‹: {reasoning_content[:200]}...")
        else:
            content = response.choices[0].message.content.strip()
        
        # æå–JSONéƒ¨åˆ†
        try:
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_keys = ["relevance_score", "relevance_reasoning", "best_match_area", "is_relevant", "summary"]
                for key in required_keys:
                    if key not in result:
                        result[key] = "æœªæä¾›"
                
                # å¦‚æœæ˜¯DeepSeek R1ï¼Œæ·»åŠ æ¨ç†è¿‡ç¨‹
                if provider == "deepseek" and reasoning_content:
                    result["reasoning_process"] = reasoning_content
                
                return result
            else:
                logging.warning("æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£ææ–‡æœ¬")
                return _parse_relevance_text_response(content)
                
        except json.JSONDecodeError as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•è§£ææ–‡æœ¬")
            return _parse_relevance_text_response(content)
            
    except Exception as e:
        logging.error(f"LLMåˆ†æè®ºæ–‡ç›¸å…³æ€§å¤±è´¥: {e}")
        return {}

def _parse_relevance_text_response(content: str) -> Dict:
    """ä»LLMçš„æ–‡æœ¬å“åº”ä¸­æå–ç›¸å…³æ€§ä¿¡æ¯"""
    try:
        # å°è¯•æ‰¾åˆ°ç›¸å…³æ€§åˆ†æ•°
        import re
        score_match = re.search(r'ç›¸å…³æ€§.*?(\d+\.?\d*)', content)
        relevance_score = float(score_match.group(1)) if score_match else 0.5
        
        # å°è¯•æ‰¾åˆ°æœ€ä½³åŒ¹é…é¢†åŸŸ
        best_area = "æœªçŸ¥"
        for area in research_areas.keys():
            if area in content:
                best_area = area
                break
        
        # æå–æ¨ç†è¿‡ç¨‹
        reasoning = content.split('\n')[-1] if content else "æ— æ³•æå–æ¨ç†è¿‡ç¨‹"
        
        return {
            "relevance_score": relevance_score,
            "relevance_reasoning": reasoning,
            "best_match_area": best_area,
            "is_relevant": relevance_score > 5, # å‡è®¾åˆ†æ•°å¤§äº5è¡¨ç¤ºç›¸å…³
            "summary": content.split('\n')[-1] if content else "æ— æ³•æå–æ‘˜è¦"
        }
        
    except Exception as e:
        logging.warning(f"ä»æ–‡æœ¬æå–ç›¸å…³æ€§ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "relevance_score": 0.5,
            "relevance_reasoning": "æ— æ³•æå–ä¿¡æ¯",
            "best_match_area": "æœªçŸ¥",
            "is_relevant": False,
            "summary": "æ— æ³•æå–æ‘˜è¦"
        }

def load_paper_data():
    """åŠ è½½çˆ¬å–çš„è®ºæ–‡æ•°æ®"""
    papers = []
    
    # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ•°æ®æ–‡ä»¶ï¼ˆä»250821ç›®å½•ï¼‰
    pattern = os.path.join("250821", "*_papers_*.json")
    json_files = glob.glob(pattern)
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                file_papers = json.load(f)
                if isinstance(file_papers, list):
                    papers.extend(file_papers)
                    logging.info(f"ä» {json_file} åŠ è½½äº† {len(file_papers)} ç¯‡è®ºæ–‡")
        except Exception as e:
            logging.error(f"è¯»å–æ–‡ä»¶ {json_file} æ—¶å‡ºé”™: {e}")
    
    # å»é‡ï¼ˆåŸºäºè®ºæ–‡IDï¼‰
    unique_papers = {}
    for paper in papers:
        paper_id = paper.get('id')
        if paper_id and paper_id not in unique_papers:
            unique_papers[paper_id] = paper
    
    papers = list(unique_papers.values())
    logging.info(f"æ€»å…±åŠ è½½äº† {len(papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    return papers

def save_analysis_results(analysis_results, analysis_dir):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    # ä½¿ç”¨ä¼ å…¥çš„åˆ†æç›®å½•
    output_dir = analysis_dir
    
    # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
    all_results_file = os.path.join(output_dir, f"all_paper_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    try:
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        logging.info(f"æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°: {all_results_file}")
    except Exception as e:
        logging.error(f"ä¿å­˜åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
    
    # æŒ‰ç±»åˆ«åˆ†åˆ«ä¿å­˜
    papers_by_category = {}
    for result in analysis_results:
        # ä»åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­è·å–ç±»åˆ«
        category = result.get("matched_category", "å…¶ä»–")
        if category not in papers_by_category:
            papers_by_category[category] = []
        papers_by_category[category].append(result)
    
    for category, papers in papers_by_category.items():
        safe_category = category.replace('/', '_').replace('\\', '_')
        category_file = os.path.join(output_dir, f"{safe_category}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        try:
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
            logging.info(f"{category} ç±»åˆ«åˆ†æç»“æœå·²ä¿å­˜åˆ°: {category_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜ {category} ç±»åˆ«ç»“æœæ—¶å‡ºé”™: {e}")

def main_paper_analysis():
    """
    ä¸»è¦çš„è®ºæ–‡åˆ†æå‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘tokenæ¶ˆè€—
    """
    try:
        # åŠ è½½è®ºæ–‡æ•°æ®
        papers = load_paper_data()
        if not papers:
            logging.warning("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")
            return
        
        logging.info(f"å¼€å§‹åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        
        # è·å–å½“å‰æ—¥æœŸ
        date_str = datetime.now().strftime("%y%m%d")
        
        # åˆ›å»ºåˆ†æç»“æœç›®å½•
        analysis_dir = f"./{date_str}/paper_analysis"
        os.makedirs(analysis_dir, exist_ok=True)
        
        all_analysis_results = []
        
        for i, paper in enumerate(papers, 1):
            try:
                logging.info(f"åˆ†æè®ºæ–‡ {i}/{len(papers)}: {paper.get('title', 'Unknown')[:50]}...")
                
                # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†ææ–¹æ³•ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜
                analysis_result = analyze_paper_with_questions(
                    paper_title=paper.get('title', ''),
                    paper_abstract=paper.get('abstract', ''),
                    pdf_content=None  # æš‚æ—¶ä¸ä½¿ç”¨PDFå†…å®¹ï¼Œå‡å°‘tokenæ¶ˆè€—
                )
                
                if analysis_result:
                    # æ·»åŠ è®ºæ–‡åŸºæœ¬ä¿¡æ¯
                    analysis_result.update({
                        'paper_id': paper.get('id', ''),
                        'paper_title': paper.get('title', ''),
                        'paper_url': paper.get('url', ''),
                        'analysis_time': datetime.now().isoformat(),
                        'llm_provider': get_api_config_with_scenario("paper_analysis")["model"] if get_api_config_with_scenario("paper_analysis") else "unknown"
                    })
                    
                    all_analysis_results.append(analysis_result)
                    
                    # ä¿å­˜å•ç¯‡è®ºæ–‡çš„åˆ†æç»“æœ
                    paper_filename = f"paper_analysis_{paper.get('id', f'paper_{i}')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    paper_filepath = os.path.join(analysis_dir, paper_filename)
                    
                    with open(paper_filepath, 'w', encoding='utf-8') as f:
                        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                    
                    logging.info(f"è®ºæ–‡åˆ†æå®Œæˆ: {paper_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(2)
                else:
                    logging.warning(f"è®ºæ–‡ {i} åˆ†æå¤±è´¥")
                
            except Exception as e:
                logging.error(f"åˆ†æè®ºæ–‡ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        if all_analysis_results:
            all_results_filename = f"all_paper_analysis_{date_str}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            all_results_filepath = os.path.join(analysis_dir, all_results_filename)
            
            with open(all_results_filepath, 'w', encoding='utf-8') as f:
                json.dump(all_analysis_results, f, ensure_ascii=False, indent=2)
            
            logging.info(f"æ‰€æœ‰è®ºæ–‡åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {all_results_filepath}")
            logging.info(f"æˆåŠŸåˆ†æ {len(all_analysis_results)} ç¯‡è®ºæ–‡")
        else:
            logging.warning("æ²¡æœ‰æˆåŠŸåˆ†æçš„è®ºæ–‡")
        
        # æ˜¾ç¤ºå’Œä¿å­˜tokenä½¿ç”¨é‡ç»Ÿè®¡
        token_tracker.print_summary()
        token_usage_filename = os.path.join(analysis_dir, f"token_usage_{date_str}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        token_tracker.save_summary(token_usage_filename)
            
    except Exception as e:
        logging.error(f"è®ºæ–‡åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # å³ä½¿å‡ºé”™ä¹Ÿè¦æ˜¾ç¤ºtokenç»Ÿè®¡
        token_tracker.print_summary()
        raise

# ==================== Tokenç»Ÿè®¡ç›¸å…³å‡½æ•° ====================
def get_token_usage_summary() -> Dict:
    """è·å–å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦"""
    return token_tracker.get_summary()

def print_token_usage_summary():
    """æ‰“å°å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦"""
    token_tracker.print_summary()

def save_token_usage_summary(filename: str = None):
    """ä¿å­˜å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦åˆ°æ–‡ä»¶"""
    token_tracker.save_summary(filename)

def reset_token_tracker():
    """é‡ç½®tokenè·Ÿè¸ªå™¨"""
    global token_tracker
    token_tracker = TokenUsageTracker()
    logging.info("Tokenè·Ÿè¸ªå™¨å·²é‡ç½®")

def estimate_text_tokens(text: str, language: str = "auto") -> int:
    """
    ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    
    Args:
        text: è¦ä¼°ç®—çš„æ–‡æœ¬
        language: è¯­è¨€ç±»å‹ ("auto", "chinese", "english")
    
    Returns:
        int: ä¼°ç®—çš„tokenæ•°é‡
    """
    if not text:
        return 0
    
    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    if language == "auto":
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_chars = sum(1 for char in text if char.isascii() and char.isalpha())
        
        if chinese_chars > english_chars:
            language = "chinese"
        else:
            language = "english"
    
    # æ ¹æ®è¯­è¨€ä¼°ç®—tokenæ•°é‡
    if language == "chinese":
        # ä¸­æ–‡å­—ç¬¦ï¼š1ä¸ªå­—ç¬¦ â‰ˆ 0.6ä¸ªtoken
        return int(len(text) * 0.6)
    elif language == "english":
        # è‹±æ–‡å­—ç¬¦ï¼š1ä¸ªå­—ç¬¦ â‰ˆ 0.3ä¸ªtoken
        return int(len(text) * 0.3)
    else:
        # æ··åˆè¯­è¨€ï¼šå–å¹³å‡å€¼
        return int(len(text) * 0.45)

def analyze_prompt_tokens(prompt: str) -> Dict:
    """
    åˆ†æpromptçš„tokenä½¿ç”¨æƒ…å†µ
    
    Args:
        prompt: è¦åˆ†æçš„prompt
    
    Returns:
        Dict: åŒ…å«tokenåˆ†æä¿¡æ¯çš„å­—å…¸
    """
    total_chars = len(prompt)
    estimated_tokens = estimate_text_tokens(prompt)
    
    # åˆ†æä¸­è‹±æ–‡æ¯”ä¾‹
    chinese_chars = sum(1 for char in prompt if '\u4e00' <= char <= '\u9fff')
    english_chars = sum(1 for char in prompt if char.isascii() and char.isalpha())
    other_chars = total_chars - chinese_chars - english_chars
    
    return {
        "total_characters": total_chars,
        "chinese_characters": chinese_chars,
        "english_characters": english_chars,
        "other_characters": other_chars,
        "estimated_tokens": estimated_tokens,
        "chinese_ratio": chinese_chars / total_chars if total_chars > 0 else 0,
        "english_ratio": english_chars / total_chars if total_chars > 0 else 0
    }

# ==================== ä¸»æ‰§è¡Œæµç¨‹ ====================
if __name__ == '__main__':
    main_paper_analysis()