import os
import time
import json
import logging
import glob
import requests
import httpx
import base64
from datetime import datetime
from openai import OpenAI
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config_manager import get_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Tokenç»Ÿè®¡ç±»
class TokenUsageTracker:
    """Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_estimate = 0.0
        self.api_calls = 0
        self.start_time = datetime.now()
    
    def add_usage(self, input_tokens: int, output_tokens: int, model: str = "unknown"):
        """æ·»åŠ tokenä½¿ç”¨é‡"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.api_calls += 1
        
        # ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºå¸¸è§æ¨¡å‹çš„å®šä»·ï¼‰
        cost = self._estimate_cost(input_tokens, output_tokens, model)
        self.total_cost_estimate += cost
        
        logging.info(f"Tokenä½¿ç”¨é‡: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}, ä¼°ç®—æˆæœ¬=${cost:.4f}")
    
    def _estimate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        # åŸºäºå¸¸è§æ¨¡å‹çš„å®šä»·ï¼ˆæ¯1000 tokensï¼‰
        pricing = {
            "deepseek-reasoner": {"input": 0.0007, "output": 0.0014},  # DeepSeek R1
            "kimi-k2-0711-preview": {"input": 0.0007, "output": 0.0014},  # Kimi
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},  # OpenAI
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},  # OpenAI GPT-4
        }
        
        # è·å–æ¨¡å‹å®šä»·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        model_pricing = pricing.get(model, {"input": 0.001, "output": 0.002})
        
        input_cost = (input_tokens / 1000) * model_pricing["input"]
        output_cost = (output_tokens / 1000) * model_pricing["output"]
        
        return input_cost + output_cost
    
    def get_summary(self) -> Dict:
        """è·å–ä½¿ç”¨é‡æ‘˜è¦"""
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "api_calls": self.api_calls,
            "total_cost_estimate": self.total_cost_estimate,
            "duration_seconds": duration,
            "start_time": self.start_time.isoformat(),
            "end_time": end_time.isoformat()
        }
    
    def print_summary(self):
        """æ‰“å°ä½¿ç”¨é‡æ‘˜è¦"""
        summary = self.get_summary()
        
        print("=" * 60)
        print("ğŸ“Š Tokenä½¿ç”¨é‡ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è¾“å…¥Token: {summary['total_input_tokens']:,}")
        print(f"æ€»è¾“å‡ºToken: {summary['total_output_tokens']:,}")
        print(f"æ€»Token: {summary['total_tokens']:,}")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {summary['api_calls']}")
        print(f"ä¼°ç®—æ€»æˆæœ¬: ${summary['total_cost_estimate']:.4f}")
        print(f"è¿è¡Œæ—¶é•¿: {summary['duration_seconds']:.1f} ç§’")
        print(f"å¼€å§‹æ—¶é—´: {summary['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {summary['end_time']}")
        print("=" * 60)
    
    def save_summary(self, filename: str = None):
        """ä¿å­˜ä½¿ç”¨é‡æ‘˜è¦åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"token_usage_summary_{timestamp}.json"
        
        summary = self.get_summary()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            logging.info(f"Tokenä½¿ç”¨é‡æ‘˜è¦å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logging.error(f"ä¿å­˜Tokenä½¿ç”¨é‡æ‘˜è¦å¤±è´¥: {e}")
    
    def reset_tracker(self):
        """é‡ç½®è·Ÿè¸ªå™¨"""
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_estimate = 0.0
        self.api_calls = 0
        self.start_time = datetime.now()
        logging.info("Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨å·²é‡ç½®")

# å…¨å±€tokenè·Ÿè¸ªå™¨
token_tracker = TokenUsageTracker()

# ==================== Configuration ====================
# ä»é…ç½®æ–‡ä»¶è¯»å–APIé…ç½®ï¼Œä¸å†ç¡¬ç¼–ç 
config = get_config()

# è®ºæ–‡è§£è¯»ç›¸å…³çš„é…ç½®
PAPER_DATA_DIR = './'  # æ ¹ç›®å½•ï¼Œç”¨äºåˆ›å»ºæ—¥æœŸå­ç›®å½•

# è®ºæ–‡è§£è¯»çš„æç¤ºæ¨¡æ¿
ABSTRACT_TRANSLATION_PROMPT = '''
è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒå­¦æœ¯æ€§å’Œå‡†ç¡®æ€§ï¼š

{abstract}

è¯·ç›´æ¥è¾“å‡ºä¸­æ–‡ç¿»è¯‘ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚
'''

PAPER_ANALYSIS_PROMPT = '''
è¯·åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ‘˜è¦ï¼š{abstract}
å­¦ç§‘åˆ†ç±»ï¼š{subjects}
urlï¼š{url}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚
'''

# è®ºæ–‡è§£è¯»çš„é—®é¢˜åˆ—è¡¨
ANALYSIS_QUESTIONS = [
    "æ€»ç»“ä¸€ä¸‹è®ºæ–‡çš„ä¸»è¦å†…å®¹",
    "è¿™ç¯‡è®ºæ–‡è¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "æœ‰å“ªäº›ç›¸å…³ç ”ç©¶ï¼Ÿå¼•ç”¨ä¸èƒ½åªç»™å‡ºåºå·ï¼Œéœ€è¦ç»“åˆpdf referenceç« èŠ‚ç»™å‡ºç›¸å…³ç ”ç©¶çš„è®ºæ–‡æ ‡é¢˜ã€‚",
    "è®ºæ–‡å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ",
    "è®ºæ–‡åšäº†å“ªäº›å®éªŒï¼Ÿå®éªŒç»“è®ºå¦‚ä½•ï¼Ÿ",
    "æœ‰ä»€ä¹ˆå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„ç‚¹ï¼Ÿ"
]

# è®¾ç½®æ—¥å¿—è®°å½•
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ==================== è·å–APIé…ç½®çš„å‡½æ•° ====================
def get_api_config():
    """ä»é…ç½®æ–‡ä»¶è·å–APIé…ç½®"""
    try:
        llm_config = config.get("llm", {})
        provider = llm_config.get("provider", "kimi")
        
        if provider == "kimi":
            kimi_config = llm_config.get("kimi", {})
            return {
                "api_key": kimi_config.get("api_key"),
                "base_url": kimi_config.get("base_url", "https://api.moonshot.cn/v1"),
                "model": kimi_config.get("model", "kimi-k2-0711-preview"),
                "temperature": kimi_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "openai":
            openai_config = llm_config.get("openai", {})
            return {
                "api_key": openai_config.get("api_key"),
                "base_url": openai_config.get("base_url", "https://api.openai.com/v1"),
                "model": openai_config.get("model", "gpt-4-turbo"),
                "temperature": openai_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "deepseek":
            deepseek_config = llm_config.get("deepseek", {})
            return {
                "api_key": deepseek_config.get("api_key"),
                "base_url": deepseek_config.get("base_url", "https://api.deepseek.com/v1"),
                "model": deepseek_config.get("model", "deepseek-chat"),
                "temperature": deepseek_config.get("temperature", 0.3),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return None
            
    except Exception as e:
        logging.error(f"è·å–APIé…ç½®å¤±è´¥: {e}")
        return None

def get_kimi_client():
    """è·å–Kimiå®¢æˆ·ç«¯"""
    try:
        api_config = get_api_config()
        if not api_config or not api_config.get("api_key"):
            logging.error("æœªæ‰¾åˆ°æœ‰æ•ˆçš„APIé…ç½®")
            return None
            
        return OpenAI(
            api_key=api_config["api_key"],
            base_url=api_config["base_url"]
        )
    except Exception as e:
        logging.error(f"åˆ›å»ºKimiå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def get_deepseek_client():
    """è·å–DeepSeekå®¢æˆ·ç«¯"""
    try:
        config = get_config()
        deepseek_config = config.get("llm", {}).get("deepseek", {})
        
        if not deepseek_config.get("api_key"):
            logging.error("DeepSeek APIå¯†é’¥æœªé…ç½®")
            return None
        
        # DeepSeek R1ä½¿ç”¨ä¸åŒçš„base_urlæ ¼å¼
        base_url = deepseek_config.get("base_url", "https://api.deepseek.com")
        if not base_url.endswith("/v1"):
            base_url = f"{base_url}/v1"
        
        client = OpenAI(
            api_key=deepseek_config["api_key"],
            base_url=base_url
        )
        
        logging.info("DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        return client
        
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def get_temperature_for_scenario(scenario: str) -> float:
    """
    æ ¹æ®ä½¿ç”¨åœºæ™¯è·å–åˆé€‚çš„temperatureå€¼
    
    Args:
        scenario: ä½¿ç”¨åœºæ™¯
        
    Returns:
        float: åˆé€‚çš„temperatureå€¼
    """
    config = get_config()
    temperature_config = config.get("llm", {}).get("temperature_by_scenario", {})
    
    # åœºæ™¯åŒ–temperatureé…ç½®
    scenario_temperatures = {
        "paper_relevance": 0.1,      # è®ºæ–‡ç›¸å…³æ€§åˆ†æ - éœ€è¦ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
        "paper_analysis": 0.3,       # è®ºæ–‡å†…å®¹åˆ†æ - éœ€è¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
        "report_generation": 0.7,    # æŠ¥å‘Šç”Ÿæˆ - éœ€è¦ä¸€å®šçš„åˆ›é€ æ€§ä½†ä¿æŒå‡†ç¡®æ€§
        "general_conversation": 1.3, # é€šç”¨å¯¹è¯ - éœ€è¦çµæ´»æ€§å’Œåˆ›é€ æ€§
        "creative_writing": 1.5,     # åˆ›æ„å†™ä½œ - éœ€è¦é«˜åˆ›é€ æ€§
        "code_generation": 0.0,      # ä»£ç ç”Ÿæˆ - éœ€è¦ç²¾ç¡®æ€§
        "data_extraction": 1.0,      # æ•°æ®æŠ½å– - éœ€è¦å‡†ç¡®æ€§
        "translation": 1.3,          # ç¿»è¯‘ä»»åŠ¡ - éœ€è¦çµæ´»æ€§
    }
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    temperature = temperature_config.get(scenario, scenario_temperatures.get(scenario, 1.0))
    
    logging.info(f"åœºæ™¯ '{scenario}' ä½¿ç”¨ temperature: {temperature}")
    return temperature

def get_api_config_with_scenario(scenario: str = "general"):
    """
    æ ¹æ®åœºæ™¯è·å–APIé…ç½®ï¼ŒåŒ…æ‹¬åˆé€‚çš„temperature
    
    Args:
        scenario: ä½¿ç”¨åœºæ™¯
        
    Returns:
        Dict: APIé…ç½®å­—å…¸
    """
    try:
        llm_config = config.get("llm", {})
        provider = llm_config.get("provider", "kimi")
        
        if provider == "kimi":
            kimi_config = llm_config.get("kimi", {})
            return {
                "api_key": kimi_config.get("api_key"),
                "base_url": kimi_config.get("base_url", "https://api.moonshot.cn/v1"),
                "model": kimi_config.get("model", "kimi-k2-0711-preview"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": kimi_config.get("max_tokens", 4000)
            }
        elif provider == "openai":
            openai_config = llm_config.get("openai", {})
            return {
                "api_key": openai_config.get("api_key"),
                "base_url": openai_config.get("base_url", "https://api.openai.com/v1"),
                "model": openai_config.get("model", "gpt-4-turbo"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": openai_config.get("max_tokens", 4000)
            }
        elif provider == "deepseek":
            deepseek_config = llm_config.get("deepseek", {})
            return {
                "api_key": deepseek_config.get("api_key"),
                "base_url": deepseek_config.get("base_url", "https://api.deepseek.com/v1"),
                "model": deepseek_config.get("model", "deepseek-chat"),
                "temperature": get_temperature_for_scenario(scenario),
                "max_tokens": deepseek_config.get("max_tokens", 4000)
            }
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return None
    except Exception as e:
        logging.error(f"è·å–APIé…ç½®å¤±è´¥: {e}")
        return None

# ==================== è®ºæ–‡è§£è¯»ç›¸å…³å‡½æ•° ====================
def analyze_paper_with_questions(paper_title: str, paper_abstract: str, paper_url: str = None, paper_id: str = None, save_results: bool = True) -> Dict:
    """
    ä½¿ç”¨LLMåˆ†æè®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜ï¼Œå‡å°‘tokenæ¶ˆè€—
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
        paper_abstract: è®ºæ–‡æ‘˜è¦
        paper_url: è®ºæ–‡URLï¼ˆç”¨äºä¸‹è½½PDFï¼‰
        paper_id: è®ºæ–‡IDï¼ˆç”¨äºä¿å­˜ç»“æœï¼‰
        save_results: æ˜¯å¦ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    
    Returns:
        Dict: åŒ…å«æ‰€æœ‰é—®é¢˜ç­”æ¡ˆçš„å­—å…¸
    """
    try:
        # æ ¹æ®é…ç½®é€‰æ‹©å®¢æˆ·ç«¯
        config = get_config()
        provider = config.get("llm", {}).get("provider", "deepseek")
        
        if provider == "deepseek":
            client = get_deepseek_client()
        elif provider == "kimi":
            client = get_kimi_client()
        elif provider == "openai":
            client = get_openai_client()
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return {}
        
        if not client:
            logging.error("æ— æ³•è·å–LLMå®¢æˆ·ç«¯")
            return {}
        
        # å°è¯•ä¸‹è½½PDFæ–‡ä»¶
        pdf_path = None
        pdf_content = None
        
        if paper_url and paper_id:
            try:
                pdf_path = download_pdf(paper_url, paper_id)
                if pdf_path:
                    logging.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_path}")
                    # å¯¹äºDeepSeekï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
                    if provider == "deepseek":
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœå¤ªå¤§åˆ™ä½¿ç”¨base64ç¼–ç 
                        file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)
                        if file_size_mb > 20:  # å¦‚æœPDFå¤§äº20MBï¼Œä½¿ç”¨base64ç¼–ç 
                            pdf_content = encode_pdf_to_base64(pdf_path)
                            logging.info(f"PDFæ–‡ä»¶è¾ƒå¤§ï¼Œä½¿ç”¨base64ç¼–ç  (å¤§å°: {file_size_mb:.1f}MB)")
                        else:
                            logging.info(f"PDFæ–‡ä»¶å¤§å°é€‚ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ (å¤§å°: {file_size_mb:.1f}MB)")
                else:
                    logging.warning("PDFä¸‹è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨æ‘˜è¦è¿›è¡Œåˆ†æ")
            except Exception as e:
                logging.warning(f"PDFå¤„ç†å¤±è´¥: {e}ï¼Œå°†ä»…ä½¿ç”¨æ‘˜è¦è¿›è¡Œåˆ†æ")
        
        # æ„å»ºä¼˜åŒ–çš„promptï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜
        if pdf_path and pdf_content:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰6ä¸ªé—®é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}
PDFå†…å®¹: [å·²æä¾›PDFæ–‡ä»¶ï¼Œè¯·ä»”ç»†é˜…è¯»å…¨æ–‡å†…å®¹]

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”æ‰€æœ‰é—®é¢˜:
{{
    "q1_main_content": "è®ºæ–‡ä¸»è¦å†…å®¹æ€»ç»“",
    "q2_problem": "è®ºæ–‡è¯•å›¾è§£å†³çš„å…·ä½“é—®é¢˜",
    "q3_related_work": "ç›¸å…³ç ”ç©¶ï¼ˆç»“åˆPDF referenceç« èŠ‚ï¼Œç»™å‡ºå…·ä½“è®ºæ–‡æ ‡é¢˜ï¼‰",
    "q4_solution": "è®ºæ–‡çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•",
    "q5_experiments": "å®éªŒè®¾è®¡å’Œç»“è®º",
    "q6_future_work": "å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. æ¯ä¸ªç­”æ¡ˆè¦ç®€æ´ä½†å®Œæ•´
3. ç›¸å…³ç ”ç©¶è¦ç»“åˆPDFä¸­çš„å…·ä½“å¼•ç”¨
4. ä¸è¦æ·»åŠ åºå·ã€æ ‡é¢˜ç­‰é¢å¤–æ ¼å¼
"""
        elif pdf_path:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰6ä¸ªé—®é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}
PDFæ–‡ä»¶: [å·²æä¾›PDFæ–‡ä»¶ï¼Œè¯·ä»”ç»†é˜…è¯»å…¨æ–‡å†…å®¹]

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”æ‰€æœ‰é—®é¢˜:
{{
    "q1_main_content": "è®ºæ–‡ä¸»è¦å†…å®¹æ€»ç»“",
    "q2_problem": "è®ºæ–‡è¯•å›¾è§£å†³çš„å…·ä½“é—®é¢˜",
    "q3_related_work": "ç›¸å…³ç ”ç©¶ï¼ˆç»“åˆPDF referenceç« èŠ‚ï¼Œç»™å‡ºå…·ä½“è®ºæ–‡æ ‡é¢˜ï¼‰",
    "q4_solution": "è®ºæ–‡çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•",
    "q5_experiments": "å®éªŒè®¾è®¡å’Œç»“è®º",
    "q6_future_work": "å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. æ¯ä¸ªç­”æ¡ˆè¦ç®€æ´ä½†å®Œæ•´
3. ç›¸å…³ç ”ç©¶è¦ç»“åˆPDFä¸­çš„å…·ä½“å¼•ç”¨
4. ä¸è¦æ·»åŠ åºå·ã€æ ‡é¢˜ç­‰é¢å¤–æ ¼å¼
"""
        else:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰6ä¸ªé—®é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›ç­”æ‰€æœ‰é—®é¢˜:
{{
    "q1_main_content": "è®ºæ–‡ä¸»è¦å†…å®¹æ€»ç»“",
    "q2_problem": "è®ºæ–‡è¯•å›¾è§£å†³çš„å…·ä½“é—®é¢˜",
    "q3_related_work": "ç›¸å…³ç ”ç©¶ï¼ˆåŸºäºæ‘˜è¦å†…å®¹åˆ†æï¼‰",
    "q4_solution": "è®ºæ–‡çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•",
    "q5_experiments": "å®éªŒè®¾è®¡å’Œç»“è®º",
    "q6_future_work": "å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. æ¯ä¸ªç­”æ¡ˆè¦ç®€æ´ä½†å®Œæ•´
3. ä¸è¦æ·»åŠ åºå·ã€æ ‡é¢˜ç­‰é¢å¤–æ ¼å¼
"""

        # æ ¹æ®æä¾›å•†æ„å»ºä¸åŒçš„APIè°ƒç”¨å‚æ•°
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            # å¦‚æœæœ‰PDFæ–‡ä»¶ï¼Œå°è¯•æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if pdf_path:
                try:
                    # å¯¹äºDeepSeekï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ 
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®DeepSeekçš„å…·ä½“APIæ–‡æ¡£æ¥è°ƒæ•´
                    if pdf_content:
                        # å¦‚æœPDFå¤ªå¤§ï¼Œåœ¨promptä¸­è¯´æ˜
                        messages[1]["content"] += f"\n\næ³¨æ„ï¼šç”±äºPDFæ–‡ä»¶è¾ƒå¤§ï¼Œè¯·åŸºäºæ‘˜è¦å’Œæ ‡é¢˜è¿›è¡Œåˆ†æã€‚"
                    else:
                        # å¦‚æœPDFé€‚ä¸­ï¼Œå¯ä»¥å°è¯•ç›´æ¥ä½¿ç”¨
                        messages[1]["content"] += f"\n\næ³¨æ„ï¼šè¯·åŸºäºæä¾›çš„PDFæ–‡ä»¶å†…å®¹è¿›è¡Œåˆ†æã€‚"
                except Exception as e:
                    logging.warning(f"PDFæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            
            api_params = {
                "model": get_api_config_with_scenario("paper_analysis")["model"],
                "messages": messages,
                "max_tokens": get_api_config_with_scenario("paper_analysis").get("max_tokens", 32000)
                # æ³¨æ„ï¼šDeepSeek R1ä¸æ”¯æŒtemperatureã€top_pç­‰å‚æ•°
            }
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨æ ‡å‡†å‚æ•°
            api_params = {
                "model": get_api_config_with_scenario("paper_analysis")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": get_temperature_for_scenario("paper_analysis"),
                "max_tokens": get_api_config_with_scenario("paper_analysis").get("max_tokens", 4000)
            }

        # è°ƒç”¨LLM
        response = client.chat.completions.create(**api_params)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if hasattr(response, 'usage') and response.usage:
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            model_name = get_api_config_with_scenario("paper_analysis")["model"] if get_api_config_with_scenario("paper_analysis") else "unknown"
            token_tracker.add_usage(input_tokens, output_tokens, model_name)
        
        # è§£æå“åº”
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåŒæ—¶è·å–reasoning_contentå’Œcontent
            reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
            content = response.choices[0].message.content.strip()
            
            if reasoning_content:
                logging.info(f"DeepSeek R1æ¨ç†è¿‡ç¨‹: {reasoning_content[:200]}...")
        else:
            content = response.choices[0].message.content.strip()
        
        # æå–JSONéƒ¨åˆ†
        try:
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                
                # éªŒè¯æ‰€æœ‰é—®é¢˜éƒ½æœ‰ç­”æ¡ˆ
                required_keys = [
                    "q1_main_content", "q2_problem", "q3_related_work",
                    "q4_solution", "q5_experiments", "q6_future_work"
                ]
                
                for key in required_keys:
                    if key not in result or not result[key]:
                        result[key] = "æœªæä¾›ç­”æ¡ˆ"
                
                # æ·»åŠ è®ºæ–‡åŸºæœ¬ä¿¡æ¯
                result.update({
                    'paper_title': paper_title,
                    'paper_abstract': paper_abstract,
                    'paper_url': paper_url,
                    'paper_id': paper_id,
                    'analysis_time': datetime.now().isoformat(),
                    'llm_provider': provider,
                    'pdf_used': pdf_path is not None
                })
                
                # å¦‚æœæ˜¯DeepSeek R1ï¼Œæ·»åŠ æ¨ç†è¿‡ç¨‹
                if provider == "deepseek" and reasoning_content:
                    result["reasoning_process"] = reasoning_content
                
                # ä¿å­˜åˆ†æç»“æœ
                if save_results and paper_id:
                    save_analysis_result(result, paper_id)
                
                return result
            else:
                logging.warning("æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£ææ–‡æœ¬")
                return _parse_text_response(content)
                
        except json.JSONDecodeError as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•è§£ææ–‡æœ¬")
            return _parse_text_response(content)
            
    except Exception as e:
        logging.error(f"LLMåˆ†æè®ºæ–‡å¤±è´¥: {e}")
        return {}

def _parse_text_response(content: str) -> Dict:
    """ä»LLMçš„æ–‡æœ¬å“åº”ä¸­è§£æç­”æ¡ˆ"""
    try:
        # å°è¯•ä»æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ
        lines = content.split('\n')
        result = {}
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        current_question = None
        current_answer = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜
            if any(q in line.lower() for q in ["ä¸»è¦å†…å®¹", "é—®é¢˜", "ç›¸å…³ç ”ç©¶", "è§£å†³æ–¹æ¡ˆ", "å®éªŒ", "æ¢ç´¢"]):
                # ä¿å­˜ä¹‹å‰çš„ç­”æ¡ˆ
                if current_question and current_answer:
                    result[current_question] = " ".join(current_answer).strip()
                
                # å¼€å§‹æ–°é—®é¢˜
                if "ä¸»è¦å†…å®¹" in line:
                    current_question = "q1_main_content"
                elif "é—®é¢˜" in line:
                    current_question = "q2_problem"
                elif "ç›¸å…³ç ”ç©¶" in line:
                    current_question = "q3_related_work"
                elif "è§£å†³æ–¹æ¡ˆ" in line:
                    current_question = "q4_solution"
                elif "å®éªŒ" in line:
                    current_question = "q5_experiments"
                elif "æ¢ç´¢" in line:
                    current_question = "q6_future_work"
                
                current_answer = []
            else:
                # ç´¯ç§¯ç­”æ¡ˆå†…å®¹
                if current_question:
                    current_answer.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç­”æ¡ˆ
        if current_question and current_answer:
            result[current_question] = " ".join(current_answer).strip()
        
        # ç¡®ä¿æ‰€æœ‰é—®é¢˜éƒ½æœ‰ç­”æ¡ˆ
        required_keys = [
            "q1_main_content", "q2_problem", "q3_related_work",
            "q4_solution", "q5_experiments", "q6_future_work"
        ]
        
        for key in required_keys:
            if key not in result:
                result[key] = "æ— æ³•è§£æç­”æ¡ˆ"
        
        return result
        
    except Exception as e:
        logging.error(f"æ–‡æœ¬è§£æå¤±è´¥: {e}")
        return {
            "q1_main_content": "è§£æå¤±è´¥",
            "q2_problem": "è§£æå¤±è´¥",
            "q3_related_work": "è§£æå¤±è´¥",
            "q4_solution": "è§£æå¤±è´¥",
            "q5_experiments": "è§£æå¤±è´¥",
            "q6_future_work": "è§£æå¤±è´¥"
        }

# ==================== è®ºæ–‡ç›¸å…³æ€§åˆ†æ ====================
def analyze_paper_relevance(paper_title: str, paper_abstract: str, research_areas: Dict[str, str]) -> Dict:
    """
    ä½¿ç”¨LLMåˆ†æè®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§
    
    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜
        paper_abstract: è®ºæ–‡æ‘˜è¦
        research_areas: ç ”ç©¶é¢†åŸŸå®šä¹‰
    
    Returns:
        Dict: åŒ…å«ç›¸å…³æ€§åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # æ ¹æ®é…ç½®é€‰æ‹©å®¢æˆ·ç«¯
        config = get_config()
        provider = config.get("llm", {}).get("provider", "deepseek")
        
        if provider == "deepseek":
            client = get_deepseek_client()
        elif provider == "kimi":
            client = get_kimi_client()
        elif provider == "openai":
            client = get_openai_client()
        else:
            logging.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return {}
        
        if not client:
            logging.error("æ— æ³•è·å–LLMå®¢æˆ·ç«¯")
            return {}
        
        # æ„å»ºç ”ç©¶é¢†åŸŸæè¿°
        areas_description = "\n".join([f"- {area}: {desc}" for area, desc in research_areas.items()])
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ä¸æˆ‘ä»¬å…³æ³¨çš„ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}
è®ºæ–‡æ‘˜è¦: {paper_abstract}

æˆ‘ä»¬å…³æ³¨çš„ç ”ç©¶é¢†åŸŸ:
{areas_description}

è¯·åˆ†æè¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸æˆ‘ä»¬çš„ç ”ç©¶é¢†åŸŸç›¸å…³ï¼Œå¹¶ç»™å‡ºç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-10åˆ†ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ï¼‰ã€‚

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡º:
{{
    "relevance_score": ç›¸å…³æ€§è¯„åˆ†(0-10),
    "relevance_reasoning": "ç›¸å…³æ€§åˆ†ææ¨ç†è¿‡ç¨‹",
    "best_match_area": "æœ€åŒ¹é…çš„ç ”ç©¶é¢†åŸŸ",
    "is_relevant": true/false,
    "summary": "è®ºæ–‡å†…å®¹ç®€è¦æ€»ç»“"
}}

æ³¨æ„:
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º
2. ç›¸å…³æ€§è¯„åˆ†è¦å®¢è§‚å‡†ç¡®
3. æ¨ç†è¿‡ç¨‹è¦è¯¦ç»†è¯´æ˜åˆ¤æ–­ä¾æ®
4. å¦‚æœè®ºæ–‡æ¶‰åŠç¡¬ä»¶ã€èŠ¯ç‰‡è®¾è®¡ã€ç”µè·¯ç­‰éAIç®—æ³•å†…å®¹ï¼Œè¯·ç»™å‡ºè¾ƒä½è¯„åˆ†
"""
        
        # æ ¹æ®æä¾›å•†æ„å»ºä¸åŒçš„APIè°ƒç”¨å‚æ•°
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†
            api_params = {
                "model": get_api_config_with_scenario("paper_relevance")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­è®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": get_api_config_with_scenario("paper_relevance").get("max_tokens", 32000)
                # æ³¨æ„ï¼šDeepSeek R1ä¸æ”¯æŒtemperatureã€top_pç­‰å‚æ•°
            }
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨æ ‡å‡†å‚æ•°
            api_params = {
                "model": get_api_config_with_scenario("paper_relevance")["model"],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ¤æ–­è®ºæ–‡ä¸ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": get_temperature_for_scenario("paper_relevance"),
                "max_tokens": get_api_config_with_scenario("paper_relevance").get("max_tokens", 4000)
            }
        
        # è°ƒç”¨LLM
        response = client.chat.completions.create(**api_params)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if hasattr(response, 'usage') and response.usage:
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            model_name = get_api_config_with_scenario("paper_relevance")["model"] if get_api_config_with_scenario("paper_relevance") else "unknown"
            token_tracker.add_usage(input_tokens, output_tokens, model_name)
        
        # è§£æå“åº”
        if provider == "deepseek":
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåŒæ—¶è·å–reasoning_contentå’Œcontent
            reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
            content = response.choices[0].message.content.strip()
            
            if reasoning_content:
                logging.info(f"DeepSeek R1æ¨ç†è¿‡ç¨‹: {reasoning_content[:200]}...")
        else:
            content = response.choices[0].message.content.strip()
        
        # æå–JSONéƒ¨åˆ†
        try:
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_keys = ["relevance_score", "relevance_reasoning", "best_match_area", "is_relevant", "summary"]
                for key in required_keys:
                    if key not in result:
                        result[key] = "æœªæä¾›"
                
                # å¦‚æœæ˜¯DeepSeek R1ï¼Œæ·»åŠ æ¨ç†è¿‡ç¨‹
                if provider == "deepseek" and reasoning_content:
                    result["reasoning_process"] = reasoning_content
                
                return result
            else:
                logging.warning("æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•è§£ææ–‡æœ¬")
                return _parse_relevance_text_response(content)
                
        except json.JSONDecodeError as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•è§£ææ–‡æœ¬")
            return _parse_relevance_text_response(content)
            
    except Exception as e:
        logging.error(f"LLMåˆ†æè®ºæ–‡ç›¸å…³æ€§å¤±è´¥: {e}")
        return {}

def _parse_relevance_text_response(content: str) -> Dict:
    """ä»LLMçš„æ–‡æœ¬å“åº”ä¸­æå–ç›¸å…³æ€§ä¿¡æ¯"""
    try:
        # å°è¯•æ‰¾åˆ°ç›¸å…³æ€§åˆ†æ•°
        import re
        score_match = re.search(r'ç›¸å…³æ€§.*?(\d+\.?\d*)', content)
        relevance_score = float(score_match.group(1)) if score_match else 0.5
        
        # å°è¯•æ‰¾åˆ°æœ€ä½³åŒ¹é…é¢†åŸŸ
        best_area = "æœªçŸ¥"
        for area in research_areas.keys():
            if area in content:
                best_area = area
                break
        
        # æå–æ¨ç†è¿‡ç¨‹
        reasoning = content.split('\n')[-1] if content else "æ— æ³•æå–æ¨ç†è¿‡ç¨‹"
        
        return {
            "relevance_score": relevance_score,
            "relevance_reasoning": reasoning,
            "best_match_area": best_area,
            "is_relevant": relevance_score > 5, # å‡è®¾åˆ†æ•°å¤§äº5è¡¨ç¤ºç›¸å…³
            "summary": content.split('\n')[-1] if content else "æ— æ³•æå–æ‘˜è¦"
        }
        
    except Exception as e:
        logging.warning(f"ä»æ–‡æœ¬æå–ç›¸å…³æ€§ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "relevance_score": 0.5,
            "relevance_reasoning": "æ— æ³•æå–ä¿¡æ¯",
            "best_match_area": "æœªçŸ¥",
            "is_relevant": False,
            "summary": "æ— æ³•æå–æ‘˜è¦"
        }

def load_paper_data():
    """åŠ è½½çˆ¬å–çš„è®ºæ–‡æ•°æ®"""
    papers = []
    
    # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ•°æ®æ–‡ä»¶ï¼ˆä»250821ç›®å½•ï¼‰
    pattern = os.path.join("250821", "*_papers_*.json")
    json_files = glob.glob(pattern)
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                file_papers = json.load(f)
                if isinstance(file_papers, list):
                    papers.extend(file_papers)
                    logging.info(f"ä» {json_file} åŠ è½½äº† {len(file_papers)} ç¯‡è®ºæ–‡")
        except Exception as e:
            logging.error(f"è¯»å–æ–‡ä»¶ {json_file} æ—¶å‡ºé”™: {e}")
    
    # å»é‡ï¼ˆåŸºäºè®ºæ–‡IDï¼‰
    unique_papers = {}
    for paper in papers:
        paper_id = paper.get('id')
        if paper_id and paper_id not in unique_papers:
            unique_papers[paper_id] = paper
    
    papers = list(unique_papers.values())
    logging.info(f"æ€»å…±åŠ è½½äº† {len(papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    return papers

def save_analysis_results(analysis_results, analysis_dir):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    # ä½¿ç”¨ä¼ å…¥çš„åˆ†æç›®å½•
    output_dir = analysis_dir
    
    # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
    all_results_file = os.path.join(output_dir, f"all_paper_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    try:
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        logging.info(f"æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°: {all_results_file}")
    except Exception as e:
        logging.error(f"ä¿å­˜åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
    
    # æŒ‰ç±»åˆ«åˆ†åˆ«ä¿å­˜
    papers_by_category = {}
    for result in analysis_results:
        # ä»åŸå§‹è®ºæ–‡ä¿¡æ¯ä¸­è·å–ç±»åˆ«
        category = result.get("matched_category", "å…¶ä»–")
        if category not in papers_by_category:
            papers_by_category[category] = []
        papers_by_category[category].append(result)
    
    for category, papers in papers_by_category.items():
        safe_category = category.replace('/', '_').replace('\\', '_')
        category_file = os.path.join(output_dir, f"{safe_category}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        try:
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
            logging.info(f"{category} ç±»åˆ«åˆ†æç»“æœå·²ä¿å­˜åˆ°: {category_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜ {category} ç±»åˆ«ç»“æœæ—¶å‡ºé”™: {e}")

def save_analysis_result(result: Dict, paper_id: str):
    """
    ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    
    Args:
        result: åˆ†æç»“æœå­—å…¸
        paper_id: è®ºæ–‡ID
    """
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•
        today = datetime.now().strftime("%y%m%d")
        save_dir = os.path.join(PAPER_DATA_DIR, today, "analysis_results")
        os.makedirs(save_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%H%M%S")
        filename = f"analysis_{paper_id}_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)
        
        # ä¿å­˜ç»“æœ
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logging.info(f"åˆ†æç»“æœå·²ä¿å­˜: {filepath}")
        
        # åŒæ—¶ä¿å­˜åˆ°æ±‡æ€»æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        summary_file = os.path.join(save_dir, f"analysis_summary_{today}.json")
        save_to_summary_file(result, summary_file)
        
    except Exception as e:
        logging.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

def save_to_summary_file(result: Dict, summary_file: str):
    """
    å°†åˆ†æç»“æœè¿½åŠ åˆ°æ±‡æ€»æ–‡ä»¶
    
    Args:
        result: åˆ†æç»“æœå­—å…¸
        summary_file: æ±‡æ€»æ–‡ä»¶è·¯å¾„
    """
    try:
        # è¯»å–ç°æœ‰æ±‡æ€»æ–‡ä»¶
        existing_results = []
        if os.path.exists(summary_file):
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                if not isinstance(existing_results, list):
                    existing_results = []
            except Exception as e:
                logging.warning(f"è¯»å–æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                existing_results = []
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè®ºæ–‡IDçš„ç»“æœ
        paper_id = result.get('paper_id')
        if paper_id:
            # ç§»é™¤æ—§çš„ç»“æœ
            existing_results = [r for r in existing_results if r.get('paper_id') != paper_id]
        
        # æ·»åŠ æ–°ç»“æœ
        existing_results.append(result)
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(existing_results, f, ensure_ascii=False, indent=2)
        
        logging.info(f"åˆ†æç»“æœå·²è¿½åŠ åˆ°æ±‡æ€»æ–‡ä»¶: {summary_file} (æ€»è®¡: {len(existing_results)} ç¯‡)")
        
    except Exception as e:
        logging.error(f"ä¿å­˜åˆ°æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")

def load_analysis_results(date_str: str = None) -> List[Dict]:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸçš„åˆ†æç»“æœ
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šYYMMDDï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©
    
    Returns:
        List[Dict]: åˆ†æç»“æœåˆ—è¡¨
    """
    try:
        if not date_str:
            date_str = datetime.now().strftime("%y%m%d")
        
        summary_file = os.path.join(PAPER_DATA_DIR, date_str, "analysis_results", f"analysis_summary_{date_str}.json")
        
        if not os.path.exists(summary_file):
            logging.info(f"æ±‡æ€»æ–‡ä»¶ä¸å­˜åœ¨: {summary_file}")
            return []
        
        with open(summary_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        logging.info(f"æˆåŠŸåŠ è½½åˆ†æç»“æœ: {summary_file} (æ€»è®¡: {len(results)} ç¯‡)")
        return results
        
    except Exception as e:
        logging.error(f"åŠ è½½åˆ†æç»“æœå¤±è´¥: {e}")
        return []

def main_paper_analysis():
    """
    ä¸»è¦çš„è®ºæ–‡åˆ†æå‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘tokenæ¶ˆè€—
    """
    try:
        # åŠ è½½è®ºæ–‡æ•°æ®
        papers = load_paper_data()
        if not papers:
            logging.warning("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")
            return
        
        logging.info(f"å¼€å§‹åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        
        # è·å–å½“å‰æ—¥æœŸ
        date_str = datetime.now().strftime("%y%m%d")
        
        # åˆ›å»ºåˆ†æç»“æœç›®å½•
        analysis_dir = f"./{date_str}/paper_analysis"
        os.makedirs(analysis_dir, exist_ok=True)
        
        all_analysis_results = []
        
        for i, paper in enumerate(papers, 1):
            try:
                logging.info(f"åˆ†æè®ºæ–‡ {i}/{len(papers)}: {paper.get('title', 'Unknown')[:50]}...")
                
                # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†ææ–¹æ³•ï¼Œä¸€æ¬¡æ€§å›ç­”æ‰€æœ‰é—®é¢˜
                analysis_result = analyze_paper_with_questions(
                    paper_title=paper.get('title', ''),
                    paper_abstract=paper.get('abstract', ''),
                    paper_url=paper.get('url'), # ä¼ é€’URL
                    paper_id=paper.get('id'), # ä¼ é€’ID
                    save_results=True # ä¿å­˜ç»“æœ
                )
                
                if analysis_result:
                    # æ·»åŠ è®ºæ–‡åŸºæœ¬ä¿¡æ¯
                    analysis_result.update({
                        'paper_id': paper.get('id', ''),
                        'paper_title': paper.get('title', ''),
                        'paper_url': paper.get('url', ''),
                        'analysis_time': datetime.now().isoformat(),
                        'llm_provider': get_api_config_with_scenario("paper_analysis")["model"] if get_api_config_with_scenario("paper_analysis") else "unknown"
                    })
                    
                    all_analysis_results.append(analysis_result)
                    
                    # ä¿å­˜å•ç¯‡è®ºæ–‡çš„åˆ†æç»“æœ
                    paper_filename = f"paper_analysis_{paper.get('id', f'paper_{i}')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    paper_filepath = os.path.join(analysis_dir, paper_filename)
                    
                    with open(paper_filepath, 'w', encoding='utf-8') as f:
                        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                    
                    logging.info(f"è®ºæ–‡åˆ†æå®Œæˆ: {paper_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(2)
                else:
                    logging.warning(f"è®ºæ–‡ {i} åˆ†æå¤±è´¥")
                
            except Exception as e:
                logging.error(f"åˆ†æè®ºæ–‡ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        if all_analysis_results:
            all_results_filename = f"all_paper_analysis_{date_str}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            all_results_filepath = os.path.join(analysis_dir, all_results_filename)
            
            with open(all_results_filepath, 'w', encoding='utf-8') as f:
                json.dump(all_analysis_results, f, ensure_ascii=False, indent=2)
            
            logging.info(f"æ‰€æœ‰è®ºæ–‡åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {all_results_filepath}")
            logging.info(f"æˆåŠŸåˆ†æ {len(all_analysis_results)} ç¯‡è®ºæ–‡")
        else:
            logging.warning("æ²¡æœ‰æˆåŠŸåˆ†æçš„è®ºæ–‡")
        
        # æ˜¾ç¤ºå’Œä¿å­˜tokenä½¿ç”¨é‡ç»Ÿè®¡
        token_tracker.print_summary()
        token_usage_filename = os.path.join(analysis_dir, f"token_usage_{date_str}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        token_tracker.save_summary(token_usage_filename)
            
    except Exception as e:
        logging.error(f"è®ºæ–‡åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # å³ä½¿å‡ºé”™ä¹Ÿè¦æ˜¾ç¤ºtokenç»Ÿè®¡
        token_tracker.print_summary()
        raise

# ==================== Tokenç»Ÿè®¡ç›¸å…³å‡½æ•° ====================
def get_token_usage_summary() -> Dict:
    """è·å–å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦"""
    return token_tracker.get_summary()

def print_token_usage_summary():
    """æ‰“å°å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦"""
    token_tracker.print_summary()

def save_token_usage_summary(filename: str = None):
    """ä¿å­˜å½“å‰tokenä½¿ç”¨é‡æ‘˜è¦åˆ°æ–‡ä»¶"""
    token_tracker.save_summary(filename)

def reset_token_tracker():
    """é‡ç½®tokenè·Ÿè¸ªå™¨"""
    global token_tracker
    token_tracker = TokenUsageTracker()
    logging.info("Tokenè·Ÿè¸ªå™¨å·²é‡ç½®")

# ==================== PDFå¤„ç†å‡½æ•° ====================
def download_pdf(url: str, paper_id: str, pdf_dir: str = None) -> Optional[str]:
    """
    ä¸‹è½½PDFæ–‡ä»¶
    
    Args:
        url: è®ºæ–‡URL
        paper_id: è®ºæ–‡ID
        pdf_dir: PDFä¿å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
    
    Returns:
        str: PDFæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸‹è½½å¤±è´¥è¿”å›None
    """
    try:
        if not pdf_dir:
            # ä½¿ç”¨é»˜è®¤ç›®å½•
            today = datetime.now().strftime("%y%m%d")
            pdf_dir = os.path.join(PAPER_DATA_DIR, today, "pdf_downloads")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(pdf_dir, exist_ok=True)
        
        pdf_path = os.path.join(pdf_dir, f"{paper_id}.pdf")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
        if os.path.exists(pdf_path):
            logging.info(f"PDFæ–‡ä»¶å·²å­˜åœ¨: {pdf_path}")
            return pdf_path
        
        # å°†absé“¾æ¥è½¬æ¢ä¸ºpdfé“¾æ¥
        pdf_url = url.replace('/abs/', '/pdf/') + '.pdf'
        logging.info(f"æ­£åœ¨ä¸‹è½½PDF: {pdf_url}")
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(pdf_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('content-type', '')
        if 'pdf' not in content_type.lower() and not response.content.startswith(b'%PDF'):
            logging.warning(f"ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½ä¸æ˜¯PDF: {content_type}")
        
        with open(pdf_path, 'wb') as f:
            f.write(response.content)
        
        logging.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_path} (å¤§å°: {len(response.content)} bytes)")
        return pdf_path
        
    except Exception as e:
        logging.error(f"ä¸‹è½½PDFå¤±è´¥ {paper_id}: {e}")
        return None

def encode_pdf_to_base64(pdf_path: str) -> Optional[str]:
    """
    å°†PDFæ–‡ä»¶ç¼–ç ä¸ºbase64å­—ç¬¦ä¸²
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
    
    Returns:
        str: base64ç¼–ç çš„PDFå†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    try:
        if not os.path.exists(pdf_path):
            logging.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return None
        
        with open(pdf_path, 'rb') as f:
            pdf_content = f.read()
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = len(pdf_content) / (1024 * 1024)
        if file_size_mb > 10:  # å¦‚æœPDFå¤§äº10MBï¼Œç»™å‡ºè­¦å‘Š
            logging.warning(f"PDFæ–‡ä»¶è¾ƒå¤§ ({file_size_mb:.1f}MB)ï¼Œå¯èƒ½å½±å“APIè°ƒç”¨")
        
        # ç¼–ç ä¸ºbase64
        base64_content = base64.b64encode(pdf_content).decode('utf-8')
        logging.info(f"PDFç¼–ç æˆåŠŸ: {pdf_path} -> base64 (é•¿åº¦: {len(base64_content)} å­—ç¬¦)")
        
        return base64_content
        
    except Exception as e:
        logging.error(f"PDFç¼–ç å¤±è´¥ {pdf_path}: {e}")
        return None

def create_file_upload_message(pdf_path: str, filename: str = None) -> Dict:
    """
    åˆ›å»ºæ–‡ä»¶ä¸Šä¼ æ¶ˆæ¯ï¼ˆç”¨äºæ”¯æŒæ–‡ä»¶ä¸Šä¼ çš„APIï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸæ–‡ä»¶å
    
    Returns:
        Dict: æ–‡ä»¶ä¸Šä¼ æ¶ˆæ¯
    """
    try:
        if not filename:
            filename = os.path.basename(pdf_path)
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(pdf_path, 'rb') as f:
            file_content = f.read()
        
        # åˆ›å»ºæ–‡ä»¶ä¸Šä¼ æ¶ˆæ¯
        file_message = {
            "type": "file",
            "file": {
                "name": filename,
                "content": file_content,
                "mime_type": "application/pdf"
            }
        }
        
        logging.info(f"æ–‡ä»¶ä¸Šä¼ æ¶ˆæ¯åˆ›å»ºæˆåŠŸ: {filename}")
        return file_message
        
    except Exception as e:
        logging.error(f"åˆ›å»ºæ–‡ä»¶ä¸Šä¼ æ¶ˆæ¯å¤±è´¥ {pdf_path}: {e}")
        return {}

# ==================== ä¸»æ‰§è¡Œæµç¨‹ ====================
if __name__ == '__main__':
    main_paper_analysis()